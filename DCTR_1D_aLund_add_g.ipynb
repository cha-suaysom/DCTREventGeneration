{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the alternative DCTR fitting method applied on Lund jet datasets\n",
    "\n",
    "*Note:* <span style=\"color:red\">Dimension of the input should be 4 instead of 7  <em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import keras\n",
    "\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import keras.backend as K\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=True)\n",
    "\n",
    "#load model from saved file\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_aLund.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_0 = np.load('test1D_default.npz')\n",
    "test_dataset_1 = np.load('test1D_aLund.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = preprocess_data(test_dataset_0['jet'][:,:,:])\n",
    "X_unknown = preprocess_data(test_dataset_1['jet'][:,:,:])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)\n",
    "Y_fit = to_categorical(Y_fit, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit, _, Y_fit, _ = data_split(X_fit, Y_fit, test=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddParams2Input(keras.layers.Layer):\n",
    "    \"\"\" Custom layer for tuning with DCTR: \n",
    "    Arguments:\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Usage: \n",
    "    Let X_dim be the input dimension of each particle to a PFN model, and n_MC_params be the number of MC parameters. \n",
    "    Defines a Layer that takes in an array of dimension \n",
    "    (batch_size, padded_multiplicity, X_dim - n_MC_params)\n",
    "    This layer appends each particle by the default_MC_params and makes then trainable or non-trainable based on trainable_MC_params\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_MC_params, default_MC_params, trainable_MC_params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        # Definitions\n",
    "        self.n_MC_params = n_MC_params\n",
    "        self.MC_params = default_MC_params\n",
    "        self.trainable_MC_params = trainable_MC_params\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Convert input MC parameters to weights and make then trainable or non-trainable\n",
    "        for i in range(self.n_MC_params):\n",
    "            self.MC_params[i] = self.add_weight(name='MC_param_{}'.format(i), \n",
    "                                                shape=(1, 1),\n",
    "                                                initializer=keras.initializers.Constant(self.MC_params[i]),\n",
    "                                                trainable=self.trainable_MC_params[i])\n",
    "            \n",
    "        self.MC_params = keras.backend.tf.concat(self.MC_params, axis = -1)\n",
    "        super(AddParams2Input, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Add MC params to each input particle (but not to the padded rows)\n",
    "        concat_input_and_params = keras.backend.tf.where(keras.backend.abs(input[...,0])>0,\n",
    "                                                         self.MC_params*keras.backend.ones_like(input[...,0:self.n_MC_params]),\n",
    "                                                         keras.backend.zeros_like(input[...,0:self.n_MC_params]))\n",
    "        return keras.backend.concatenate([input, concat_input_and_params], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]+self.n_MC_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import variables as variables_module\n",
    "\n",
    "def _backtrack_identity(tensor):\n",
    "    while tensor.op.type == 'Identity':\n",
    "        tensor = tensor.op.inputs[0]\n",
    "    return tensor\n",
    "\n",
    "def my_loss_wrapper():\n",
    "    def my_loss(y_true,y_pred):\n",
    "        target = y_true\n",
    "        output = y_pred\n",
    "        axis = -1\n",
    "        from_logits = False\n",
    "        target.shape.assert_is_compatible_with(output.shape)\n",
    "        if from_logits:\n",
    "            return nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=target, logits=output, axis=axis)\n",
    "\n",
    "        if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):\n",
    "            output = _backtrack_identity(output)\n",
    "            if output.op.type == 'Softmax':\n",
    "                # When softmax activation function is used for output operation, we\n",
    "                # use logits from the softmax function directly to compute loss in order\n",
    "                # to prevent collapsing zero when training.\n",
    "                # See b/117284466\n",
    "                assert len(output.op.inputs) == 1\n",
    "                output = output.op.inputs[0]\n",
    "                return nn.softmax_cross_entropy_with_logits_v2(\n",
    "                  labels=target, logits=output)\n",
    "\n",
    "        # scale preds so that the class probas of each sample sum to 1\n",
    "        output = output / math_ops.reduce_sum(output, axis, True)\n",
    "        # Compute cross entropy from probabilities.\n",
    "        epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "        output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n",
    "        return -math_ops.reduce_sum(target * math_ops.log(output), axis)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCTR_fit_model(DCTR_model, \n",
    "                       X_dim, \n",
    "                       n_MC_params, \n",
    "                       default_MC_params,\n",
    "                       trainable_MC_params):\n",
    "    \"\"\" \n",
    "    Get a DCTR model that trains on the input MC parameters\n",
    "    \n",
    "    Arguments:\n",
    "    - DCTR_model : a PFN model that has been trained on a to continuously interpolate over the input MC dimensions\n",
    "    - X_dim : (int) - the dimension of the input expected by DCTR_model\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Returns:\n",
    "    - DCTR_fit_model: a compiled model that gradient descends only on the trainable MC parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do sanity checks on inputs\n",
    "    assert X_dim >=n_MC_params, \"X_dim must be larger than n_MC_params. X_dim includes the dimensionality of the 4-vector + number of MC parameters\"\n",
    "    assert n_MC_params == len(default_MC_params), \"Dimension mismatch between n_MC_params and number of default MC parameters given. len(default_MC_params) must equal n_MC_params\"\n",
    "    assert n_MC_params == len(trainable_MC_params), \"Dimension mismatch between n_MC_params and trainable_MC_params. len(trainable_MC_params) must equal n_MC_params.\"\n",
    "    assert np.any(trainable_MC_params), \"All parameters are set to non-trainable.\"\n",
    "    \n",
    "    # Define input to DCTR_fit_model\n",
    "    non_param_input = keras.layers.Input((None, X_dim - n_MC_params))\n",
    "\n",
    "    # Construct layer that adds trainable and non-trainable parameters to the input\n",
    "    add_params_layer = AddParams2Input(n_MC_params, default_MC_params, trainable_MC_params)\n",
    "    time_dist     = keras.layers.TimeDistributed(add_params_layer, name='tdist')(non_param_input)     \n",
    "\n",
    "    # Set all weights in DCTR_model to non-trainable\n",
    "    for layer in DCTR_model.model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # get the graph and the weights from the DCTR_model\n",
    "    output = DCTR_model.model(inputs = time_dist)\n",
    "\n",
    "    # Define full model\n",
    "    DCTR_fit_model = fitmodel = keras.models.Model(inputs = non_param_input, outputs = output)\n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    # Compile with loss function\n",
    "    DCTR_fit_model.compile(optimizer=optimizer, loss=my_loss_wrapper())\n",
    "    \n",
    "    \n",
    "    return DCTR_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_MC_params(dctr_fit_model, MC_params):\n",
    "    alphaS, aLund, StoUD = MC_params\n",
    "    weights = [np.array([[alphaS]],   dtype=np.float32),\n",
    "               np.array([[aLund]],    dtype=np.float32),\n",
    "               np.array([[StoUD]], dtype=np.float32)]\n",
    "    dctr_fit_model.layers[1].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(X, Y, dctr_fit_model, MC_params, batch_size = 1000):\n",
    "    set_MC_params(dctr_fit_model, MC_params)\n",
    "    return dctr_fit_model.evaluate(x=X, y = Y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dctr_fit_model = get_DCTR_fit_model(dctr, \n",
    "                       X_dim =7, \n",
    "                       n_MC_params = 3, \n",
    "                       default_MC_params   = [0.1365, 0.68, 0.217], # default params for [alpha_s, aLund, StoUD]\n",
    "                       trainable_MC_params = [False, True, False]) # Only train aLund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout, Dot, Flatten\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Calculate DCTR Weight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the neural network g (DNN Model) and scan for theta with maximum loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myinputs = Input(shape=(357,))\n",
    "\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "def my_loss_wrapper_fit(myinputs, theta):\n",
    "    myinputs = tf.gather(tf.gather(myinputs, np.arange(batch_size)),np.arange(51), axis = 1)\n",
    "    print(\"Shape of myinputs is\", myinputs.shape)\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "    \n",
    "    #Turn [current_value,current_value,...,0,0] to [theta0,theta0,...,0,0]\n",
    "    aLund_current_value = myinputs[:,:,5]\n",
    "    aLund_replace_weight = tf.equal(aLund_current_value,0)\n",
    "    aLund_replace_weight = (1-tf.cast(aLund_replace_weight,tf.float32))*theta0\n",
    "    print(aLund_replace_weight.shape)\n",
    "    #*theta0\n",
    "    \n",
    "    aLund_tensor = tf.reshape(aLund_replace_weight,[batch_size,51,1])\n",
    "    print(\"Shape of aLund is\", aLund_tensor)\n",
    "    \n",
    "    myinputs_replace_weight = K.concatenate((myinputs[:,:,:5], aLund_tensor, myinputs[:,:,6:]),axis = 2)\n",
    "    \n",
    "    predicted_dctr_weight = dctr.model(myinputs_replace_weight)\n",
    "    weight = predicted_dctr_weight[:,1]/predicted_dctr_weight[:,0]\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        print(\"y_true shape\", y_true.shape)\n",
    "        print(\"y_pred shape\", y_pred.shape)\n",
    "        t_loss = my_sign*(y_true*(y_true - y_pred)**2+(weight)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        return K.mean(t_loss)\n",
    "    \n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0.10, 0.18, 33)\n",
    "lvals = []\n",
    "\n",
    "for theta in thetas:\n",
    "    print(\"trainnig theta = :\", theta)\n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(myinputs,theta),metrics=['accuracy'])\n",
    "    history = model.fit(X_train, Y_train, epochs=1, batch_size=batch_size,validation_data=(X_test, Y_test),verbose=1)\n",
    "    vlvals+=[history.history['val_loss']]\n",
    "    lvals+=[history.history['loss']]\n",
    "    print\n",
    "    pass\n",
    "print(lvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_val = [loss_num[-1] for loss_num in lvals]\n",
    "plt.cla()\n",
    "plt.plot(thetas,loss_val)\n",
    "plt.savefig(\"aLund.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the neural network g (DNN Model) and scan for theta with maximum loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0.6,0.9,20)\n",
    "num_data_test = 10000\n",
    "predicted_weight_list = []\n",
    "X_fit_replace_weight_list = []\n",
    "X_val_replace_weight_list = []\n",
    "for i in range(len(thetas)):\n",
    "    theta = thetas[i]\n",
    "    X_fit_replace_weight = replace_weight(X_fit[:num_data_test],theta)\n",
    "    X_fit_replace_weight_list.append(X_fit_replace_weight)\n",
    "    X_val_replace_weight = replace_weight(X_fit[-num_data_test:],theta)\n",
    "    X_val_replace_weight_list.append(X_val_replace_weight)\n",
    "    predicted_weight_list.append(dctr.predict(X_fit_replace_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myinputs = Input(shape=(357,))\n",
    "\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "def my_loss_wrapper(weight,val=0.0):\n",
    "            \n",
    "    theta0 = val #target value \n",
    "    def my_loss(y_true,y_pred):\n",
    "        print(\"y_true shape\", y_true.shape)\n",
    "        print(\"y_pred shape\", y_pred.shape)\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(weight)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        return K.mean(t_loss)\n",
    "        #return tf.convert_to_tensor(1.0)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lvals = []\n",
    "for i in range(len(thetas)):\n",
    "    print(thetas[i])\n",
    "    theta = thetas[i]\n",
    "    predicted_weight = predicted_weight_list[i]\n",
    "    X_fit_replace_weight = X_fit_replace_weight_list[i]\n",
    "    X_val_replace_weight = X_val_replace_weight_list[i]    \n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(predicted_weight[:,0]/predicted_weight[:,1],theta),metrics=['accuracy'])\n",
    "    model.fit(X_fit_replace_weight.reshape(-1,357), np.argmax(Y_fit[:num_data_test],axis=1), epochs=1, batch_size=1000,validation_data=(X_val_replace_weight.reshape(-1,357), np.argmax(Y_fit[-num_data_test:],axis=1)),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "    #print(lvals) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_val = [loss_num[-1] for loss_num in lvals]\n",
    "plt.cla()\n",
    "plt.plot(thetas,loss_val)\n",
    "plt.savefig(\"aLund.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the neural network g (DNN Model with more layers) and scan for theta with maximum loss\n",
    "\n",
    "*Note:* <span style=\"color:red\">Dimension of the input should be 4 instead of 7  <em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0.6,0.9,20)\n",
    "num_data_test = 10000\n",
    "predicted_weight_list = []\n",
    "X_fit_replace_weight_list = []\n",
    "X_val_replace_weight_list = []\n",
    "for i in range(len(thetas)):\n",
    "    theta = thetas[i]\n",
    "    X_fit_replace_weight = replace_weight(X_fit[:num_data_test],theta)\n",
    "    X_fit_replace_weight_list.append(X_fit_replace_weight)\n",
    "    X_val_replace_weight = replace_weight(X_fit[-num_data_test:],theta)\n",
    "    X_val_replace_weight_list.append(X_val_replace_weight)\n",
    "    predicted_weight_list.append(dctr.predict(X_fit_replace_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myinputs = Input(shape=(357,))\n",
    "\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "def my_loss_wrapper(weight,val=0.0):\n",
    "            \n",
    "    theta0 = val #target value \n",
    "    def my_loss(y_true,y_pred):\n",
    "        print(\"y_true shape\", y_true.shape)\n",
    "        print(\"y_pred shape\", y_pred.shape)\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(weight)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        return K.mean(t_loss)\n",
    "        #return tf.convert_to_tensor(1.0)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lvals = []\n",
    "for i in range(len(thetas)):\n",
    "    print(thetas[i])\n",
    "    theta = thetas[i]\n",
    "    predicted_weight = predicted_weight_list[i]\n",
    "    X_fit_replace_weight = X_fit_replace_weight_list[i]\n",
    "    X_val_replace_weight = X_val_replace_weight_list[i]    \n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(predicted_weight[:,0]/predicted_weight[:,1],theta),metrics=['accuracy'])\n",
    "    model.fit(X_fit_replace_weight.reshape(-1,357), np.argmax(Y_fit[:num_data_test],axis=1), epochs=1, batch_size=1000,validation_data=(X_val_replace_weight.reshape(-1,357), np.argmax(Y_fit[-num_data_test:],axis=1)),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "    #print(lvals) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_val = [loss_num[-1] for loss_num in lvals]\n",
    "plt.cla()\n",
    "plt.plot(thetas,loss_val)\n",
    "plt.savefig(\"aLund.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the neural network g (PFN Models) and scan for theta with maximum loss\n",
    "\n",
    "*Note:* <span style=\"color:red\">Dimension of the input should be 4 instead of 7  <em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0.75,0.85,50)\n",
    "num_data_test = 10000\n",
    "predicted_weight_list = []\n",
    "X_fit_replace_weight_list = []\n",
    "X_val_replace_weight_list = []\n",
    "for i in range(len(thetas)):\n",
    "    theta = thetas[i]\n",
    "    X_fit_replace_weight = replace_weight(X_fit[:num_data_test],theta)\n",
    "    X_fit_replace_weight_list.append(X_fit_replace_weight)\n",
    "    X_val_replace_weight = replace_weight(X_fit[-num_data_test:],theta)\n",
    "    X_val_replace_weight_list.append(X_val_replace_weight)\n",
    "    predicted_weight_list.append(dctr.predict(X_fit_replace_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PFN(input_dim=7, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "\n",
    "def my_loss_wrapper(weight,val=0.0):\n",
    "            \n",
    "    theta0 = val #target value \n",
    "    def my_loss(y_true,y_pred):\n",
    "        print(\"y_true shape\", y_true.shape)\n",
    "        print(\"y_pred shape\", y_pred.shape)\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(weight)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        return K.mean(t_loss)\n",
    "        #return tf.convert_to_tensor(1.0)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lvals = []\n",
    "for i in range(len(thetas)):\n",
    "    print(thetas[i])\n",
    "    theta = thetas[i]\n",
    "    predicted_weight = predicted_weight_list[i]\n",
    "    X_fit_replace_weight = X_fit_replace_weight_list[i]\n",
    "    X_val_replace_weight = X_val_replace_weight_list[i] \n",
    "    model.model.compile(optimizer='adam', loss=my_loss_wrapper(predicted_weight[:,1]/predicted_weight[:,0],theta),metrics=['accuracy'])\n",
    "    model.fit(X_fit_replace_weight, np.argmax(Y_fit[:num_data_test],axis=1), epochs=1, batch_size=100,validation_data=(X_val_replace_weight, np.argmax(Y_fit[-num_data_test:],axis=1)),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "    #print(lvals) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_val = [loss_num[-1] for loss_num in lvals]\n",
    "plt.cla()\n",
    "plt.plot(thetas,loss_val)\n",
    "plt.savefig(\"aLund.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g,theta) fit for PFN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.79\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PFN_model = PFN(input_dim=7, \n",
    "            Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "            output_dim = 1, output_act = 'sigmoid',\n",
    "            summary=False)\n",
    "myinputs_fit = PFN_model.inputs[0]\n",
    "\n",
    "identity = Lambda(lambda x: x + 0)(PFN_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "num_data_test = 10000\n",
    "num_val_test = int(num_data_test/5)\n",
    "batch_size = 10000 #larger batch_size leads to better precision (at least for Guassian case)\n",
    "epochs = 50 #but requires more epochs to train\n",
    "\n",
    "Y_fit_train = np.argmax(Y_fit[:num_data_test],axis=1)\n",
    "Y_fit_val = np.argmax(Y_fit[-num_val_test:],axis=1)\n",
    "X_fit_train = X_fit[:num_data_test]\n",
    "X_fit_val = X_fit[-num_val_test:]\n",
    "#X_nonzero_mask = (X_fit_train[:,:,5] !=  0).reshape(num_data_test,51,1)\n",
    "\n",
    "def my_loss_wrapper_fit(myinputs, train_theta, my_sign = 1):\n",
    "    myinputs = tf.gather(tf.gather(myinputs, np.arange(batch_size)),np.arange(51), axis = 1)\n",
    "    print(\"Shape of myinputs is\", myinputs.shape)\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "    \n",
    "    #Turn [current_value,current_value,...,0,0] to [theta0,theta0,...,0,0]\n",
    "    aLund_current_value = myinputs[:,:,5]\n",
    "    aLund_replace_weight = tf.equal(aLund_current_value,0)\n",
    "    aLund_replace_weight = (1-tf.cast(aLund_replace_weight,tf.float32))*theta0\n",
    "    print(aLund_replace_weight.shape)\n",
    "    #*theta0\n",
    "    \n",
    "    aLund_tensor = tf.reshape(aLund_replace_weight,[batch_size,51,1])\n",
    "    print(\"Shape of aLund is\", aLund_tensor)\n",
    "    \n",
    "    myinputs_replace_weight = K.concatenate((myinputs[:,:,:5], aLund_tensor, myinputs[:,:,6:]),axis = 2)\n",
    "    \n",
    "    predicted_dctr_weight = dctr.model(myinputs_replace_weight)\n",
    "    weight = predicted_dctr_weight[:,1]/predicted_dctr_weight[:,0]\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        print(\"y_true shape\", y_true.shape)\n",
    "        print(\"y_pred shape\", y_pred.shape)\n",
    "        t_loss = my_sign*(y_true*(y_true - y_pred)**2+(weight)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        return K.mean(t_loss)\n",
    "    \n",
    "    return my_loss\n",
    "\n",
    "\n",
    "    \n",
    "for k in range(epochs):  \n",
    "    \n",
    "    theta0 = model_fit.layers[-1].get_weights() \n",
    "    print(\"Theta fit value is\", theta0[0])\n",
    "    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit, train_theta, 1),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    \n",
    "    model_fit.fit(X_fit_train, Y_fit_train, \n",
    "                  epochs=1, batch_size=batch_size,validation_data=(X_fit_val,  Y_fit_val),\n",
    "                  verbose=1,callbacks=callbacks)  \n",
    "    \n",
    "    #######################################################################################\n",
    "    #Now, fix g and train \\theta.\n",
    "    \n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit, train_theta, -1),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_fit_train, Y_fit_train, \n",
    "                  epochs=1, batch_size=batch_size,validation_data=(X_fit_val,  Y_fit_val),\n",
    "                  verbose=1,callbacks=callbacks)    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.80, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"fitting plots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g,theta) fit for DNN Model\n",
    "\n",
    "*Note:* <span style=\"color:red\">Dimension of the input should be 4 instead of 7  <em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LambdaCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(\". theta fit = \",model_fit.layers[-1].get_weights()[-1]))\n",
    "theta_fit_init = 0.79\n",
    "fit_vals = [theta_fit_init]\n",
    "append_fit_value = LambdaCallback(on_epoch_end=lambda batch, logs: \n",
    "                                               fit_vals.append(model_fit.layers[-1].get_weights()[0]))\n",
    "\n",
    "callbacks = [print_weights, append_fit_value]\n",
    "\n",
    "myinputs = Input(shape=(357,))\n",
    "x = Dense(20, activation='relu')(myinputs)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "dnn_model = Model(inputs=myinputs, outputs=predictions)\n",
    "dnn_model.summary()\n",
    "\n",
    "myinputs_fit = dnn_model.inputs[0]\n",
    "identity = Lambda(lambda x: x + 0)(dnn_model.output)\n",
    "\n",
    "model_fit = Model(inputs=myinputs_fit, outputs=identity)\n",
    "model_fit.layers[np.size(model_fit.layers)-1].add_weight(name=\"thetaX\",shape=list(),\n",
    "                                                         initializer = keras.initializers.Constant(value = theta_fit_init),\n",
    "                                                         trainable=True)\n",
    "model_fit.summary()\n",
    "\n",
    "train_theta = False\n",
    "\n",
    "num_data_test = 10000\n",
    "num_val_test = int(num_data_test/5)\n",
    "batch_size = 10000 #larger batch_size leads to better precision (at least for Guassian case)\n",
    "epochs = 20 #but requires more epochs to train\n",
    "\n",
    "Y_fit_train = np.argmax(Y_fit[:num_data_test],axis=1)\n",
    "Y_fit_val = np.argmax(Y_fit[-num_val_test:],axis=1)\n",
    "X_fit_train = X_fit[:num_data_test].reshape(-1,357)\n",
    "X_fit_val = X_fit[-num_val_test:].reshape(-1,357)\n",
    "#X_nonzero_mask = (X_fit_train[:,:,5] !=  0).reshape(num_data_test,51,1)\n",
    "\n",
    "def my_loss_wrapper_fit(myinputs, train_theta, my_sign = 1):\n",
    "    myinputs = tf.gather(myinputs, np.arange(batch_size))\n",
    "    myinputs = tf.reshape(myinputs, [batch_size,51,7])\n",
    "    print(\"Shape of myinputs is\", myinputs.shape)\n",
    "    #Getting theta0:\n",
    "    if train_theta == False:\n",
    "        theta0 = model_fit.layers[-1].get_weights() #when not training theta, fetch as np array \n",
    "    else:\n",
    "        theta0 = model_fit.trainable_weights[-1] #when training theta, fetch as tf.Variable\n",
    "    \n",
    "    #Turn [current_value,current_value,...,0,0] to [theta0,theta0,...,0,0]\n",
    "#     aLund_current_value = myinputs[:,:,5]\n",
    "#     aLund_replace_weight = tf.equal(aLund_current_value,0)\n",
    "#     aLund_replace_weight = (1-tf.cast(aLund_replace_weight,tf.float32))*theta0\n",
    "    \n",
    "    aLund_current_value = myinputs[:,:,5]\n",
    "    aLund_replace_weight = tf.equal(aLund_current_value,0)\n",
    "    aLund_replace_weight = (1-tf.cast(aLund_replace_weight,tf.float32))*theta0\n",
    "    print(aLund_replace_weight.shape)\n",
    "    #*theta0\n",
    "    \n",
    "    aLund_tensor = tf.reshape(aLund_replace_weight,[batch_size,51,1])\n",
    "    print(\"Shape of aLund is\", aLund_tensor)\n",
    "    \n",
    "    myinputs_replace_weight = K.concatenate((myinputs[:,:,:5], aLund_tensor, myinputs[:,:,6:]),axis = 2)\n",
    "    #myinputs_replace_weight = myinputs\n",
    "    \n",
    "    predicted_dctr_weight = dctr.model(myinputs_replace_weight)\n",
    "    weight = predicted_dctr_weight[:,1]/predicted_dctr_weight[:,0]\n",
    "    \n",
    "    def my_loss(y_true,y_pred):\n",
    "        print(\"y_true shape\", y_true.shape)\n",
    "        print(\"y_pred shape\", y_pred.shape)\n",
    "        t_loss = my_sign*(y_true*(y_true - y_pred)**2+(weight)*(1.-y_true)*(y_true - y_pred)**2)\n",
    "        return K.mean(t_loss)\n",
    "    \n",
    "    return my_loss\n",
    "\n",
    "\n",
    "    \n",
    "for k in range(epochs):  \n",
    "    \n",
    "    theta0 = model_fit.layers[-1].get_weights() \n",
    "    print(\"Theta fit value is\", theta0[0])\n",
    "    \n",
    "    print(\"Epoch: \",k )\n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = True\n",
    "        pass\n",
    "    train_theta = False\n",
    "    model_fit.layers[-1].trainable = False\n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit, train_theta, 1),\n",
    "                      metrics=['accuracy'])\n",
    "    print(\"Training g\")\n",
    "    \n",
    "    model_fit.fit(X_fit_train, Y_fit_train, \n",
    "                  epochs=1, batch_size=batch_size,validation_data=(X_fit_val,  Y_fit_val),\n",
    "                  verbose=1,callbacks=callbacks)  \n",
    "    \n",
    "    #######################################################################################\n",
    "    #Now, fix g and train \\theta.\n",
    "    \n",
    "    for i in range(len(model_fit.layers)-1):\n",
    "        model_fit.layers[i].trainable = False\n",
    "        pass    \n",
    "    train_theta = True\n",
    "    model_fit.layers[-1].trainable = True\n",
    "    model_fit.compile(optimizer='adam', loss=my_loss_wrapper_fit(myinputs_fit, train_theta, -1),\n",
    "                      metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    print(\"Training theta\")\n",
    "    model_fit.fit(X_fit_train, Y_fit_train, \n",
    "                  epochs=1, batch_size=batch_size,validation_data=(X_fit_val,  Y_fit_val),\n",
    "                  verbose=1,callbacks=callbacks)    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit_vals, label='Model Fit')\n",
    "plt.hlines(0.80, 0, len(fit_vals), label = 'Truth')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(r'$\\theta_{fit}$')\n",
    "plt.legend()\n",
    "plt.title(\"fitting plots\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
