{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the alternative DCTR fitting method applied on Lund jet datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import keras\n",
    "\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 7)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    800         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            202         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 57,130\n",
      "Trainable params: 57,130\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=True)\n",
    "\n",
    "#load model from saved file\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_aLund.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dataset_0 = np.load('test1D_default.npz')\n",
    "test_dataset_1 = np.load('test1D_aLund.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<energyflow.archs.efn.PFN at 0x7f8c56ccf128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(test_dataset_0['jet'][:,:,:])\n",
    "X_unknown = preprocess_data(test_dataset_1['jet'][:,:,:])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)\n",
    "Y_fit = to_categorical(Y_fit, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fit, _, Y_fit, _ = data_split(X_fit, Y_fit, test=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddParams2Input(keras.layers.Layer):\n",
    "    \"\"\" Custom layer for tuning with DCTR: \n",
    "    Arguments:\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Usage: \n",
    "    Let X_dim be the input dimension of each particle to a PFN model, and n_MC_params be the number of MC parameters. \n",
    "    Defines a Layer that takes in an array of dimension \n",
    "    (batch_size, padded_multiplicity, X_dim - n_MC_params)\n",
    "    This layer appends each particle by the default_MC_params and makes then trainable or non-trainable based on trainable_MC_params\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_MC_params, default_MC_params, trainable_MC_params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        # Definitions\n",
    "        self.n_MC_params = n_MC_params\n",
    "        self.MC_params = default_MC_params\n",
    "        self.trainable_MC_params = trainable_MC_params\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Convert input MC parameters to weights and make then trainable or non-trainable\n",
    "        for i in range(self.n_MC_params):\n",
    "            self.MC_params[i] = self.add_weight(name='MC_param_{}'.format(i), \n",
    "                                                shape=(1, 1),\n",
    "                                                initializer=keras.initializers.Constant(self.MC_params[i]),\n",
    "                                                trainable=self.trainable_MC_params[i])\n",
    "            \n",
    "        self.MC_params = keras.backend.tf.concat(self.MC_params, axis = -1)\n",
    "        super(AddParams2Input, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Add MC params to each input particle (but not to the padded rows)\n",
    "        concat_input_and_params = keras.backend.tf.where(keras.backend.abs(input[...,0])>0,\n",
    "                                                         self.MC_params*keras.backend.ones_like(input[...,0:self.n_MC_params]),\n",
    "                                                         keras.backend.zeros_like(input[...,0:self.n_MC_params]))\n",
    "        return keras.backend.concatenate([input, concat_input_and_params], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]+self.n_MC_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import variables as variables_module\n",
    "\n",
    "def _backtrack_identity(tensor):\n",
    "    while tensor.op.type == 'Identity':\n",
    "        tensor = tensor.op.inputs[0]\n",
    "    return tensor\n",
    "\n",
    "def my_loss_wrapper():\n",
    "    def my_loss(y_true,y_pred):\n",
    "        target = y_true\n",
    "        output = y_pred\n",
    "        axis = -1\n",
    "        from_logits = False\n",
    "        target.shape.assert_is_compatible_with(output.shape)\n",
    "        if from_logits:\n",
    "            return nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=target, logits=output, axis=axis)\n",
    "\n",
    "        if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):\n",
    "            output = _backtrack_identity(output)\n",
    "            if output.op.type == 'Softmax':\n",
    "                # When softmax activation function is used for output operation, we\n",
    "                # use logits from the softmax function directly to compute loss in order\n",
    "                # to prevent collapsing zero when training.\n",
    "                # See b/117284466\n",
    "                assert len(output.op.inputs) == 1\n",
    "                output = output.op.inputs[0]\n",
    "                return nn.softmax_cross_entropy_with_logits_v2(\n",
    "                  labels=target, logits=output)\n",
    "\n",
    "        # scale preds so that the class probas of each sample sum to 1\n",
    "        output = output / math_ops.reduce_sum(output, axis, True)\n",
    "        # Compute cross entropy from probabilities.\n",
    "        epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "        output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n",
    "        return -math_ops.reduce_sum(target * math_ops.log(output), axis)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_DCTR_fit_model(DCTR_model, \n",
    "                       X_dim, \n",
    "                       n_MC_params, \n",
    "                       default_MC_params,\n",
    "                       trainable_MC_params):\n",
    "    \"\"\" \n",
    "    Get a DCTR model that trains on the input MC parameters\n",
    "    \n",
    "    Arguments:\n",
    "    - DCTR_model : a PFN model that has been trained on a to continuously interpolate over the input MC dimensions\n",
    "    - X_dim : (int) - the dimension of the input expected by DCTR_model\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Returns:\n",
    "    - DCTR_fit_model: a compiled model that gradient descends only on the trainable MC parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do sanity checks on inputs\n",
    "    assert X_dim >=n_MC_params, \"X_dim must be larger than n_MC_params. X_dim includes the dimensionality of the 4-vector + number of MC parameters\"\n",
    "    assert n_MC_params == len(default_MC_params), \"Dimension mismatch between n_MC_params and number of default MC parameters given. len(default_MC_params) must equal n_MC_params\"\n",
    "    assert n_MC_params == len(trainable_MC_params), \"Dimension mismatch between n_MC_params and trainable_MC_params. len(trainable_MC_params) must equal n_MC_params.\"\n",
    "    assert np.any(trainable_MC_params), \"All parameters are set to non-trainable.\"\n",
    "    \n",
    "    # Define input to DCTR_fit_model\n",
    "    non_param_input = keras.layers.Input((None, X_dim - n_MC_params))\n",
    "\n",
    "    # Construct layer that adds trainable and non-trainable parameters to the input\n",
    "    add_params_layer = AddParams2Input(n_MC_params, default_MC_params, trainable_MC_params)\n",
    "    time_dist     = keras.layers.TimeDistributed(add_params_layer, name='tdist')(non_param_input)     \n",
    "\n",
    "    # Set all weights in DCTR_model to non-trainable\n",
    "    for layer in DCTR_model.model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # get the graph and the weights from the DCTR_model\n",
    "    output = DCTR_model.model(inputs = time_dist)\n",
    "\n",
    "    # Define full model\n",
    "    DCTR_fit_model = fitmodel = keras.models.Model(inputs = non_param_input, outputs = output)\n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    # Compile with loss function\n",
    "    DCTR_fit_model.compile(optimizer=optimizer, loss=my_loss_wrapper())\n",
    "    \n",
    "    \n",
    "    return DCTR_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_MC_params(dctr_fit_model, MC_params):\n",
    "    alphaS, aLund, StoUD = MC_params\n",
    "    weights = [np.array([[alphaS]],   dtype=np.float32),\n",
    "               np.array([[aLund]],    dtype=np.float32),\n",
    "               np.array([[StoUD]], dtype=np.float32)]\n",
    "    dctr_fit_model.layers[1].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(X, Y, dctr_fit_model, MC_params, batch_size = 1000):\n",
    "    set_MC_params(dctr_fit_model, MC_params)\n",
    "    return dctr_fit_model.evaluate(x=X, y = Y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dctr_fit_model = get_DCTR_fit_model(dctr, \n",
    "                       X_dim =7, \n",
    "                       n_MC_params = 3, \n",
    "                       default_MC_params   = [0.1365, 0.68, 0.217], # default params for [alpha_s, aLund, StoUD]\n",
    "                       trainable_MC_params = [False, True, False]) # Only train aLund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout, Dot, Flatten\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 357)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               45824     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 62,465\n",
      "Trainable params: 62,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(357,))\n",
    "\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "#x = Flatten()(x)\n",
    "#x3 = Dot(1, normalize = True)(x2)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "def my_loss_wrapper(weight,val=0.0):\n",
    "            \n",
    "    theta0 = val #target value \n",
    "    def my_loss(y_true,y_pred):\n",
    "        print(\"y_true shape\", y_true.shape)\n",
    "        print(\"y_pred shape\", y_pred.shape)\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(weight)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        return K.mean(t_loss)\n",
    "        #return tf.convert_to_tensor(1.0)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the neural network g and find where the minimum happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_weight(X_fit_slice,theta):\n",
    "    for i in range(len(X_fit_slice)):\n",
    "        for j in range(len(X_fit_slice[i,:,:])):\n",
    "            if (X_fit_slice[i,j,5]!=0):\n",
    "                X_fit_slice[i,j,5] = theta\n",
    "    return X_fit_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 357)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fit_replace_weight.reshape(-1,357).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 16s 313us/step - loss: 0.3244 - acc: 0.5056 - val_loss: 0.2787 - val_acc: 0.5017\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 9s 170us/step - loss: 0.2785 - acc: 0.5023 - val_loss: 0.2585 - val_acc: 0.5014\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.2595 - acc: 0.4992 - val_loss: 0.2537 - val_acc: 0.5001\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.2547 - acc: 0.4979 - val_loss: 0.2527 - val_acc: 0.4998\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.2539 - acc: 0.4969 - val_loss: 0.2522 - val_acc: 0.4998\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.2539 - acc: 0.4967 - val_loss: 0.2520 - val_acc: 0.5003\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.2536 - acc: 0.4972 - val_loss: 0.2521 - val_acc: 0.4999\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.2535 - acc: 0.4967 - val_loss: 0.2520 - val_acc: 0.4997\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.2534 - acc: 0.4970 - val_loss: 0.2519 - val_acc: 0.4997\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.2534 - acc: 0.4968 - val_loss: 0.2518 - val_acc: 0.4998\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.2534 - acc: 0.4966 - val_loss: 0.2517 - val_acc: 0.4997\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.2534 - acc: 0.4964 - val_loss: 0.2516 - val_acc: 0.4996\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.2533 - acc: 0.4964 - val_loss: 0.2515 - val_acc: 0.4997\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.2531 - acc: 0.4965 - val_loss: 0.2513 - val_acc: 0.4998\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.2529 - acc: 0.4966 - val_loss: 0.2512 - val_acc: 0.4997\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.2527 - acc: 0.4968 - val_loss: 0.2512 - val_acc: 0.4997\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 0.2526 - acc: 0.4969 - val_loss: 0.2512 - val_acc: 0.4995\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.2525 - acc: 0.4968 - val_loss: 0.2512 - val_acc: 0.4996\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.2524 - acc: 0.4968 - val_loss: 0.2511 - val_acc: 0.4996\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.2524 - acc: 0.4968 - val_loss: 0.2510 - val_acc: 0.4997\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.2524 - acc: 0.4968 - val_loss: 0.2511 - val_acc: 0.4996\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 0.2522 - acc: 0.4971 - val_loss: 0.2510 - val_acc: 0.4995\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 0.2520 - acc: 0.4972 - val_loss: 0.2508 - val_acc: 0.4997\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 0.2519 - acc: 0.4973 - val_loss: 0.2505 - val_acc: 0.4998\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 8s 160us/step - loss: 0.2517 - acc: 0.4975 - val_loss: 0.2501 - val_acc: 0.5001\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 8s 160us/step - loss: 0.2514 - acc: 0.4976 - val_loss: 0.2495 - val_acc: 0.5008\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.2508 - acc: 0.4979 - val_loss: 0.2487 - val_acc: 0.5014\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.2499 - acc: 0.4985 - val_loss: 0.2479 - val_acc: 0.5023\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.2489 - acc: 0.4997 - val_loss: 0.2475 - val_acc: 0.5023\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.2480 - acc: 0.5008 - val_loss: 0.2473 - val_acc: 0.5031\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 0.2472 - acc: 0.5016 - val_loss: 0.2466 - val_acc: 0.5040\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 0.2460 - acc: 0.5031 - val_loss: 0.2451 - val_acc: 0.5033\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.2444 - acc: 0.5030 - val_loss: 0.2430 - val_acc: 0.5030\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 11s 225us/step - loss: 0.2424 - acc: 0.5028 - val_loss: 0.2407 - val_acc: 0.5021\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.2402 - acc: 0.5023 - val_loss: 0.2388 - val_acc: 0.5021\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 9s 185us/step - loss: 0.2383 - acc: 0.5020 - val_loss: 0.2371 - val_acc: 0.5017\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.2366 - acc: 0.5028 - val_loss: 0.2353 - val_acc: 0.5025\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.2348 - acc: 0.5026 - val_loss: 0.2333 - val_acc: 0.5027\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.2326 - acc: 0.5031 - val_loss: 0.2309 - val_acc: 0.5024\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 10s 191us/step - loss: 0.2301 - acc: 0.5027 - val_loss: 0.2280 - val_acc: 0.5022\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 16s 320us/step - loss: 0.2271 - acc: 0.5023 - val_loss: 0.2245 - val_acc: 0.5020\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 0.2232 - acc: 0.5034 - val_loss: 0.2215 - val_acc: 0.5023\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.2197 - acc: 0.5051 - val_loss: 0.2182 - val_acc: 0.5029\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 191us/step - loss: 0.2161 - acc: 0.5052 - val_loss: 0.2146 - val_acc: 0.5025\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.2125 - acc: 0.5054 - val_loss: 0.2110 - val_acc: 0.5025\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 231us/step - loss: 0.2090 - acc: 0.5052 - val_loss: 0.2075 - val_acc: 0.5014\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 239us/step - loss: 0.2054 - acc: 0.5049 - val_loss: 0.2040 - val_acc: 0.5013\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 0.2021 - acc: 0.5046 - val_loss: 0.2007 - val_acc: 0.5015\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 0.1990 - acc: 0.5038 - val_loss: 0.1977 - val_acc: 0.5010\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 0.1960 - acc: 0.5039 - val_loss: 0.1949 - val_acc: 0.5018\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 0.1934 - acc: 0.5039 - val_loss: 0.1924 - val_acc: 0.5017\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 191us/step - loss: 0.1911 - acc: 0.5037 - val_loss: 0.1902 - val_acc: 0.5016\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 0.1890 - acc: 0.5038 - val_loss: 0.1883 - val_acc: 0.5014\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 0.1871 - acc: 0.5030 - val_loss: 0.1867 - val_acc: 0.5013\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.1855 - acc: 0.5020 - val_loss: 0.1853 - val_acc: 0.5012\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1841 - acc: 0.5017 - val_loss: 0.1841 - val_acc: 0.5009\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1829 - acc: 0.5010 - val_loss: 0.1830 - val_acc: 0.5011\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1818 - acc: 0.5003 - val_loss: 0.1821 - val_acc: 0.5008\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.1808 - acc: 0.5002 - val_loss: 0.1812 - val_acc: 0.5005\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.1800 - acc: 0.4998 - val_loss: 0.1804 - val_acc: 0.5005\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 230us/step - loss: 0.1792 - acc: 0.5001 - val_loss: 0.1799 - val_acc: 0.5004\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1784 - acc: 0.5006 - val_loss: 0.1791 - val_acc: 0.5002\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1776 - acc: 0.5005 - val_loss: 0.1785 - val_acc: 0.5003\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1770 - acc: 0.5002 - val_loss: 0.1780 - val_acc: 0.5001\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1763 - acc: 0.5004 - val_loss: 0.1776 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 0.1758 - acc: 0.5006 - val_loss: 0.1771 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1752 - acc: 0.5006 - val_loss: 0.1767 - val_acc: 0.4996\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.1747 - acc: 0.5006 - val_loss: 0.1764 - val_acc: 0.4995\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.1743 - acc: 0.5007 - val_loss: 0.1760 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.1739 - acc: 0.5005 - val_loss: 0.1758 - val_acc: 0.4998\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1736 - acc: 0.5007 - val_loss: 0.1755 - val_acc: 0.4999\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.1732 - acc: 0.5009 - val_loss: 0.1753 - val_acc: 0.4998\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 8s 161us/step - loss: 0.1729 - acc: 0.5009 - val_loss: 0.1750 - val_acc: 0.4997\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1726 - acc: 0.5008 - val_loss: 0.1747 - val_acc: 0.4996\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1723 - acc: 0.5006 - val_loss: 0.1745 - val_acc: 0.4998\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.1721 - acc: 0.5005 - val_loss: 0.1743 - val_acc: 0.4999\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1718 - acc: 0.5007 - val_loss: 0.1741 - val_acc: 0.4999\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1716 - acc: 0.5008 - val_loss: 0.1739 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.1714 - acc: 0.5010 - val_loss: 0.1737 - val_acc: 0.4999\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1711 - acc: 0.5013 - val_loss: 0.1736 - val_acc: 0.4999\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.1710 - acc: 0.5012 - val_loss: 0.1736 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1708 - acc: 0.5013 - val_loss: 0.1733 - val_acc: 0.5001\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1705 - acc: 0.5010 - val_loss: 0.1733 - val_acc: 0.4999\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 8s 160us/step - loss: 0.1703 - acc: 0.5017 - val_loss: 0.1734 - val_acc: 0.5002\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 0.1701 - acc: 0.5017 - val_loss: 0.1732 - val_acc: 0.5003\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1699 - acc: 0.5020 - val_loss: 0.1730 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1697 - acc: 0.5016 - val_loss: 0.1728 - val_acc: 0.4998\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1696 - acc: 0.5014 - val_loss: 0.1727 - val_acc: 0.4999\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1693 - acc: 0.5013 - val_loss: 0.1727 - val_acc: 0.5003\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1692 - acc: 0.5018 - val_loss: 0.1727 - val_acc: 0.5004\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.1690 - acc: 0.5020 - val_loss: 0.1726 - val_acc: 0.5006\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1688 - acc: 0.5022 - val_loss: 0.1725 - val_acc: 0.5005\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1686 - acc: 0.5024 - val_loss: 0.1724 - val_acc: 0.5001\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1684 - acc: 0.5023 - val_loss: 0.1723 - val_acc: 0.5002\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1682 - acc: 0.5023 - val_loss: 0.1722 - val_acc: 0.5002\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1681 - acc: 0.5026 - val_loss: 0.1722 - val_acc: 0.5001\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.1679 - acc: 0.5031 - val_loss: 0.1721 - val_acc: 0.4999\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1676 - acc: 0.5032 - val_loss: 0.1720 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1675 - acc: 0.5032 - val_loss: 0.1720 - val_acc: 0.4999\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.1673 - acc: 0.5034 - val_loss: 0.1719 - val_acc: 0.4998\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 225us/step - loss: 0.1671 - acc: 0.5037 - val_loss: 0.1723 - val_acc: 0.4997\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1672 - acc: 0.5047 - val_loss: 0.1715 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1668 - acc: 0.5029 - val_loss: 0.1716 - val_acc: 0.5001\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1667 - acc: 0.5033 - val_loss: 0.1719 - val_acc: 0.5001\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1666 - acc: 0.5047 - val_loss: 0.1719 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 0.1664 - acc: 0.5056 - val_loss: 0.1717 - val_acc: 0.4998\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.1662 - acc: 0.5057 - val_loss: 0.1715 - val_acc: 0.4998\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1660 - acc: 0.5051 - val_loss: 0.1713 - val_acc: 0.5001\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1659 - acc: 0.5045 - val_loss: 0.1711 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.1659 - acc: 0.5040 - val_loss: 0.1711 - val_acc: 0.4999\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1657 - acc: 0.5042 - val_loss: 0.1711 - val_acc: 0.4997\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1655 - acc: 0.5051 - val_loss: 0.1712 - val_acc: 0.4998\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1654 - acc: 0.5058 - val_loss: 0.1713 - val_acc: 0.4999\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1653 - acc: 0.5064 - val_loss: 0.1713 - val_acc: 0.4998\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1651 - acc: 0.5066 - val_loss: 0.1711 - val_acc: 0.4998\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1649 - acc: 0.5066 - val_loss: 0.1710 - val_acc: 0.4998\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.1648 - acc: 0.5063 - val_loss: 0.1710 - val_acc: 0.4998\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1647 - acc: 0.5061 - val_loss: 0.1710 - val_acc: 0.4999\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.1646 - acc: 0.5063 - val_loss: 0.1710 - val_acc: 0.4998\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 4s 89us/step - loss: 0.1644 - acc: 0.5067 - val_loss: 0.1709 - val_acc: 0.4995\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.1643 - acc: 0.5069 - val_loss: 0.1714 - val_acc: 0.4993\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 4s 88us/step - loss: 0.1646 - acc: 0.5078 - val_loss: 0.1712 - val_acc: 0.5001\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 4s 83us/step - loss: 0.1641 - acc: 0.5087 - val_loss: 0.1711 - val_acc: 0.4998\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.1641 - acc: 0.5077 - val_loss: 0.1709 - val_acc: 0.4995\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.1640 - acc: 0.5067 - val_loss: 0.1707 - val_acc: 0.4995\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 0.1638 - acc: 0.5065 - val_loss: 0.1709 - val_acc: 0.4995\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 4s 75us/step - loss: 0.1636 - acc: 0.5078 - val_loss: 0.1711 - val_acc: 0.4995\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 0.1635 - acc: 0.5091 - val_loss: 0.1713 - val_acc: 0.4994\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 0.1634 - acc: 0.5101 - val_loss: 0.1712 - val_acc: 0.4997\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.1632 - acc: 0.5097 - val_loss: 0.1710 - val_acc: 0.5001\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.1631 - acc: 0.5094 - val_loss: 0.1709 - val_acc: 0.4999\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.1630 - acc: 0.5093 - val_loss: 0.1709 - val_acc: 0.4997\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 0.1629 - acc: 0.5096 - val_loss: 0.1709 - val_acc: 0.4997\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.1627 - acc: 0.5100 - val_loss: 0.1709 - val_acc: 0.4995\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 0.1626 - acc: 0.5104 - val_loss: 0.1710 - val_acc: 0.4996\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.1625 - acc: 0.5107 - val_loss: 0.1711 - val_acc: 0.4995\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.1624 - acc: 0.5113 - val_loss: 0.1712 - val_acc: 0.4995\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 9s 183us/step - loss: 0.1622 - acc: 0.5118 - val_loss: 0.1713 - val_acc: 0.4994\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.1621 - acc: 0.5124 - val_loss: 0.1713 - val_acc: 0.4996\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.1620 - acc: 0.5127 - val_loss: 0.1713 - val_acc: 0.4995\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 0.1618 - acc: 0.5128 - val_loss: 0.1715 - val_acc: 0.4995\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.1622 - acc: 0.5111 - val_loss: 0.1717 - val_acc: 0.4996\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.1617 - acc: 0.5158 - val_loss: 0.1716 - val_acc: 0.4998\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.1618 - acc: 0.5147 - val_loss: 0.1713 - val_acc: 0.5001\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 0.1616 - acc: 0.5128 - val_loss: 0.1712 - val_acc: 0.4996\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 0.1614 - acc: 0.5122 - val_loss: 0.1714 - val_acc: 0.4997\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 0.1613 - acc: 0.5137 - val_loss: 0.1718 - val_acc: 0.4998\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 11s 227us/step - loss: 0.1613 - acc: 0.5156 - val_loss: 0.1720 - val_acc: 0.4996\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.1612 - acc: 0.5162 - val_loss: 0.1717 - val_acc: 0.4997\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 0.1610 - acc: 0.5160 - val_loss: 0.1715 - val_acc: 0.4998\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 0.1608 - acc: 0.5151 - val_loss: 0.1714 - val_acc: 0.4996\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 8s 163us/step - loss: 0.1608 - acc: 0.5148 - val_loss: 0.1715 - val_acc: 0.4995\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.1607 - acc: 0.5155 - val_loss: 0.1718 - val_acc: 0.4996\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 161us/step - loss: 0.1605 - acc: 0.5168 - val_loss: 0.1719 - val_acc: 0.5001\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.1604 - acc: 0.5179 - val_loss: 0.1720 - val_acc: 0.5001\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.1603 - acc: 0.5182 - val_loss: 0.1721 - val_acc: 0.5002\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 0.1602 - acc: 0.5185 - val_loss: 0.1721 - val_acc: 0.4998\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 0.1601 - acc: 0.5186 - val_loss: 0.1721 - val_acc: 0.4998\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 10s 201us/step - loss: 0.1599 - acc: 0.5190 - val_loss: 0.1722 - val_acc: 0.5001\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 0.1598 - acc: 0.5199 - val_loss: 0.1722 - val_acc: 0.4998\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 0.1597 - acc: 0.5199 - val_loss: 0.1730 - val_acc: 0.4999\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1605 - acc: 0.5180 - val_loss: 0.1729 - val_acc: 0.4998\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.1597 - acc: 0.5239 - val_loss: 0.1727 - val_acc: 0.4998\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.1599 - acc: 0.5242 - val_loss: 0.1722 - val_acc: 0.4997\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1598 - acc: 0.5204 - val_loss: 0.1720 - val_acc: 0.4996\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.1595 - acc: 0.5186 - val_loss: 0.1722 - val_acc: 0.4997\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.1593 - acc: 0.5198 - val_loss: 0.1727 - val_acc: 0.4996\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1593 - acc: 0.5221 - val_loss: 0.1731 - val_acc: 0.4999\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1592 - acc: 0.5232 - val_loss: 0.1730 - val_acc: 0.4999\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.1591 - acc: 0.5234 - val_loss: 0.1729 - val_acc: 0.5001\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.1589 - acc: 0.5235 - val_loss: 0.1728 - val_acc: 0.4999\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1588 - acc: 0.5233 - val_loss: 0.1727 - val_acc: 0.4995\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1587 - acc: 0.5238 - val_loss: 0.1728 - val_acc: 0.4992\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1587 - acc: 0.5247 - val_loss: 0.1728 - val_acc: 0.4994\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1585 - acc: 0.5256 - val_loss: 0.1729 - val_acc: 0.4998\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1584 - acc: 0.5260 - val_loss: 0.1731 - val_acc: 0.4996\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1583 - acc: 0.5260 - val_loss: 0.1732 - val_acc: 0.4997\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1582 - acc: 0.5263 - val_loss: 0.1733 - val_acc: 0.4998\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1581 - acc: 0.5274 - val_loss: 0.1735 - val_acc: 0.4997\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1580 - acc: 0.5281 - val_loss: 0.1736 - val_acc: 0.4995\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.1578 - acc: 0.5281 - val_loss: 0.1736 - val_acc: 0.4993\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1589 - acc: 0.5218 - val_loss: 0.1743 - val_acc: 0.4998\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1580 - acc: 0.5338 - val_loss: 0.1743 - val_acc: 0.4997\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1581 - acc: 0.5344 - val_loss: 0.1736 - val_acc: 0.4995\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1579 - acc: 0.5280 - val_loss: 0.1732 - val_acc: 0.4997\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1578 - acc: 0.5241 - val_loss: 0.1734 - val_acc: 0.4997\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1576 - acc: 0.5252 - val_loss: 0.1739 - val_acc: 0.4992\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1574 - acc: 0.5289 - val_loss: 0.1744 - val_acc: 0.4991\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.1574 - acc: 0.5324 - val_loss: 0.1743 - val_acc: 0.4993\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.1573 - acc: 0.5332 - val_loss: 0.1738 - val_acc: 0.4997\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1572 - acc: 0.5304 - val_loss: 0.1734 - val_acc: 0.5001\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.1571 - acc: 0.5284 - val_loss: 0.1733 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.1570 - acc: 0.5284 - val_loss: 0.1737 - val_acc: 0.4998\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.1569 - acc: 0.5315 - val_loss: 0.1742 - val_acc: 0.4998\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1568 - acc: 0.5340 - val_loss: 0.1745 - val_acc: 0.4999\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1567 - acc: 0.5354 - val_loss: 0.1744 - val_acc: 0.4999\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.1566 - acc: 0.5337 - val_loss: 0.1743 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1565 - acc: 0.5325 - val_loss: 0.1742 - val_acc: 0.4998\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1565 - acc: 0.5324 - val_loss: 0.1743 - val_acc: 0.4997\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.1563 - acc: 0.5333 - val_loss: 0.1746 - val_acc: 0.5000\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 0.1562 - acc: 0.5356 - val_loss: 0.1745 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 161us/step - loss: 0.1586 - acc: 0.5203 - val_loss: 0.1756 - val_acc: 0.4995\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.1565 - acc: 0.5389 - val_loss: 0.1776 - val_acc: 0.4993\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1574 - acc: 0.5487 - val_loss: 0.1759 - val_acc: 0.4997\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 0.1566 - acc: 0.5435 - val_loss: 0.1740 - val_acc: 0.4995\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 0.1562 - acc: 0.5322 - val_loss: 0.1733 - val_acc: 0.4997\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 193us/step - loss: 0.1567 - acc: 0.5245 - val_loss: 0.1733 - val_acc: 0.4997\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 0.1567 - acc: 0.5237 - val_loss: 0.1737 - val_acc: 0.4997\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 0.1561 - acc: 0.5290 - val_loss: 0.1746 - val_acc: 0.4999\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 0.1558 - acc: 0.5370 - val_loss: 0.1758 - val_acc: 0.4997\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 0.1560 - acc: 0.5431 - val_loss: 0.1762 - val_acc: 0.4996\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 8s 170us/step - loss: 0.1561 - acc: 0.5442 - val_loss: 0.1757 - val_acc: 0.4997\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 9s 178us/step - loss: 0.1558 - acc: 0.5416 - val_loss: 0.1749 - val_acc: 0.4998\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.1556 - acc: 0.5365 - val_loss: 0.1743 - val_acc: 0.4992\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1556 - acc: 0.5328 - val_loss: 0.1742 - val_acc: 0.4991\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1557 - acc: 0.5310 - val_loss: 0.1742 - val_acc: 0.4989\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1555 - acc: 0.5327 - val_loss: 0.1746 - val_acc: 0.4999\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.1553 - acc: 0.5366 - val_loss: 0.1753 - val_acc: 0.4999\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1553 - acc: 0.5415 - val_loss: 0.1757 - val_acc: 0.4999\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1554 - acc: 0.5441 - val_loss: 0.1756 - val_acc: 0.5000\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 239us/step - loss: 0.1552 - acc: 0.5433 - val_loss: 0.1747 - val_acc: 0.4999\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1588 - acc: 0.5167 - val_loss: 0.1748 - val_acc: 0.4991\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1554 - acc: 0.5336 - val_loss: 0.1783 - val_acc: 0.4993\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.1565 - acc: 0.5517 - val_loss: 0.1786 - val_acc: 0.4992\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1566 - acc: 0.5542 - val_loss: 0.1761 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1552 - acc: 0.5466 - val_loss: 0.1742 - val_acc: 0.4997\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1551 - acc: 0.5341 - val_loss: 0.1737 - val_acc: 0.4993\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.1558 - acc: 0.5263 - val_loss: 0.1736 - val_acc: 0.4994\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1559 - acc: 0.5253 - val_loss: 0.1739 - val_acc: 0.4994\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1553 - acc: 0.5307 - val_loss: 0.1747 - val_acc: 0.5002\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1548 - acc: 0.5382 - val_loss: 0.1760 - val_acc: 0.5001\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1549 - acc: 0.5461 - val_loss: 0.1770 - val_acc: 0.4995\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1552 - acc: 0.5497 - val_loss: 0.1770 - val_acc: 0.4996\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1551 - acc: 0.5490 - val_loss: 0.1761 - val_acc: 0.5003\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1547 - acc: 0.5448 - val_loss: 0.1752 - val_acc: 0.4998\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1545 - acc: 0.5394 - val_loss: 0.1747 - val_acc: 0.4993\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1547 - acc: 0.5352 - val_loss: 0.1746 - val_acc: 0.4991\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1548 - acc: 0.5337 - val_loss: 0.1747 - val_acc: 0.4993\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.1545 - acc: 0.5358 - val_loss: 0.1751 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.1543 - acc: 0.5398 - val_loss: 0.1759 - val_acc: 0.5001\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 270us/step - loss: 0.1542 - acc: 0.5449 - val_loss: 0.1753 - val_acc: 0.4997\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 0.1581 - acc: 0.5192 - val_loss: 0.1758 - val_acc: 0.4993\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1544 - acc: 0.5397 - val_loss: 0.1798 - val_acc: 0.4998\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.1560 - acc: 0.5584 - val_loss: 0.1792 - val_acc: 0.5003\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.1556 - acc: 0.5580 - val_loss: 0.1762 - val_acc: 0.4999\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1542 - acc: 0.5483 - val_loss: 0.1744 - val_acc: 0.4999\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1544 - acc: 0.5350 - val_loss: 0.1740 - val_acc: 0.4997\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.1552 - acc: 0.5279 - val_loss: 0.1741 - val_acc: 0.4995\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 0.1550 - acc: 0.5286 - val_loss: 0.1745 - val_acc: 0.4998\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.1543 - acc: 0.5350 - val_loss: 0.1755 - val_acc: 0.5005\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.1538 - acc: 0.5438 - val_loss: 0.1770 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 0.1541 - acc: 0.5512 - val_loss: 0.1780 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.1544 - acc: 0.5538 - val_loss: 0.1775 - val_acc: 0.4999\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1542 - acc: 0.5522 - val_loss: 0.1765 - val_acc: 0.5007\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1537 - acc: 0.5469 - val_loss: 0.1756 - val_acc: 0.4999\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1537 - acc: 0.5411 - val_loss: 0.1752 - val_acc: 0.4994\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.1538 - acc: 0.5376 - val_loss: 0.1750 - val_acc: 0.4993\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.1539 - acc: 0.5368 - val_loss: 0.1752 - val_acc: 0.4995\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1536 - acc: 0.5393 - val_loss: 0.1758 - val_acc: 0.5005\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1533 - acc: 0.5447 - val_loss: 0.1766 - val_acc: 0.5003\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.1534 - acc: 0.5499 - val_loss: 0.1757 - val_acc: 0.4997\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1576 - acc: 0.5203 - val_loss: 0.1762 - val_acc: 0.4996\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.1537 - acc: 0.5415 - val_loss: 0.1803 - val_acc: 0.5007\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1551 - acc: 0.5611 - val_loss: 0.1800 - val_acc: 0.5010\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1549 - acc: 0.5624 - val_loss: 0.1770 - val_acc: 0.5002\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 0.1534 - acc: 0.5526 - val_loss: 0.1750 - val_acc: 0.5002\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.1536 - acc: 0.5387 - val_loss: 0.1745 - val_acc: 0.4994\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 0.1543 - acc: 0.5314 - val_loss: 0.1745 - val_acc: 0.4994\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 10s 210us/step - loss: 0.1543 - acc: 0.5317 - val_loss: 0.1749 - val_acc: 0.5001\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.1535 - acc: 0.5377 - val_loss: 0.1760 - val_acc: 0.5001\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 0.1530 - acc: 0.5465 - val_loss: 0.1776 - val_acc: 0.5002\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1532 - acc: 0.5544 - val_loss: 0.1786 - val_acc: 0.5006\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.1536 - acc: 0.5582 - val_loss: 0.1782 - val_acc: 0.5001\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 0.1533 - acc: 0.5560 - val_loss: 0.1771 - val_acc: 0.5008\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1529 - acc: 0.5505 - val_loss: 0.1762 - val_acc: 0.5008\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1528 - acc: 0.5443 - val_loss: 0.1757 - val_acc: 0.5001\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1531 - acc: 0.5403 - val_loss: 0.1755 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1531 - acc: 0.5395 - val_loss: 0.1758 - val_acc: 0.5002\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.1527 - acc: 0.5421 - val_loss: 0.1764 - val_acc: 0.5006\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 0.1525 - acc: 0.5482 - val_loss: 0.1772 - val_acc: 0.5010\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 14s 279us/step - loss: 0.1526 - acc: 0.5533 - val_loss: 0.1761 - val_acc: 0.4997\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1572 - acc: 0.5213 - val_loss: 0.1765 - val_acc: 0.4999\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1528 - acc: 0.5432 - val_loss: 0.1812 - val_acc: 0.4996\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1545 - acc: 0.5653 - val_loss: 0.1811 - val_acc: 0.5010\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1544 - acc: 0.5681 - val_loss: 0.1779 - val_acc: 0.5007\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1526 - acc: 0.5574 - val_loss: 0.1756 - val_acc: 0.4999\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1526 - acc: 0.5424 - val_loss: 0.1749 - val_acc: 0.4997\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 0.1536 - acc: 0.5337 - val_loss: 0.1749 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 0.1537 - acc: 0.5331 - val_loss: 0.1753 - val_acc: 0.5001\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1529 - acc: 0.5387 - val_loss: 0.1763 - val_acc: 0.5005\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.1522 - acc: 0.5483 - val_loss: 0.1779 - val_acc: 0.5006\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1523 - acc: 0.5573 - val_loss: 0.1792 - val_acc: 0.5012\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.1528 - acc: 0.5621 - val_loss: 0.1790 - val_acc: 0.5011\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.1527 - acc: 0.5611 - val_loss: 0.1780 - val_acc: 0.5005\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.1522 - acc: 0.5555 - val_loss: 0.1768 - val_acc: 0.5010\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s 161us/step - loss: 0.1520 - acc: 0.5489 - val_loss: 0.1762 - val_acc: 0.5005\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.1522 - acc: 0.5435 - val_loss: 0.1759 - val_acc: 0.5007\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.1523 - acc: 0.5411 - val_loss: 0.1760 - val_acc: 0.5002\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1521 - acc: 0.5434 - val_loss: 0.1765 - val_acc: 0.5008\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1518 - acc: 0.5489 - val_loss: 0.1775 - val_acc: 0.5014\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.1517 - acc: 0.5545 - val_loss: 0.1768 - val_acc: 0.4994\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.1564 - acc: 0.5248 - val_loss: 0.1777 - val_acc: 0.5005\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1521 - acc: 0.5494 - val_loss: 0.1823 - val_acc: 0.5007\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1541 - acc: 0.5712 - val_loss: 0.1811 - val_acc: 0.5011\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1532 - acc: 0.5707 - val_loss: 0.1776 - val_acc: 0.5011\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1517 - acc: 0.5571 - val_loss: 0.1757 - val_acc: 0.4998\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1523 - acc: 0.5416 - val_loss: 0.1753 - val_acc: 0.5004\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1531 - acc: 0.5349 - val_loss: 0.1754 - val_acc: 0.5004\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 0.1527 - acc: 0.5366 - val_loss: 0.1760 - val_acc: 0.4999\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 0.1518 - acc: 0.5441 - val_loss: 0.1775 - val_acc: 0.5009\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.1514 - acc: 0.5552 - val_loss: 0.1792 - val_acc: 0.5006\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.1519 - acc: 0.5634 - val_loss: 0.1799 - val_acc: 0.5006\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1521 - acc: 0.5656 - val_loss: 0.1792 - val_acc: 0.5007\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1517 - acc: 0.5621 - val_loss: 0.1780 - val_acc: 0.5011\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.1513 - acc: 0.5546 - val_loss: 0.1770 - val_acc: 0.5012\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.1514 - acc: 0.5479 - val_loss: 0.1765 - val_acc: 0.5007\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1516 - acc: 0.5444 - val_loss: 0.1765 - val_acc: 0.5004\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1515 - acc: 0.5447 - val_loss: 0.1768 - val_acc: 0.5008\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.1511 - acc: 0.5490 - val_loss: 0.1776 - val_acc: 0.5010\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.1510 - acc: 0.5554 - val_loss: 0.1786 - val_acc: 0.5014\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 0.1511 - acc: 0.5618 - val_loss: 0.1768 - val_acc: 0.4994\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.1563 - acc: 0.5233 - val_loss: 0.1772 - val_acc: 0.5005\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 6s 126us/step - loss: 0.1514 - acc: 0.5463 - val_loss: 0.1823 - val_acc: 0.5007\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1530 - acc: 0.5723 - val_loss: 0.1827 - val_acc: 0.5013\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.1532 - acc: 0.5752 - val_loss: 0.1792 - val_acc: 0.5012\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1512 - acc: 0.5654 - val_loss: 0.1766 - val_acc: 0.5004\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1511 - acc: 0.5485 - val_loss: 0.1758 - val_acc: 0.5006\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1521 - acc: 0.5386 - val_loss: 0.1758 - val_acc: 0.5006\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1523 - acc: 0.5373 - val_loss: 0.1761 - val_acc: 0.4998\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.1515 - acc: 0.5429 - val_loss: 0.1772 - val_acc: 0.5009\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1507 - acc: 0.5536 - val_loss: 0.1790 - val_acc: 0.5013\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.1509 - acc: 0.5643 - val_loss: 0.1803 - val_acc: 0.5004\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1514 - acc: 0.5698 - val_loss: 0.1803 - val_acc: 0.5004\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1513 - acc: 0.5690 - val_loss: 0.1791 - val_acc: 0.5010\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.1507 - acc: 0.5629 - val_loss: 0.1778 - val_acc: 0.5013\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.1505 - acc: 0.5544 - val_loss: 0.1771 - val_acc: 0.5013\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1508 - acc: 0.5484 - val_loss: 0.1769 - val_acc: 0.5011\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1509 - acc: 0.5462 - val_loss: 0.1771 - val_acc: 0.5008\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1506 - acc: 0.5482 - val_loss: 0.1776 - val_acc: 0.5010\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1503 - acc: 0.5545 - val_loss: 0.1786 - val_acc: 0.5013\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 0.1502 - acc: 0.5612 - val_loss: 0.1776 - val_acc: 0.4993\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.1561 - acc: 0.5252 - val_loss: 0.1781 - val_acc: 0.5013\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1505 - acc: 0.5521 - val_loss: 0.1843 - val_acc: 0.5010\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1533 - acc: 0.5786 - val_loss: 0.1836 - val_acc: 0.5015\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.1526 - acc: 0.5798 - val_loss: 0.1794 - val_acc: 0.5016\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.1504 - acc: 0.5675 - val_loss: 0.1768 - val_acc: 0.5004\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.1506 - acc: 0.5479 - val_loss: 0.1761 - val_acc: 0.5003\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 6s 126us/step - loss: 0.1518 - acc: 0.5386 - val_loss: 0.1761 - val_acc: 0.5004\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1518 - acc: 0.5383 - val_loss: 0.1765 - val_acc: 0.5004\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.1509 - acc: 0.5448 - val_loss: 0.1777 - val_acc: 0.5012\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1500 - acc: 0.5568 - val_loss: 0.1797 - val_acc: 0.5010\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1503 - acc: 0.5674 - val_loss: 0.1812 - val_acc: 0.5007\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 142us/step - loss: 0.1509 - acc: 0.5734 - val_loss: 0.1811 - val_acc: 0.5007\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1508 - acc: 0.5726 - val_loss: 0.1798 - val_acc: 0.5013\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 0.1501 - acc: 0.5662 - val_loss: 0.1784 - val_acc: 0.5012\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1498 - acc: 0.5578 - val_loss: 0.1775 - val_acc: 0.5013\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 152us/step - loss: 0.1501 - acc: 0.5508 - val_loss: 0.1772 - val_acc: 0.5010\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1503 - acc: 0.5478 - val_loss: 0.1773 - val_acc: 0.5011\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1501 - acc: 0.5493 - val_loss: 0.1778 - val_acc: 0.5014\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 0.1497 - acc: 0.5556 - val_loss: 0.1788 - val_acc: 0.5015\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 14s 272us/step - loss: 0.1495 - acc: 0.5626 - val_loss: 0.1787 - val_acc: 0.4997\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.1537 - acc: 0.5363 - val_loss: 0.1808 - val_acc: 0.5017\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1504 - acc: 0.5644 - val_loss: 0.1837 - val_acc: 0.5016\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.1516 - acc: 0.5817 - val_loss: 0.1808 - val_acc: 0.5010\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1502 - acc: 0.5738 - val_loss: 0.1776 - val_acc: 0.5005\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.1501 - acc: 0.5561 - val_loss: 0.1766 - val_acc: 0.5006\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1509 - acc: 0.5446 - val_loss: 0.1767 - val_acc: 0.5009\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.1507 - acc: 0.5451 - val_loss: 0.1776 - val_acc: 0.5010\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.1497 - acc: 0.5545 - val_loss: 0.1793 - val_acc: 0.5016\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.1493 - acc: 0.5662 - val_loss: 0.1811 - val_acc: 0.5006\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 0.1498 - acc: 0.5738 - val_loss: 0.1813 - val_acc: 0.5006\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.1499 - acc: 0.5734 - val_loss: 0.1802 - val_acc: 0.5011\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.1494 - acc: 0.5670 - val_loss: 0.1790 - val_acc: 0.5014\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 0.1493 - acc: 0.5589 - val_loss: 0.1784 - val_acc: 0.5010\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.1495 - acc: 0.5536 - val_loss: 0.1781 - val_acc: 0.5013\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 0.1494 - acc: 0.5534 - val_loss: 0.1784 - val_acc: 0.5012\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1490 - acc: 0.5586 - val_loss: 0.1793 - val_acc: 0.5017\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.1489 - acc: 0.5661 - val_loss: 0.1803 - val_acc: 0.5012\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1491 - acc: 0.5726 - val_loss: 0.1806 - val_acc: 0.5014\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1491 - acc: 0.5742 - val_loss: 0.1799 - val_acc: 0.5016\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.1488 - acc: 0.5708 - val_loss: 0.1791 - val_acc: 0.5003\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 7s 146us/step - loss: 0.1542 - acc: 0.5341 - val_loss: 0.1805 - val_acc: 0.5018\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 140us/step - loss: 0.1498 - acc: 0.5601 - val_loss: 0.1847 - val_acc: 0.5018\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1511 - acc: 0.5833 - val_loss: 0.1831 - val_acc: 0.5017\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 141us/step - loss: 0.1500 - acc: 0.5828 - val_loss: 0.1794 - val_acc: 0.5011\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 7s 147us/step - loss: 0.1489 - acc: 0.5677 - val_loss: 0.1774 - val_acc: 0.5008\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.1498 - acc: 0.5512 - val_loss: 0.1770 - val_acc: 0.5012\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.1504 - acc: 0.5447 - val_loss: 0.1773 - val_acc: 0.5008\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 7s 143us/step - loss: 0.1498 - acc: 0.5498 - val_loss: 0.1784 - val_acc: 0.5012\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 7s 136us/step - loss: 0.1488 - acc: 0.5613 - val_loss: 0.1803 - val_acc: 0.5010\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 0.1487 - acc: 0.5732 - val_loss: 0.1820 - val_acc: 0.5003\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1492 - acc: 0.5786 - val_loss: 0.1820 - val_acc: 0.5005\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 7s 133us/step - loss: 0.1490 - acc: 0.5775 - val_loss: 0.1808 - val_acc: 0.5007\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.1486 - acc: 0.5706 - val_loss: 0.1796 - val_acc: 0.5012\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.1485 - acc: 0.5626 - val_loss: 0.1790 - val_acc: 0.5011\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.1488 - acc: 0.5567 - val_loss: 0.1787 - val_acc: 0.5014\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 0.1488 - acc: 0.5562 - val_loss: 0.1790 - val_acc: 0.5014\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 0.1483 - acc: 0.5604 - val_loss: 0.1797 - val_acc: 0.5010\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.1481 - acc: 0.5674 - val_loss: 0.1808 - val_acc: 0.5010\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.1482 - acc: 0.5749 - val_loss: 0.1814 - val_acc: 0.5017\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.1483 - acc: 0.5778 - val_loss: 0.1785 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1548 - acc: 0.5271 - val_loss: 0.1787 - val_acc: 0.5006\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.1488 - acc: 0.5541 - val_loss: 0.1848 - val_acc: 0.5019\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.1501 - acc: 0.5861 - val_loss: 0.1862 - val_acc: 0.5018\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 7s 144us/step - loss: 0.1511 - acc: 0.5908 - val_loss: 0.1823 - val_acc: 0.5007\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 6s 116us/step - loss: 0.1486 - acc: 0.5807 - val_loss: 0.1789 - val_acc: 0.5012\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 7s 145us/step - loss: 0.1481 - acc: 0.5624 - val_loss: 0.1777 - val_acc: 0.5002\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 0.1493 - acc: 0.5488 - val_loss: 0.1775 - val_acc: 0.5003\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 0.1498 - acc: 0.5458 - val_loss: 0.1778 - val_acc: 0.5007\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.1489 - acc: 0.5522 - val_loss: 0.1790 - val_acc: 0.5012\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 0.1479 - acc: 0.5646 - val_loss: 0.1811 - val_acc: 0.5012\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 12s 239us/step - loss: 0.1480 - acc: 0.5776 - val_loss: 0.1829 - val_acc: 0.5015\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.1487 - acc: 0.5844 - val_loss: 0.1830 - val_acc: 0.5009\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 0.1486 - acc: 0.5845 - val_loss: 0.1816 - val_acc: 0.5006\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 0.1479 - acc: 0.5778 - val_loss: 0.1801 - val_acc: 0.5013\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 11s 226us/step - loss: 0.1476 - acc: 0.5677 - val_loss: 0.1792 - val_acc: 0.5012\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 11s 225us/step - loss: 0.1479 - acc: 0.5607 - val_loss: 0.1789 - val_acc: 0.5014\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 12s 233us/step - loss: 0.1482 - acc: 0.5568 - val_loss: 0.1790 - val_acc: 0.5010\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 11s 223us/step - loss: 0.1480 - acc: 0.5592 - val_loss: 0.1796 - val_acc: 0.5014\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 11s 229us/step - loss: 0.1475 - acc: 0.5651 - val_loss: 0.1807 - val_acc: 0.5012\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.1474 - acc: 0.5727 - val_loss: 0.1794 - val_acc: 0.5001\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.1549 - acc: 0.5293 - val_loss: 0.1801 - val_acc: 0.5012\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.1476 - acc: 0.5645 - val_loss: 0.1885 - val_acc: 0.5018\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 11s 226us/step - loss: 0.1518 - acc: 0.5951 - val_loss: 0.1870 - val_acc: 0.5021\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.1506 - acc: 0.5964 - val_loss: 0.1818 - val_acc: 0.5015\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 16s 312us/step - loss: 0.1476 - acc: 0.5814 - val_loss: 0.1786 - val_acc: 0.5013\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.1479 - acc: 0.5589 - val_loss: 0.1778 - val_acc: 0.5004\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.1495 - acc: 0.5460 - val_loss: 0.1778 - val_acc: 0.5006\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 0.1497 - acc: 0.5450 - val_loss: 0.1782 - val_acc: 0.5013\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.1484 - acc: 0.5538 - val_loss: 0.1795 - val_acc: 0.5013\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 14s 282us/step - loss: 0.1473 - acc: 0.5671 - val_loss: 0.1819 - val_acc: 0.5005\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 12s 231us/step - loss: 0.1475 - acc: 0.5812 - val_loss: 0.1839 - val_acc: 0.5018\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.1484 - acc: 0.5884 - val_loss: 0.1840 - val_acc: 0.5018\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.1483 - acc: 0.5883 - val_loss: 0.1824 - val_acc: 0.5006\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 10s 193us/step - loss: 0.1475 - acc: 0.5826 - val_loss: 0.1806 - val_acc: 0.5008\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 0.1470 - acc: 0.5713 - val_loss: 0.1794 - val_acc: 0.5014\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 9s 185us/step - loss: 0.1473 - acc: 0.5624 - val_loss: 0.1790 - val_acc: 0.5011\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 9s 179us/step - loss: 0.1477 - acc: 0.5579 - val_loss: 0.1790 - val_acc: 0.5013\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 9s 183us/step - loss: 0.1477 - acc: 0.5585 - val_loss: 0.1795 - val_acc: 0.5015\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 0.1472 - acc: 0.5638 - val_loss: 0.1805 - val_acc: 0.5011\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.1468 - acc: 0.5718 - val_loss: 0.1895 - val_acc: 0.5026\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.1517 - acc: 0.6036 - val_loss: 0.1796 - val_acc: 0.5011\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.1478 - acc: 0.5624 - val_loss: 0.1787 - val_acc: 0.5004\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 8s 170us/step - loss: 0.1504 - acc: 0.5424 - val_loss: 0.1789 - val_acc: 0.5009\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 9s 170us/step - loss: 0.1491 - acc: 0.5480 - val_loss: 0.1804 - val_acc: 0.5014\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 9s 185us/step - loss: 0.1471 - acc: 0.5659 - val_loss: 0.1836 - val_acc: 0.5011\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.1477 - acc: 0.5823 - val_loss: 0.1853 - val_acc: 0.5012\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.1486 - acc: 0.5896 - val_loss: 0.1839 - val_acc: 0.5010\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 9s 170us/step - loss: 0.1476 - acc: 0.5879 - val_loss: 0.1815 - val_acc: 0.5006\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.1468 - acc: 0.5771 - val_loss: 0.1798 - val_acc: 0.5015\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.1471 - acc: 0.5648 - val_loss: 0.1792 - val_acc: 0.5011\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 9s 184us/step - loss: 0.1476 - acc: 0.5583 - val_loss: 0.1792 - val_acc: 0.5011\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.1475 - acc: 0.5593 - val_loss: 0.1798 - val_acc: 0.5013\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 10s 191us/step - loss: 0.1468 - acc: 0.5655 - val_loss: 0.1811 - val_acc: 0.5014\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1465 - acc: 0.5761 - val_loss: 0.1826 - val_acc: 0.5009\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 0.1467 - acc: 0.5844 - val_loss: 0.1835 - val_acc: 0.5014\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 0.1471 - acc: 0.5875 - val_loss: 0.1830 - val_acc: 0.5005\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 0.1468 - acc: 0.5849 - val_loss: 0.1816 - val_acc: 0.5012\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 9s 185us/step - loss: 0.1463 - acc: 0.5789 - val_loss: 0.1806 - val_acc: 0.5014\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 0.1463 - acc: 0.5714 - val_loss: 0.1800 - val_acc: 0.5014\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 18s 366us/step - loss: 0.1465 - acc: 0.5666 - val_loss: 0.1973 - val_acc: 0.4999\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.1582 - acc: 0.6058 - val_loss: 0.1835 - val_acc: 0.5012\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.1466 - acc: 0.5887 - val_loss: 0.1788 - val_acc: 0.5008\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.1486 - acc: 0.5502 - val_loss: 0.1790 - val_acc: 0.5003\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 0.1522 - acc: 0.5344 - val_loss: 0.1788 - val_acc: 0.5001\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 0.1517 - acc: 0.5360 - val_loss: 0.1786 - val_acc: 0.5005\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 9s 185us/step - loss: 0.1487 - acc: 0.5484 - val_loss: 0.1799 - val_acc: 0.5017\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 0.1464 - acc: 0.5688 - val_loss: 0.1833 - val_acc: 0.5019\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.1467 - acc: 0.5893 - val_loss: 0.1868 - val_acc: 0.5024\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.1488 - acc: 0.5995 - val_loss: 0.1874 - val_acc: 0.5018\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 0.1493 - acc: 0.6006 - val_loss: 0.1852 - val_acc: 0.5020\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 9s 181us/step - loss: 0.1477 - acc: 0.5949 - val_loss: 0.1822 - val_acc: 0.5020\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 9s 170us/step - loss: 0.1463 - acc: 0.5836 - val_loss: 0.1800 - val_acc: 0.5016\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 0.1462 - acc: 0.5694 - val_loss: 0.1791 - val_acc: 0.5013\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 9s 184us/step - loss: 0.1471 - acc: 0.5588 - val_loss: 0.1789 - val_acc: 0.5004\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 9s 179us/step - loss: 0.1477 - acc: 0.5535 - val_loss: 0.1790 - val_acc: 0.5006\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 0.1476 - acc: 0.5547 - val_loss: 0.1794 - val_acc: 0.5016\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 0.1467 - acc: 0.5610 - val_loss: 0.1804 - val_acc: 0.5013\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 0.1460 - acc: 0.5705 - val_loss: 0.1820 - val_acc: 0.5015\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 8s 162us/step - loss: 0.1460 - acc: 0.5811 - val_loss: 0.1836 - val_acc: 0.5010\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 17s 350us/step - loss: 0.1465 - acc: 0.5888 - val_loss: 0.1794 - val_acc: 0.5003\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 0.1533 - acc: 0.5314 - val_loss: 0.1791 - val_acc: 0.5009\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 0.1472 - acc: 0.5566 - val_loss: 0.1847 - val_acc: 0.5020\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 0.1469 - acc: 0.5932 - val_loss: 0.1886 - val_acc: 0.5012\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 11s 230us/step - loss: 0.1496 - acc: 0.6031 - val_loss: 0.1852 - val_acc: 0.5019\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.1472 - acc: 0.5960 - val_loss: 0.1809 - val_acc: 0.5013\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 10s 194us/step - loss: 0.1458 - acc: 0.5764 - val_loss: 0.1791 - val_acc: 0.5010\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 0.1469 - acc: 0.5592 - val_loss: 0.1788 - val_acc: 0.5003\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.1478 - acc: 0.5527 - val_loss: 0.1791 - val_acc: 0.5011\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 239us/step - loss: 0.1472 - acc: 0.5567 - val_loss: 0.1801 - val_acc: 0.5017\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.1460 - acc: 0.5684 - val_loss: 0.1822 - val_acc: 0.5012\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 9s 170us/step - loss: 0.1457 - acc: 0.5828 - val_loss: 0.1845 - val_acc: 0.5020\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.1465 - acc: 0.5920 - val_loss: 0.1851 - val_acc: 0.5024\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.1467 - acc: 0.5945 - val_loss: 0.1838 - val_acc: 0.5012\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.1460 - acc: 0.5902 - val_loss: 0.1818 - val_acc: 0.5013\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 0.1454 - acc: 0.5805 - val_loss: 0.1804 - val_acc: 0.5012\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 0.1457 - acc: 0.5709 - val_loss: 0.1800 - val_acc: 0.5010\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.1461 - acc: 0.5649 - val_loss: 0.1801 - val_acc: 0.5011\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.1460 - acc: 0.5657 - val_loss: 0.1808 - val_acc: 0.5016\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 0.1455 - acc: 0.5726 - val_loss: 0.1820 - val_acc: 0.5010\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.75,0.85,25)\n",
    "num_data_test = 50000\n",
    "lvals = []\n",
    "for i in range(len(thetas)):\n",
    "    theta = thetas[i]\n",
    "    X_fit_replace_weight = replace_weight(X_fit[:num_data_test],theta)\n",
    "    X_val_replace_weight = replace_weight(X_fit[-num_data_test:],theta)\n",
    "    predicted_weight = dctr.predict(X_fit_replace_weight)\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(predicted_weight,theta),metrics=['accuracy'])\n",
    "    model.fit(X_fit_replace_weight.reshape(-1,357), np.argmax(Y_fit[:num_data_test],axis=1), epochs=20, batch_size=int(num_data_test),validation_data=(X_val_replace_weight.reshape(-1,357), np.argmax(Y_fit[-num_data_test:],axis=1)),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "#print(lvals) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nztcSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_val = [loss_num[-1] for loss_num in lvals]\n",
    "plt.cla()\n",
    "plt.plot(thetas,loss_val)\n",
    "plt.savefig(\"aLund.png\")\n",
    "display(Image(filename=\"aLund.png\"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
