{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the alternative DCTR fitting method applied on Lund jet datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import keras\n",
    "\n",
    "# standard numerical library imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras.backend as K\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize pT and center (y, phi)\n",
    "def normalize(x):\n",
    "    mask = x[:,0] > 0\n",
    "    yphi_avg = np.average(x[mask,1:3], weights=x[mask,0], axis=0)\n",
    "    x[mask,1:3] -= yphi_avg\n",
    "    x[mask,0] /= x[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    for x in X:\n",
    "        normalize(x)\n",
    "    \n",
    "    # Remap PIDs to unique values in range [0,1]\n",
    "    remap_pids(X, pid_i=3)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 7)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tdist_0 (TimeDistributed)       (None, None, 100)    800         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 100)    0           tdist_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_1 (TimeDistributed)       (None, None, 100)    10100       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 100)    0           tdist_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tdist_2 (TimeDistributed)       (None, None, 128)    12928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mask (Lambda)                   (None, None)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 128)    0           tdist_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sum (Dot)                       (None, 128)          0           mask[0][0]                       \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, 100)          12900       sum[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10100       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          10100       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            202         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 2)            0           output[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 57,130\n",
      "Trainable params: 57,130\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim=7, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=True)\n",
    "\n",
    "#load model from saved file\n",
    "dctr.model.load_weights('./saved_models/DCTR_ee_dijets_1D_aLund.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dataset_0 = np.load('test1D_default.npz')\n",
    "test_dataset_1 = np.load('test1D_aLund.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<energyflow.archs.efn.PFN at 0x7f9a8d000128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_default = preprocess_data(test_dataset_0['jet'][:,:,:4])\n",
    "X_unknown = preprocess_data(test_dataset_1['jet'][:,:,:4])\n",
    "\n",
    "Y_default = np.zeros_like(X_unknown[:,0,0])\n",
    "Y_unknown = np.ones_like(X_unknown[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fit = np.concatenate((X_default, X_unknown), axis = 0)\n",
    "\n",
    "Y_fit = np.concatenate((Y_default, Y_unknown), axis = 0)\n",
    "Y_fit = to_categorical(Y_fit, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fit, _, Y_fit, _ = data_split(X_fit, Y_fit, test=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Curve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddParams2Input(keras.layers.Layer):\n",
    "    \"\"\" Custom layer for tuning with DCTR: \n",
    "    Arguments:\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Usage: \n",
    "    Let X_dim be the input dimension of each particle to a PFN model, and n_MC_params be the number of MC parameters. \n",
    "    Defines a Layer that takes in an array of dimension \n",
    "    (batch_size, padded_multiplicity, X_dim - n_MC_params)\n",
    "    This layer appends each particle by the default_MC_params and makes then trainable or non-trainable based on trainable_MC_params\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_MC_params, default_MC_params, trainable_MC_params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        # Definitions\n",
    "        self.n_MC_params = n_MC_params\n",
    "        self.MC_params = default_MC_params\n",
    "        self.trainable_MC_params = trainable_MC_params\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Convert input MC parameters to weights and make then trainable or non-trainable\n",
    "        for i in range(self.n_MC_params):\n",
    "            self.MC_params[i] = self.add_weight(name='MC_param_{}'.format(i), \n",
    "                                                shape=(1, 1),\n",
    "                                                initializer=keras.initializers.Constant(self.MC_params[i]),\n",
    "                                                trainable=self.trainable_MC_params[i])\n",
    "            \n",
    "        self.MC_params = keras.backend.tf.concat(self.MC_params, axis = -1)\n",
    "        super(AddParams2Input, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Add MC params to each input particle (but not to the padded rows)\n",
    "        concat_input_and_params = keras.backend.tf.where(keras.backend.abs(input[...,0])>0,\n",
    "                                                         self.MC_params*keras.backend.ones_like(input[...,0:self.n_MC_params]),\n",
    "                                                         keras.backend.zeros_like(input[...,0:self.n_MC_params]))\n",
    "        return keras.backend.concatenate([input, concat_input_and_params], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]+self.n_MC_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import variables as variables_module\n",
    "\n",
    "def _backtrack_identity(tensor):\n",
    "    while tensor.op.type == 'Identity':\n",
    "        tensor = tensor.op.inputs[0]\n",
    "    return tensor\n",
    "\n",
    "def my_loss_wrapper():\n",
    "    def my_loss(y_true,y_pred):\n",
    "        target = y_true\n",
    "        output = y_pred\n",
    "        axis = -1\n",
    "        from_logits = False\n",
    "        target.shape.assert_is_compatible_with(output.shape)\n",
    "        if from_logits:\n",
    "            return nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=target, logits=output, axis=axis)\n",
    "\n",
    "        if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):\n",
    "            output = _backtrack_identity(output)\n",
    "            if output.op.type == 'Softmax':\n",
    "                # When softmax activation function is used for output operation, we\n",
    "                # use logits from the softmax function directly to compute loss in order\n",
    "                # to prevent collapsing zero when training.\n",
    "                # See b/117284466\n",
    "                assert len(output.op.inputs) == 1\n",
    "                output = output.op.inputs[0]\n",
    "                return nn.softmax_cross_entropy_with_logits_v2(\n",
    "                  labels=target, logits=output)\n",
    "\n",
    "        # scale preds so that the class probas of each sample sum to 1\n",
    "        output = output / math_ops.reduce_sum(output, axis, True)\n",
    "        # Compute cross entropy from probabilities.\n",
    "        epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "        output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n",
    "        return -math_ops.reduce_sum(target * math_ops.log(output), axis)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_DCTR_fit_model(DCTR_model, \n",
    "                       X_dim, \n",
    "                       n_MC_params, \n",
    "                       default_MC_params,\n",
    "                       trainable_MC_params):\n",
    "    \"\"\" \n",
    "    Get a DCTR model that trains on the input MC parameters\n",
    "    \n",
    "    Arguments:\n",
    "    - DCTR_model : a PFN model that has been trained on a to continuously interpolate over the input MC dimensions\n",
    "    - X_dim : (int) - the dimension of the input expected by DCTR_model\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Returns:\n",
    "    - DCTR_fit_model: a compiled model that gradient descends only on the trainable MC parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do sanity checks on inputs\n",
    "    assert X_dim >=n_MC_params, \"X_dim must be larger than n_MC_params. X_dim includes the dimensionality of the 4-vector + number of MC parameters\"\n",
    "    assert n_MC_params == len(default_MC_params), \"Dimension mismatch between n_MC_params and number of default MC parameters given. len(default_MC_params) must equal n_MC_params\"\n",
    "    assert n_MC_params == len(trainable_MC_params), \"Dimension mismatch between n_MC_params and trainable_MC_params. len(trainable_MC_params) must equal n_MC_params.\"\n",
    "    assert np.any(trainable_MC_params), \"All parameters are set to non-trainable.\"\n",
    "    \n",
    "    # Define input to DCTR_fit_model\n",
    "    non_param_input = keras.layers.Input((None, X_dim - n_MC_params))\n",
    "\n",
    "    # Construct layer that adds trainable and non-trainable parameters to the input\n",
    "    add_params_layer = AddParams2Input(n_MC_params, default_MC_params, trainable_MC_params)\n",
    "    time_dist     = keras.layers.TimeDistributed(add_params_layer, name='tdist')(non_param_input)     \n",
    "\n",
    "    # Set all weights in DCTR_model to non-trainable\n",
    "    for layer in DCTR_model.model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # get the graph and the weights from the DCTR_model\n",
    "    output = DCTR_model.model(inputs = time_dist)\n",
    "\n",
    "    # Define full model\n",
    "    DCTR_fit_model = fitmodel = keras.models.Model(inputs = non_param_input, outputs = output)\n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    # Compile with loss function\n",
    "    DCTR_fit_model.compile(optimizer=optimizer, loss=my_loss_wrapper())\n",
    "    \n",
    "    \n",
    "    return DCTR_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_MC_params(dctr_fit_model, MC_params):\n",
    "    alphaS, aLund, StoUD = MC_params\n",
    "    weights = [np.array([[alphaS]],   dtype=np.float32),\n",
    "               np.array([[aLund]],    dtype=np.float32),\n",
    "               np.array([[StoUD]], dtype=np.float32)]\n",
    "    dctr_fit_model.layers[1].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(X, Y, dctr_fit_model, MC_params, batch_size = 1000):\n",
    "    set_MC_params(dctr_fit_model, MC_params)\n",
    "    return dctr_fit_model.evaluate(x=X, y = Y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dctr_fit_model = get_DCTR_fit_model(dctr, \n",
    "                       X_dim =7, \n",
    "                       n_MC_params = 3, \n",
    "                       default_MC_params   = [0.1365, 0.68, 0.217], # default params for [alpha_s, aLund, StoUD]\n",
    "                       trainable_MC_params = [False, True, False]) # Only train aLund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weight to use in loss function using the precalculated weight in investigate_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outfile = open('aLund_value_weight.pkl','rb')\n",
    "aLund_value_weight = pickle.load(outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "[ 0.04723408  5.495222   -0.2583084   1.8400757   0.82913923  5.128278\n",
      "  3.1625454  -0.05926155  9.07004    -0.38282204 -0.6401678   2.75024\n",
      "  2.5730689  -1.0321598  -1.828589   -0.3643489   0.78350216 24.954819  ]\n",
      "0.7520400285720825\n",
      "[-4.296356    3.2198076   0.3967019   3.6509712   4.035981    0.03655957\n",
      "  3.513217   24.647442    2.5133116   2.0243905   3.677311   21.126732\n",
      "  2.9292662  -0.9438267   1.1570046   0.14417225  0.44584036  0.3665881 ]\n",
      "0.754082977771759\n",
      "[ 6.6651196   6.3782496   2.009265    2.4471297   1.1704493  -0.10741046\n",
      " -0.42744637  3.9320908  -0.9319802  -0.9596939  10.334221    2.1252723\n",
      "  2.0209723  -0.9782467  -3.0380564   5.77397    -0.6061497   6.728255  ]\n",
      "0.7561240196228027\n",
      "[ 1.3165059  2.5392728  3.5418835  0.4561424  2.8752177  2.5848367\n",
      "  0.3010595  7.8576946  4.3879337  3.1616259  5.870673   6.285832\n",
      "  1.1443235  2.3379936  1.7613163  2.8832805 -0.5864992 -2.3308892]\n",
      "0.7581650018692017\n",
      "[ 2.7924595   0.03203768  0.11737637  3.7869365   8.615347    2.5606804\n",
      " -1.1938128   0.9083514   0.2761219   3.0908546   1.3365874   9.228552\n",
      "  4.6965466   4.0847216   4.99168    -2.1658392   2.0977745  20.026852  ]\n",
      "0.7602049708366394\n",
      "[-3.05579     9.414931    3.4152167   2.4355583  -0.27948335  2.0774975\n",
      "  3.0436754   1.0767225   1.5612328   4.2555      5.4212947   1.278826\n",
      " -0.56272215 -0.5006809   2.7377608   9.847582   10.076538    0.08635183]\n",
      "0.7622470259666443\n",
      "[-0.18901016 -0.13157848  2.5947607   5.796025    2.8692446   1.8992014\n",
      "  1.4653366  -0.7901678   0.44428033 -0.47340325  4.66857     0.14958374\n",
      "  1.402246    2.0221138   4.0067225   2.4535837   1.9885963   1.1355693 ]\n",
      "0.7642859816551208\n",
      "[ 0.80919546  0.6267473  11.477598   -0.10493704  1.299716    1.8338127\n",
      "  2.1539521   3.8835123   4.2470903  -0.7633895  -4.3733406   8.321759\n",
      " 24.098648    4.46977     6.089491    2.5330503   2.9175835  -0.22987959]\n",
      "0.766327977180481\n",
      "[-0.63410264  0.7693655  -1.1813765   3.5155535  -2.106598    4.703018\n",
      "  2.7417195   1.2880077   5.6892366  -2.856534    4.670718    2.039925\n",
      "  0.42984158 -0.29124165  0.84576976  2.3287282   0.38874245  0.37411404]\n",
      "0.7683640122413635\n",
      "[ 5.0094566   1.7560833   2.4084244   1.365021    1.9162884   0.59210515\n",
      " -0.29117846 12.188751    2.1389017   1.721761    6.4970794  -0.30026913\n",
      "  1.4206436  11.100721    3.5544586   1.6737086  -0.4837618   6.6770387 ]\n",
      "0.7704049944877625\n",
      "[ 6.530794    3.048768    4.164898   12.118118   -0.41165254 -0.41627502\n",
      "  0.41790494  2.4099119   1.9297831   1.0314847   2.0289342   1.8759718\n",
      "  0.9924844   0.11801295  1.6013546   2.1854482  27.449455    0.04793258]\n",
      "0.7724469900131226\n",
      "[ 5.6694504e-02  6.6132216e+00  3.8207910e+00  1.5994961e+00\n",
      "  2.5434512e-01  1.5398636e+00  5.9718704e+00  1.7259998e+00\n",
      "  2.8720665e+00  2.5956523e+00  3.8740032e+00  2.1822308e+01\n",
      "  3.1000979e+00 -6.9038248e-01  1.4853691e+00  3.2825714e-01\n",
      "  5.2951307e+00  1.7320585e+00  2.2046118e+00  4.2862701e+00\n",
      "  6.4188647e+00  8.4020191e-01  1.1730971e+00 -6.6660887e-01\n",
      "  2.1669689e+01  1.3665137e+00 -1.1041639e-02 -2.8143654e+00\n",
      "  1.7597265e+00  4.0188341e+00 -1.0757463e-02  1.0211535e+00\n",
      "  3.8546946e+00  1.4196513e+00  5.9480557e+00 -6.7602229e-01]\n",
      "0.774495005607605\n",
      "[-0.06147575  3.4840639   4.7529364   2.5209045   0.22332007  0.09000467\n",
      "  5.628681    2.118351   17.674988   -0.29775527  1.1259279   2.7128544\n",
      " -0.14525415  0.10990857  1.4731476   2.481903    2.1028104   2.032633  ]\n",
      "0.7765309810638428\n",
      "[ 1.6360762   1.1290095  -1.7032838   0.4810059  -0.62572193 11.268745\n",
      "  2.7953959  -0.38888973  1.8532524   1.098736    2.113948   -0.7175608\n",
      "  3.2701638   5.6597376  12.807471    1.4177446  13.046354    3.1555266 ]\n",
      "0.7785680294036865\n",
      "[ 4.9131393   2.6261835   0.19649118  6.802301    1.0806143   1.2681732\n",
      " -0.6412392   9.229362    4.4630833  21.795677    7.572504    2.6657584\n",
      " -4.5731945   5.9724307   2.4567454   6.634386   -0.34622386 -0.52153486]\n",
      "0.7806130051612854\n",
      "[ 2.142335   -3.102089    3.2425308   5.621496    1.1123619   1.5263662\n",
      "  5.3135314  -0.4786759   0.7534542  10.428143    5.0553207   1.5081389\n",
      "  0.91360664  3.7248688   0.08700179  1.4018908   1.5628593  -0.5115285 ]\n",
      "0.7826539874076843\n",
      "[-0.03303742 -0.6308007   2.1077452   0.8432534   0.5224404   1.9465759\n",
      "  1.9535322  -0.17385982  1.0748818   1.7239277  -0.6638272   2.8768654\n",
      "  0.6305251   1.098453    2.100044    1.8959639   0.43794683  1.7630527 ]\n",
      "0.7846940159797668\n",
      "[-2.7578697  -3.02285     4.4057508   0.64812803 -0.3245907   2.7244349\n",
      "  2.4020195   7.2741103  -0.04603384  5.886532   -0.37561226 -0.9343581\n",
      "  2.7765834   3.0413837  -2.689682    5.0709133  18.48764     1.581523  ]\n",
      "0.7867310047149658\n",
      "[ 4.5692139e+00  9.2644561e-03 -7.2108990e-01  2.3458498e+00\n",
      "  5.2906499e+00  2.4397743e+01  2.2571211e+01  2.2687123e+00\n",
      "  3.9960380e+00  3.5516834e+00  3.4128771e+00  6.1608977e+00\n",
      "  3.0238574e+00  2.6508503e+00  5.8911324e-01 -1.9403992e+00\n",
      "  4.2456741e+00  3.2257195e+00]\n",
      "0.7887759804725647\n",
      "[ 6.282127    4.4891367  -2.311829    1.8901097   3.1918576   2.0235233\n",
      "  3.3248665   4.9025517   0.69141597 -1.424149    0.6994648   1.3402634\n",
      "  1.5254564   2.3858206  -3.1507282   3.4283159   3.144079    2.257029  ]\n",
      "0.7908160090446472\n",
      "[-0.27192503  6.076709    6.971543    4.767231    1.8421485  -0.7572836\n",
      "  4.035165   -0.23797843  1.5400333  -0.30420828  7.1513953  -2.7512798\n",
      "  8.788726    3.2076437   2.4014807   1.3798568  -0.21525669  2.4180021\n",
      "  2.6058826   4.9786596   4.0012274   4.0135365   4.350642    6.1800303\n",
      "  1.8705281  -0.08814861  1.0448232   5.4050274   0.13667825  0.74617386\n",
      " -0.64822143  0.72597975  5.867919   -3.602295    1.4808524   3.5307968 ]\n",
      "0.7928569912910461\n",
      "[-0.43275213 -0.7563858   1.4784002   4.3383884   1.8825817   1.2603679\n",
      "  0.0934298  12.918698    2.5958827   0.7363339  -0.8996353  -3.5424623\n",
      "  2.0534096   2.839887    2.4413273   5.390821    3.0625627   2.5739377 ]\n",
      "0.7948979735374451\n",
      "[-2.5722828  -0.89306164  3.5103495   2.6787555   2.039055    5.285922\n",
      "  0.2911294  -0.7862129  -0.7473984   5.8424406   4.0548935  -0.5579056\n",
      "  2.8738298   0.6096039  -0.9186435  12.935331   -0.09404495  1.0389137 ]\n",
      "0.7969409823417664\n",
      "[ 2.25869417e+00  2.41354275e+00  3.09666920e+00  3.30877653e-03\n",
      "  5.66273689e-01  1.06552105e+01  6.82649374e-01  2.79435539e+00\n",
      "  1.88690529e+01  6.32690430e+00  2.05455704e+01 -1.33123875e+00\n",
      " -6.88405097e-01  6.83838725e-01  5.78919554e+00 -6.73048913e-01\n",
      "  3.92005771e-01  1.65836847e+00]\n",
      "0.7989839911460876\n",
      "[12.306473   21.814089    6.4041157   4.4351006   0.6752973   1.4826734\n",
      "  3.9937792   4.2410383   0.47678477  5.6498823   0.6948967   3.0039082\n",
      "  1.2038326   1.6594939   2.8075495   1.3851283   0.41515353 -0.42910764]\n",
      "0.801019012928009\n",
      "[ 1.7142028   2.5434008   4.699543    0.49895102 -0.40457952  2.9009511\n",
      "  3.1171303   0.99413496  0.94282174 16.770325    6.141006   -0.05749942\n",
      "  3.5247276   2.666829    0.5831962  -0.16451357 16.410458    0.6720929 ]\n",
      "0.8030589818954468\n",
      "[ 6.5255127   3.583555    4.0607724   6.8729115   2.2910137  17.272594\n",
      "  0.7243893   5.502359    2.3716106   6.3633966   0.23536825  4.9140844\n",
      "  2.7211456  -0.2471714   0.98344237  4.517771   23.692112    0.18639046]\n",
      "0.8051040172576904\n",
      "[ 0.42172146 -0.38328907  5.6643157  -1.0940342   2.0250778   1.812561\n",
      "  0.33141315 -2.8431163   0.5674503  -3.2801132   2.6877577   5.1673555\n",
      " 20.335232   -0.1743779   1.032656   -4.994051    4.5615096  -3.1600113 ]\n",
      "0.807142972946167\n",
      "[ 2.1996279   5.834339    0.25902742 -0.32921225  2.1914563  17.494701\n",
      "  0.88180304  0.72271913  2.0423288   4.104642    0.7500496   1.552871\n",
      " -3.8426285  -0.2531204   2.573154    1.1358485   5.1793756   3.2751145 ]\n",
      "0.8091850280761719\n",
      "[ 4.061141    1.3365765   2.107298    5.2638226   2.134862    0.59018034\n",
      "  2.0599277   3.677442   -0.63684034  3.5903487   1.2053803   3.528758\n",
      "  2.954647    4.667082    0.2101183   2.9940395  -3.4187708  -0.05798623]\n",
      "0.8112239837646484\n",
      "[-0.59783936 -0.12431666  3.0423555   2.8253927   2.285472    5.924907\n",
      "  0.33467633  0.55909157  2.163418    4.4121046  -0.35442537  4.304062\n",
      "  6.6087933   4.7161117   5.7830386   6.6313343  -3.084324    2.5014086 ]\n",
      "0.813264012336731\n",
      "[ 6.6303973  -0.03729629  0.36436364  1.2164612   2.4049177   1.8001671\n",
      " -0.48032713  9.153016    4.1441946   0.6941585   4.547891    3.309407\n",
      "  1.0950229   3.1582224   0.58892363 -0.65958166  2.6782491   0.91886187\n",
      "  2.512658   -0.91240406  0.6506324   0.2815333  11.385654    4.6342\n",
      "  1.0550514   7.667269    2.3970337   2.0703166   0.647808   -0.0424013\n",
      "  7.7888746   5.446466   -0.2919197  -2.8902588   0.05594946 -0.23913002\n",
      "  1.6831555  -0.14402008  1.1949902  -0.85470295  1.0869689   2.7299635\n",
      "  1.7259697   2.6472998   2.0469134   3.087631    6.209883    1.2665663\n",
      "  3.818447    2.0426517   3.4067788  -0.1784425   5.531272    0.12469716]\n",
      "0.8153049945831299\n",
      "[ 6.4096675   1.8162613   0.94135237  0.6706185   1.551213    2.7344332\n",
      " 25.920631   -0.22074986  0.67540276  1.5761209   2.271636    7.4119844\n",
      "  6.044583   -0.6579209  -0.45812368  3.1834571   2.1508756  -1.198389  ]\n",
      "0.8173459768295288\n",
      "[ 0.31006     1.8721838   0.846056    1.8544288   3.4815187   4.2160435\n",
      " -0.9052954   2.030179    3.9756842  -0.36453053 -0.59469455  0.71759504\n",
      "  8.806975    3.6362946  -0.4475727  -0.59705776  4.1853757   1.2278327 ]\n",
      "0.819383978843689\n",
      "[ 4.992213    1.141326    3.0401618   1.0076737   6.4137897   0.35509875\n",
      "  3.2351892   4.226329    1.4090253   1.2892122  -0.2829895  -0.51868916\n",
      " -0.0193267   2.4029903   1.1938114  -0.36200476  0.01699503  2.2505758 ]\n",
      "0.8214309811592102\n",
      "[ 1.7726808   2.786       0.52811056  0.28895953 -3.128498    0.6490212\n",
      "  0.7594435  -0.3548789   1.2753792  -0.21015024  0.12042609  1.1373062\n",
      "  4.119632    2.851224    1.2868499   0.43113467  1.4411888   2.1088305 ]\n",
      "0.8234689831733704\n",
      "[-0.04846715  5.9123764  -0.18964198  1.8597449  -0.5789966  -0.16207483\n",
      "  0.47463655  3.7265053   0.24757385  0.5040331   0.23009546  0.53891706\n",
      "  3.5850809   2.559965   -0.98239136  2.0346878   0.9870631  -0.2878113 ]\n",
      "0.8255100250244141\n",
      "[ 0.19544028  1.6429722   3.235382    0.02050021 -3.1302576   3.2737975\n",
      "  0.07986259 -0.04299212  1.0972995   6.17436     3.0579538   1.0807328\n",
      "  2.732584    3.8358238   1.0925865   5.9610987  -0.9831877   3.9855952 ]\n",
      "0.827551007270813\n",
      "[ 1.909253    1.6689515   2.388567    2.78151     2.4632215   2.0142002\n",
      " -0.5460825   0.9356804  -0.17233896  4.0637026   4.3931656  -4.2541084\n",
      "  1.390955    0.03216894  0.06661892  0.12856385  2.1076221   3.7999659\n",
      "  2.7740872  -0.4306579  -0.9957824   1.3347244   1.7231071  20.052319\n",
      " 12.903031    3.238161    3.7742612   1.4842224   3.0898736  -0.60573626\n",
      "  3.1499014   4.038479   -0.29308313  1.4451054  -0.21592139  2.0585253 ]\n",
      "0.8295909762382507\n",
      "[ 1.9243106   4.3237295   0.9484444   1.9039307   2.3772602   5.4492626\n",
      "  5.30484     1.2817531  -0.39905667  1.753864   -0.19849086 -2.4502287\n",
      "  4.896596    4.9661903   2.3762302   2.914282    0.115964   19.874714  ]\n",
      "0.8316370248794556\n",
      "[-0.9520674  -0.07617949  2.543768    1.3818239   2.796347    1.1583749\n",
      "  2.0493975   3.8660312  -0.5161524   2.2668724   3.766867    4.3213434\n",
      "  3.4522295  -0.69758034  1.3523502  -1.2044554   1.0938554   1.1415281 ]\n",
      "0.8336650133132935\n",
      "[ 6.9639454   0.37348184  5.2223167  15.64343     2.023972   22.13981\n",
      " 14.034621    0.669786    2.4119017   1.1634626   1.3751264   1.3155372\n",
      "  2.5663605   1.5893775   2.7670596  -1.2392502   5.2347937  -0.22885992]\n",
      "0.8357130289077759\n",
      "[ 5.9442153  -0.5647278   1.5600863   3.8026218   3.699069   -0.80165577\n",
      "  3.1859426   0.03076646  0.59039307  2.839776    1.4663482   6.5037737\n",
      "  0.85385346  2.8952086   1.8783765  -0.77135336  2.361694   -0.4049244\n",
      "  2.957125    4.5242662   2.7772229   2.5056422   3.0592263  22.5989\n",
      "  0.25409886 15.3370285  -0.81478506  2.0448523   0.3165225   3.9383793\n",
      "  2.0542479   3.6603632   0.28073326  1.504446    3.3805473   3.0808592 ]\n",
      "0.8377519845962524\n",
      "[10.767172    3.1003501  -2.819418   -0.193171    1.1969066   2.8818548\n",
      "  4.3204393  -0.856812    3.3842113  -1.4445038   0.98919845  3.7945359\n",
      " 13.864919    3.204312    6.7574005  -1.7503691   0.2463997  -0.71517086]\n",
      "0.8397939801216125\n",
      "[-2.9240131   2.2682762  -3.3273468   0.822859    6.817295   -0.9279027\n",
      "  1.431736    2.284171    1.1132603   5.410983    2.1568785  -0.2839887\n",
      " -0.34214973  0.5297463   3.123224    0.588295    1.8195877   0.67202187]\n",
      "0.8418359756469727\n",
      "[ 1.3572713   2.4424326  -0.32095    -0.41589975  2.041689   11.408947\n",
      "  2.4455566   2.45415     9.299761    3.0288687   0.6501069   3.021046\n",
      " -4.4055004   1.1026871   0.42088315  6.565597   -0.33543918  2.6933224\n",
      "  3.7097766   6.654545   -0.67083216  2.5146103   2.785361   -2.2361937\n",
      "  2.7685728   2.5578134   1.7896798   0.15595055  1.3747025  -0.44115975\n",
      " -0.89502716 -3.8147326  -0.50350285  3.8394327   2.3518448  -0.31946474]\n",
      "0.8438779711723328\n",
      "[-0.48995498  4.597308   -0.48944855 -0.69520134  5.8253393  17.517683\n",
      "  3.7489069   3.570565    6.416839    4.7336464   2.9041007   4.370821\n",
      "  0.18577053 -0.6382533  20.322906    0.32989883  0.2450707   4.9355145 ]\n",
      "0.8459200263023376\n",
      "[ 0.1732571   7.0851054   7.521978    2.7681627  -0.2588954   4.3869457\n",
      "  4.15652     0.23659946  5.031907    1.8500347  -0.7314744  -0.10791966\n",
      "  2.949057    2.2037885   6.8191557   4.9303255   2.8647804   1.2612839 ]\n",
      "0.8479589819908142\n",
      "[ 4.7301593   0.49112326 -0.34672788  3.87675     1.0490872   1.796237\n",
      " -0.40374753  0.92567074  1.3850064   1.8569871   2.572203    2.6138668\n",
      "  3.3672113   1.8044593  -1.216291    2.6726513   4.4426603   4.200887  ]\n",
      "0.8500000238418579\n",
      "[ 3.1114094  -0.45552254 -0.4168119  23.013216   -4.7079906   3.1797855\n",
      " -0.7621565   0.36896324 -0.19015887  1.5176798  -0.28227192 16.651907\n",
      "  1.1950536  -0.13994692  6.373899    2.644167    6.753186    1.5474055 ]\n"
     ]
    }
   ],
   "source": [
    "thetas = np.linspace(0.75,0.85,50)\n",
    "aLund_value_list = list(aLund_value_weight.keys())\n",
    "train_result_list = []\n",
    "for i in range(len(thetas)):\n",
    "    #weight_value = dctr.predict(test_dataset_0['jet'][:num_data_test,:,:])\n",
    "    theta = thetas[i]\n",
    "    absolute_difference_function = lambda list_value : abs(list_value - theta)\n",
    "    closest_value = min(aLund_value_list, key=absolute_difference_function)\n",
    "    print(closest_value)\n",
    "    weight_value = aLund_value_weight[closest_value]\n",
    "    weight_num = np.log(weight_value[:,0]/weight_value[:,1])\n",
    "    print(weight_num)\n",
    "    weight = tf.convert_to_tensor(weight_num.astype(np.float32))\n",
    "    train_result_list.append(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, Dense, Input, Layer, Dropout, Dot, Flatten\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 204)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               26240     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 42,881\n",
      "Trainable params: 42,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "myinputs = Input(shape=(204,))\n",
    "\n",
    "x = Dense(128, activation='relu')(myinputs)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "#x = Flatten()(x)\n",
    "#x3 = Dot(1, normalize = True)(x2)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=myinputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "def my_loss_wrapper(weight,val=0.0):\n",
    "            \n",
    "    theta0 = val #target value \n",
    "    def my_loss(y_true,y_pred):\n",
    "        print(\"y_true shape\", y_true.shape)\n",
    "        print(\"y_pred shape\", y_pred.shape)\n",
    "        t_loss = y_true*(y_true - y_pred)**2+(weight)*(1.-y_true)*(y_true - y_pred)**2\n",
    "        return K.mean(t_loss)\n",
    "        #return tf.convert_to_tensor(1.0)\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the neural network g and find where the minimum happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 227us/step - loss: 0.4679 - acc: 0.5029 - val_loss: 0.4477 - val_acc: 0.4988\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.4492 - acc: 0.5035 - val_loss: 0.4341 - val_acc: 0.4990\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.4351 - acc: 0.5032 - val_loss: 0.4239 - val_acc: 0.4990\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.4244 - acc: 0.5032 - val_loss: 0.4160 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.4160 - acc: 0.5032 - val_loss: 0.4095 - val_acc: 0.4990\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.4092 - acc: 0.5032 - val_loss: 0.4041 - val_acc: 0.4990\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.4034 - acc: 0.5032 - val_loss: 0.3997 - val_acc: 0.4990\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3987 - acc: 0.5032 - val_loss: 0.3962 - val_acc: 0.4990\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3950 - acc: 0.5032 - val_loss: 0.3936 - val_acc: 0.4990\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3921 - acc: 0.5032 - val_loss: 0.3916 - val_acc: 0.4990\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3899 - acc: 0.5032 - val_loss: 0.3906 - val_acc: 0.4990\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3887 - acc: 0.5032 - val_loss: 0.3901 - val_acc: 0.4990\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.3880 - acc: 0.5032 - val_loss: 0.3903 - val_acc: 0.4990\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3880 - acc: 0.5032 - val_loss: 0.3907 - val_acc: 0.4990\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3882 - acc: 0.5032 - val_loss: 0.3914 - val_acc: 0.4990\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3888 - acc: 0.5032 - val_loss: 0.3921 - val_acc: 0.4990\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3894 - acc: 0.5032 - val_loss: 0.3926 - val_acc: 0.4990\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3899 - acc: 0.5032 - val_loss: 0.3931 - val_acc: 0.4990\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3902 - acc: 0.5032 - val_loss: 0.3932 - val_acc: 0.4990\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3903 - acc: 0.5032 - val_loss: 0.3931 - val_acc: 0.4990\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 207us/step - loss: 0.4105 - acc: 0.5032 - val_loss: 0.4126 - val_acc: 0.4990\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.4097 - acc: 0.5032 - val_loss: 0.4121 - val_acc: 0.4990\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.4090 - acc: 0.5032 - val_loss: 0.4113 - val_acc: 0.4990\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4082 - acc: 0.5032 - val_loss: 0.4105 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.4073 - acc: 0.5032 - val_loss: 0.4097 - val_acc: 0.4990\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.4065 - acc: 0.5032 - val_loss: 0.4090 - val_acc: 0.4990\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.4057 - acc: 0.5032 - val_loss: 0.4082 - val_acc: 0.4990\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.4047 - acc: 0.5032 - val_loss: 0.4072 - val_acc: 0.4990\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.4038 - acc: 0.5032 - val_loss: 0.4062 - val_acc: 0.4990\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.4026 - acc: 0.5032 - val_loss: 0.4052 - val_acc: 0.4990\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4014 - acc: 0.5032 - val_loss: 0.4042 - val_acc: 0.4990\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.4004 - acc: 0.5032 - val_loss: 0.4033 - val_acc: 0.4990\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3994 - acc: 0.5031 - val_loss: 0.4026 - val_acc: 0.4990\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3985 - acc: 0.5032 - val_loss: 0.4020 - val_acc: 0.4990\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3978 - acc: 0.5033 - val_loss: 0.4014 - val_acc: 0.4989\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3970 - acc: 0.5033 - val_loss: 0.4009 - val_acc: 0.4989\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3965 - acc: 0.5034 - val_loss: 0.4007 - val_acc: 0.4990\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3964 - acc: 0.5036 - val_loss: 0.4006 - val_acc: 0.4991\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3963 - acc: 0.5036 - val_loss: 0.4004 - val_acc: 0.4991\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3960 - acc: 0.5036 - val_loss: 0.4001 - val_acc: 0.4990\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 198us/step - loss: 0.3655 - acc: 0.5036 - val_loss: 0.3613 - val_acc: 0.4988\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3572 - acc: 0.5039 - val_loss: 0.3584 - val_acc: 0.4981\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3543 - acc: 0.5047 - val_loss: 0.3582 - val_acc: 0.4986\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3544 - acc: 0.5057 - val_loss: 0.3577 - val_acc: 0.4981\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3539 - acc: 0.5056 - val_loss: 0.3563 - val_acc: 0.4981\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3523 - acc: 0.5049 - val_loss: 0.3547 - val_acc: 0.4985\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3506 - acc: 0.5049 - val_loss: 0.3536 - val_acc: 0.4987\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3496 - acc: 0.5040 - val_loss: 0.3533 - val_acc: 0.4988\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3492 - acc: 0.5038 - val_loss: 0.3535 - val_acc: 0.4989\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3493 - acc: 0.5035 - val_loss: 0.3537 - val_acc: 0.4989\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3496 - acc: 0.5035 - val_loss: 0.3537 - val_acc: 0.4989\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3497 - acc: 0.5034 - val_loss: 0.3537 - val_acc: 0.4989\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3496 - acc: 0.5034 - val_loss: 0.3535 - val_acc: 0.4989\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3496 - acc: 0.5034 - val_loss: 0.3533 - val_acc: 0.4989\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3494 - acc: 0.5034 - val_loss: 0.3533 - val_acc: 0.4989\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3493 - acc: 0.5034 - val_loss: 0.3533 - val_acc: 0.4989\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3492 - acc: 0.5035 - val_loss: 0.3533 - val_acc: 0.4989\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3493 - acc: 0.5035 - val_loss: 0.3533 - val_acc: 0.4989\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3492 - acc: 0.5036 - val_loss: 0.3533 - val_acc: 0.4989\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 3s 56us/step - loss: 0.3491 - acc: 0.5036 - val_loss: 0.3532 - val_acc: 0.4989\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 16s 314us/step - loss: 0.3594 - acc: 0.5037 - val_loss: 0.3627 - val_acc: 0.4989\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3581 - acc: 0.5034 - val_loss: 0.3630 - val_acc: 0.4990\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3582 - acc: 0.5034 - val_loss: 0.3625 - val_acc: 0.4989\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 3s 50us/step - loss: 0.3576 - acc: 0.5035 - val_loss: 0.3619 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3567 - acc: 0.5037 - val_loss: 0.3620 - val_acc: 0.4989\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3566 - acc: 0.5042 - val_loss: 0.3624 - val_acc: 0.4989\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.3569 - acc: 0.5047 - val_loss: 0.3623 - val_acc: 0.4989\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.3567 - acc: 0.5049 - val_loss: 0.3620 - val_acc: 0.4989\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 0.3563 - acc: 0.5045 - val_loss: 0.3620 - val_acc: 0.4989\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 3s 65us/step - loss: 0.3562 - acc: 0.5043 - val_loss: 0.3621 - val_acc: 0.4989\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.3563 - acc: 0.5041 - val_loss: 0.3621 - val_acc: 0.4989\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 3s 65us/step - loss: 0.3562 - acc: 0.5041 - val_loss: 0.3620 - val_acc: 0.4989\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 3s 65us/step - loss: 0.3559 - acc: 0.5043 - val_loss: 0.3618 - val_acc: 0.4990\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 0.3556 - acc: 0.5045 - val_loss: 0.3618 - val_acc: 0.4990\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 3s 65us/step - loss: 0.3555 - acc: 0.5048 - val_loss: 0.3620 - val_acc: 0.4991\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.3555 - acc: 0.5051 - val_loss: 0.3620 - val_acc: 0.4990\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 0.3553 - acc: 0.5050 - val_loss: 0.3618 - val_acc: 0.4989\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 3s 67us/step - loss: 0.3552 - acc: 0.5047 - val_loss: 0.3616 - val_acc: 0.4989\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3550 - acc: 0.5045 - val_loss: 0.3616 - val_acc: 0.4989\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3549 - acc: 0.5044 - val_loss: 0.3616 - val_acc: 0.4989\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 16s 328us/step - loss: 0.3942 - acc: 0.5044 - val_loss: 0.3957 - val_acc: 0.4990\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3889 - acc: 0.5035 - val_loss: 0.3950 - val_acc: 0.4990\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 0.3886 - acc: 0.5034 - val_loss: 0.3957 - val_acc: 0.4990\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 3s 59us/step - loss: 0.3894 - acc: 0.5034 - val_loss: 0.3960 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 0.3895 - acc: 0.5034 - val_loss: 0.3954 - val_acc: 0.4990\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3887 - acc: 0.5035 - val_loss: 0.3945 - val_acc: 0.4989\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3874 - acc: 0.5037 - val_loss: 0.3941 - val_acc: 0.4989\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 0.3863 - acc: 0.5042 - val_loss: 0.3947 - val_acc: 0.4989\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 3s 58us/step - loss: 0.3864 - acc: 0.5047 - val_loss: 0.3960 - val_acc: 0.4992\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 3s 57us/step - loss: 0.3872 - acc: 0.5056 - val_loss: 0.3964 - val_acc: 0.4992\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3874 - acc: 0.5061 - val_loss: 0.3958 - val_acc: 0.4993\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3869 - acc: 0.5058 - val_loss: 0.3950 - val_acc: 0.4992\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 0.3862 - acc: 0.5052 - val_loss: 0.3945 - val_acc: 0.4990\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 3s 64us/step - loss: 0.3858 - acc: 0.5047 - val_loss: 0.3943 - val_acc: 0.4989\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.3859 - acc: 0.5045 - val_loss: 0.3945 - val_acc: 0.4989\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3862 - acc: 0.5043 - val_loss: 0.3945 - val_acc: 0.4990\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3862 - acc: 0.5042 - val_loss: 0.3942 - val_acc: 0.4990\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 0.3859 - acc: 0.5043 - val_loss: 0.3941 - val_acc: 0.4990\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 0.3856 - acc: 0.5045 - val_loss: 0.3941 - val_acc: 0.4991\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3853 - acc: 0.5047 - val_loss: 0.3943 - val_acc: 0.4990\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 14s 279us/step - loss: 0.3677 - acc: 0.5050 - val_loss: 0.3758 - val_acc: 0.4995\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3665 - acc: 0.5080 - val_loss: 0.3758 - val_acc: 0.4993\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 3s 58us/step - loss: 0.3664 - acc: 0.5080 - val_loss: 0.3745 - val_acc: 0.4992\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3654 - acc: 0.5067 - val_loss: 0.3740 - val_acc: 0.4991\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 3s 50us/step - loss: 0.3652 - acc: 0.5053 - val_loss: 0.3740 - val_acc: 0.4990\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 3s 67us/step - loss: 0.3654 - acc: 0.5047 - val_loss: 0.3739 - val_acc: 0.4990\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 0.3655 - acc: 0.5046 - val_loss: 0.3738 - val_acc: 0.4990\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3652 - acc: 0.5047 - val_loss: 0.3738 - val_acc: 0.4991\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3652 - acc: 0.5050 - val_loss: 0.3740 - val_acc: 0.4990\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3651 - acc: 0.5054 - val_loss: 0.3743 - val_acc: 0.4990\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 0.3651 - acc: 0.5056 - val_loss: 0.3743 - val_acc: 0.4991\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.3649 - acc: 0.5058 - val_loss: 0.3741 - val_acc: 0.4991\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3646 - acc: 0.5058 - val_loss: 0.3740 - val_acc: 0.4990\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3644 - acc: 0.5056 - val_loss: 0.3740 - val_acc: 0.4991\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3643 - acc: 0.5055 - val_loss: 0.3741 - val_acc: 0.4992\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3643 - acc: 0.5057 - val_loss: 0.3742 - val_acc: 0.4992\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 3s 63us/step - loss: 0.3642 - acc: 0.5059 - val_loss: 0.3743 - val_acc: 0.4991\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.3640 - acc: 0.5065 - val_loss: 0.3745 - val_acc: 0.4991\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 50us/step - loss: 0.3640 - acc: 0.5071 - val_loss: 0.3747 - val_acc: 0.4991\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 0.3639 - acc: 0.5075 - val_loss: 0.3747 - val_acc: 0.4990\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 264us/step - loss: 0.3255 - acc: 0.5075 - val_loss: 0.3269 - val_acc: 0.4999\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3176 - acc: 0.5129 - val_loss: 0.3249 - val_acc: 0.4999\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3159 - acc: 0.5147 - val_loss: 0.3247 - val_acc: 0.5002\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3159 - acc: 0.5138 - val_loss: 0.3239 - val_acc: 0.5002\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3152 - acc: 0.5157 - val_loss: 0.3223 - val_acc: 0.4996\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3135 - acc: 0.5165 - val_loss: 0.3208 - val_acc: 0.4997\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3119 - acc: 0.5157 - val_loss: 0.3198 - val_acc: 0.4995\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3110 - acc: 0.5129 - val_loss: 0.3196 - val_acc: 0.4993\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3110 - acc: 0.5102 - val_loss: 0.3196 - val_acc: 0.4994\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3113 - acc: 0.5083 - val_loss: 0.3198 - val_acc: 0.4991\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3115 - acc: 0.5071 - val_loss: 0.3198 - val_acc: 0.4992\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 0.3117 - acc: 0.5067 - val_loss: 0.3196 - val_acc: 0.4992\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3116 - acc: 0.5068 - val_loss: 0.3194 - val_acc: 0.4991\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 0.3115 - acc: 0.5073 - val_loss: 0.3193 - val_acc: 0.4993\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3113 - acc: 0.5078 - val_loss: 0.3193 - val_acc: 0.4994\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 3s 62us/step - loss: 0.3113 - acc: 0.5088 - val_loss: 0.3194 - val_acc: 0.4995\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 3s 61us/step - loss: 0.3113 - acc: 0.5098 - val_loss: 0.3195 - val_acc: 0.4996\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 0.3113 - acc: 0.5102 - val_loss: 0.3195 - val_acc: 0.4996\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 0.3112 - acc: 0.5107 - val_loss: 0.3194 - val_acc: 0.4996\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 3s 51us/step - loss: 0.3110 - acc: 0.5109 - val_loss: 0.3193 - val_acc: 0.4996\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.4589 - acc: 0.5106 - val_loss: 0.4430 - val_acc: 0.4991\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.4316 - acc: 0.5053 - val_loss: 0.4248 - val_acc: 0.4990\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4147 - acc: 0.5039 - val_loss: 0.4139 - val_acc: 0.4990\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 3s 51us/step - loss: 0.4048 - acc: 0.5034 - val_loss: 0.4079 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 0.3997 - acc: 0.5033 - val_loss: 0.4051 - val_acc: 0.4990\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 3s 57us/step - loss: 0.3975 - acc: 0.5032 - val_loss: 0.4045 - val_acc: 0.4990\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3973 - acc: 0.5032 - val_loss: 0.4051 - val_acc: 0.4990\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3981 - acc: 0.5032 - val_loss: 0.4062 - val_acc: 0.4990\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 3s 58us/step - loss: 0.3995 - acc: 0.5032 - val_loss: 0.4074 - val_acc: 0.4990\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 3s 59us/step - loss: 0.4009 - acc: 0.5032 - val_loss: 0.4085 - val_acc: 0.4990\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 3s 61us/step - loss: 0.4020 - acc: 0.5032 - val_loss: 0.4092 - val_acc: 0.4990\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 0.4028 - acc: 0.5032 - val_loss: 0.4095 - val_acc: 0.4990\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.4029 - acc: 0.5032 - val_loss: 0.4092 - val_acc: 0.4990\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 3s 51us/step - loss: 0.4027 - acc: 0.5033 - val_loss: 0.4087 - val_acc: 0.4990\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 3s 59us/step - loss: 0.4020 - acc: 0.5033 - val_loss: 0.4079 - val_acc: 0.4990\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.4009 - acc: 0.5033 - val_loss: 0.4067 - val_acc: 0.4990\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3997 - acc: 0.5034 - val_loss: 0.4056 - val_acc: 0.4990\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3981 - acc: 0.5034 - val_loss: 0.4043 - val_acc: 0.4990\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 3s 55us/step - loss: 0.3966 - acc: 0.5034 - val_loss: 0.4033 - val_acc: 0.4990\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 3s 54us/step - loss: 0.3951 - acc: 0.5036 - val_loss: 0.4025 - val_acc: 0.4990\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.3450 - acc: 0.5037 - val_loss: 0.3369 - val_acc: 0.4990\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3292 - acc: 0.5042 - val_loss: 0.3223 - val_acc: 0.4990\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3140 - acc: 0.5054 - val_loss: 0.3090 - val_acc: 0.4988\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 50us/step - loss: 0.3003 - acc: 0.5079 - val_loss: 0.2983 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.2896 - acc: 0.5114 - val_loss: 0.2912 - val_acc: 0.4989\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.2829 - acc: 0.5139 - val_loss: 0.2878 - val_acc: 0.5002\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.2802 - acc: 0.5151 - val_loss: 0.2873 - val_acc: 0.5016\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.2803 - acc: 0.5139 - val_loss: 0.2881 - val_acc: 0.5015\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 3s 60us/step - loss: 0.2818 - acc: 0.5154 - val_loss: 0.2890 - val_acc: 0.5020\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.2833 - acc: 0.5176 - val_loss: 0.2895 - val_acc: 0.5026\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.2840 - acc: 0.5202 - val_loss: 0.2893 - val_acc: 0.5030\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.2839 - acc: 0.5220 - val_loss: 0.2885 - val_acc: 0.5028\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 0.2830 - acc: 0.5222 - val_loss: 0.2871 - val_acc: 0.5028\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 3s 59us/step - loss: 0.2816 - acc: 0.5226 - val_loss: 0.2855 - val_acc: 0.5035\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.2798 - acc: 0.5227 - val_loss: 0.2839 - val_acc: 0.5030\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.2778 - acc: 0.5225 - val_loss: 0.2824 - val_acc: 0.5025\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.2762 - acc: 0.5238 - val_loss: 0.2813 - val_acc: 0.5008\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.2750 - acc: 0.5255 - val_loss: 0.2807 - val_acc: 0.5003\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.2742 - acc: 0.5254 - val_loss: 0.2804 - val_acc: 0.5004\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.2739 - acc: 0.5212 - val_loss: 0.2805 - val_acc: 0.4999\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 218us/step - loss: 0.4526 - acc: 0.5176 - val_loss: 0.4396 - val_acc: 0.4991\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.4297 - acc: 0.5095 - val_loss: 0.4220 - val_acc: 0.4992\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.4126 - acc: 0.5062 - val_loss: 0.4105 - val_acc: 0.4990\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4016 - acc: 0.5047 - val_loss: 0.4016 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 49us/step - loss: 0.3933 - acc: 0.5039 - val_loss: 0.3958 - val_acc: 0.4990\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3880 - acc: 0.5035 - val_loss: 0.3926 - val_acc: 0.4990\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3851 - acc: 0.5033 - val_loss: 0.3910 - val_acc: 0.4990\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3838 - acc: 0.5033 - val_loss: 0.3907 - val_acc: 0.4990\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3837 - acc: 0.5033 - val_loss: 0.3913 - val_acc: 0.4990\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3844 - acc: 0.5033 - val_loss: 0.3923 - val_acc: 0.4990\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3855 - acc: 0.5033 - val_loss: 0.3934 - val_acc: 0.4990\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3867 - acc: 0.5033 - val_loss: 0.3943 - val_acc: 0.4990\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3877 - acc: 0.5033 - val_loss: 0.3950 - val_acc: 0.4990\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3884 - acc: 0.5033 - val_loss: 0.3952 - val_acc: 0.4990\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3886 - acc: 0.5033 - val_loss: 0.3951 - val_acc: 0.4990\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3884 - acc: 0.5033 - val_loss: 0.3946 - val_acc: 0.4990\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.3877 - acc: 0.5033 - val_loss: 0.3938 - val_acc: 0.4990\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3868 - acc: 0.5033 - val_loss: 0.3929 - val_acc: 0.4990\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3857 - acc: 0.5034 - val_loss: 0.3917 - val_acc: 0.4990\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3843 - acc: 0.5034 - val_loss: 0.3906 - val_acc: 0.4990\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.3925 - acc: 0.5035 - val_loss: 0.3999 - val_acc: 0.4991\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3911 - acc: 0.5038 - val_loss: 0.3994 - val_acc: 0.4990\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3898 - acc: 0.5041 - val_loss: 0.3989 - val_acc: 0.4990\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3885 - acc: 0.5044 - val_loss: 0.3984 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3872 - acc: 0.5047 - val_loss: 0.3981 - val_acc: 0.4989\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3864 - acc: 0.5053 - val_loss: 0.3982 - val_acc: 0.4990\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3860 - acc: 0.5058 - val_loss: 0.3987 - val_acc: 0.4990\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3859 - acc: 0.5066 - val_loss: 0.3994 - val_acc: 0.4991\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3862 - acc: 0.5074 - val_loss: 0.3996 - val_acc: 0.4992\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3862 - acc: 0.5079 - val_loss: 0.3995 - val_acc: 0.4992\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3859 - acc: 0.5079 - val_loss: 0.3992 - val_acc: 0.4991\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3855 - acc: 0.5078 - val_loss: 0.3989 - val_acc: 0.4990\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3851 - acc: 0.5072 - val_loss: 0.3986 - val_acc: 0.4992\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3847 - acc: 0.5072 - val_loss: 0.3984 - val_acc: 0.4992\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3844 - acc: 0.5069 - val_loss: 0.3982 - val_acc: 0.4992\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3843 - acc: 0.5067 - val_loss: 0.3980 - val_acc: 0.4992\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3841 - acc: 0.5064 - val_loss: 0.3979 - val_acc: 0.4992\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3841 - acc: 0.5062 - val_loss: 0.3978 - val_acc: 0.4991\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3840 - acc: 0.5061 - val_loss: 0.3979 - val_acc: 0.4992\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3838 - acc: 0.5061 - val_loss: 0.3980 - val_acc: 0.4991\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 208us/step - loss: 0.3719 - acc: 0.5065 - val_loss: 0.3872 - val_acc: 0.4997\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3716 - acc: 0.5101 - val_loss: 0.3867 - val_acc: 0.4995\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3711 - acc: 0.5098 - val_loss: 0.3853 - val_acc: 0.4993\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3704 - acc: 0.5082 - val_loss: 0.3849 - val_acc: 0.4990\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3706 - acc: 0.5072 - val_loss: 0.3850 - val_acc: 0.4991\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3708 - acc: 0.5067 - val_loss: 0.3849 - val_acc: 0.4991\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3704 - acc: 0.5073 - val_loss: 0.3852 - val_acc: 0.4992\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3701 - acc: 0.5082 - val_loss: 0.3859 - val_acc: 0.4994\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3701 - acc: 0.5092 - val_loss: 0.3862 - val_acc: 0.4995\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3701 - acc: 0.5097 - val_loss: 0.3860 - val_acc: 0.4993\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3698 - acc: 0.5094 - val_loss: 0.3855 - val_acc: 0.4993\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3696 - acc: 0.5089 - val_loss: 0.3852 - val_acc: 0.4993\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3696 - acc: 0.5081 - val_loss: 0.3852 - val_acc: 0.4992\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3696 - acc: 0.5078 - val_loss: 0.3852 - val_acc: 0.4992\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3694 - acc: 0.5079 - val_loss: 0.3853 - val_acc: 0.4992\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3691 - acc: 0.5085 - val_loss: 0.3857 - val_acc: 0.4994\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3691 - acc: 0.5091 - val_loss: 0.3861 - val_acc: 0.4995\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3690 - acc: 0.5098 - val_loss: 0.3862 - val_acc: 0.4996\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3689 - acc: 0.5099 - val_loss: 0.3860 - val_acc: 0.4995\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3687 - acc: 0.5096 - val_loss: 0.3857 - val_acc: 0.4994\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 0.3533 - acc: 0.5091 - val_loss: 0.3703 - val_acc: 0.4999\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3526 - acc: 0.5154 - val_loss: 0.3699 - val_acc: 0.5003\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3523 - acc: 0.5149 - val_loss: 0.3682 - val_acc: 0.4999\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3511 - acc: 0.5123 - val_loss: 0.3674 - val_acc: 0.4993\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3512 - acc: 0.5096 - val_loss: 0.3674 - val_acc: 0.4992\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3516 - acc: 0.5085 - val_loss: 0.3672 - val_acc: 0.4992\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3514 - acc: 0.5085 - val_loss: 0.3672 - val_acc: 0.4993\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3511 - acc: 0.5091 - val_loss: 0.3677 - val_acc: 0.4994\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3509 - acc: 0.5103 - val_loss: 0.3682 - val_acc: 0.4996\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3511 - acc: 0.5115 - val_loss: 0.3684 - val_acc: 0.4998\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3509 - acc: 0.5119 - val_loss: 0.3681 - val_acc: 0.4996\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3506 - acc: 0.5118 - val_loss: 0.3677 - val_acc: 0.4995\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3503 - acc: 0.5113 - val_loss: 0.3677 - val_acc: 0.4994\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3503 - acc: 0.5108 - val_loss: 0.3677 - val_acc: 0.4994\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3503 - acc: 0.5108 - val_loss: 0.3679 - val_acc: 0.4996\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3501 - acc: 0.5114 - val_loss: 0.3682 - val_acc: 0.4997\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3499 - acc: 0.5122 - val_loss: 0.3685 - val_acc: 0.4998\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3499 - acc: 0.5131 - val_loss: 0.3689 - val_acc: 0.4999\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3498 - acc: 0.5141 - val_loss: 0.3689 - val_acc: 0.4999\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3497 - acc: 0.5142 - val_loss: 0.3687 - val_acc: 0.5000\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 206us/step - loss: 0.3704 - acc: 0.5136 - val_loss: 0.3862 - val_acc: 0.4991\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3684 - acc: 0.5074 - val_loss: 0.3867 - val_acc: 0.4992\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3698 - acc: 0.5064 - val_loss: 0.3866 - val_acc: 0.4991\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3689 - acc: 0.5073 - val_loss: 0.3867 - val_acc: 0.4994\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3675 - acc: 0.5096 - val_loss: 0.3883 - val_acc: 0.4997\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3676 - acc: 0.5123 - val_loss: 0.3901 - val_acc: 0.5001\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3683 - acc: 0.5141 - val_loss: 0.3899 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3682 - acc: 0.5142 - val_loss: 0.3887 - val_acc: 0.4998\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3674 - acc: 0.5128 - val_loss: 0.3875 - val_acc: 0.4996\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3670 - acc: 0.5113 - val_loss: 0.3872 - val_acc: 0.4993\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3674 - acc: 0.5099 - val_loss: 0.3872 - val_acc: 0.4993\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3675 - acc: 0.5094 - val_loss: 0.3872 - val_acc: 0.4993\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3672 - acc: 0.5097 - val_loss: 0.3874 - val_acc: 0.4994\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3668 - acc: 0.5105 - val_loss: 0.3879 - val_acc: 0.4998\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3666 - acc: 0.5116 - val_loss: 0.3887 - val_acc: 0.4998\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3667 - acc: 0.5129 - val_loss: 0.3892 - val_acc: 0.5001\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3667 - acc: 0.5134 - val_loss: 0.3890 - val_acc: 0.4999\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3666 - acc: 0.5131 - val_loss: 0.3883 - val_acc: 0.4996\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3662 - acc: 0.5123 - val_loss: 0.3877 - val_acc: 0.4996\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3662 - acc: 0.5113 - val_loss: 0.3875 - val_acc: 0.4994\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 0.3854 - acc: 0.5109 - val_loss: 0.4046 - val_acc: 0.4992\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3857 - acc: 0.5064 - val_loss: 0.4048 - val_acc: 0.4992\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3852 - acc: 0.5071 - val_loss: 0.4058 - val_acc: 0.4993\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3836 - acc: 0.5093 - val_loss: 0.4087 - val_acc: 0.4997\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3840 - acc: 0.5127 - val_loss: 0.4102 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3844 - acc: 0.5138 - val_loss: 0.4087 - val_acc: 0.4998\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3837 - acc: 0.5130 - val_loss: 0.4071 - val_acc: 0.4996\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3833 - acc: 0.5114 - val_loss: 0.4063 - val_acc: 0.4993\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3836 - acc: 0.5101 - val_loss: 0.4062 - val_acc: 0.4992\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3836 - acc: 0.5097 - val_loss: 0.4063 - val_acc: 0.4993\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3833 - acc: 0.5101 - val_loss: 0.4069 - val_acc: 0.4996\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3828 - acc: 0.5111 - val_loss: 0.4082 - val_acc: 0.4998\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3828 - acc: 0.5120 - val_loss: 0.4089 - val_acc: 0.4997\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3830 - acc: 0.5126 - val_loss: 0.4087 - val_acc: 0.4997\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3828 - acc: 0.5122 - val_loss: 0.4077 - val_acc: 0.4997\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3824 - acc: 0.5116 - val_loss: 0.4068 - val_acc: 0.4996\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3823 - acc: 0.5108 - val_loss: 0.4064 - val_acc: 0.4994\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3823 - acc: 0.5104 - val_loss: 0.4066 - val_acc: 0.4994\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3823 - acc: 0.5105 - val_loss: 0.4069 - val_acc: 0.4997\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3820 - acc: 0.5110 - val_loss: 0.4077 - val_acc: 0.4997\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 221us/step - loss: 0.3462 - acc: 0.5119 - val_loss: 0.3579 - val_acc: 0.5008\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3352 - acc: 0.5201 - val_loss: 0.3587 - val_acc: 0.5005\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3347 - acc: 0.5268 - val_loss: 0.3598 - val_acc: 0.5002\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3354 - acc: 0.5299 - val_loss: 0.3577 - val_acc: 0.5001\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3335 - acc: 0.5306 - val_loss: 0.3545 - val_acc: 0.5006\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3308 - acc: 0.5262 - val_loss: 0.3524 - val_acc: 0.5005\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3296 - acc: 0.5211 - val_loss: 0.3516 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3301 - acc: 0.5168 - val_loss: 0.3516 - val_acc: 0.4997\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3308 - acc: 0.5140 - val_loss: 0.3516 - val_acc: 0.4995\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3313 - acc: 0.5126 - val_loss: 0.3513 - val_acc: 0.4994\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3313 - acc: 0.5126 - val_loss: 0.3511 - val_acc: 0.4992\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3308 - acc: 0.5139 - val_loss: 0.3512 - val_acc: 0.4995\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3305 - acc: 0.5153 - val_loss: 0.3516 - val_acc: 0.4998\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3303 - acc: 0.5171 - val_loss: 0.3520 - val_acc: 0.4999\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3303 - acc: 0.5190 - val_loss: 0.3523 - val_acc: 0.5003\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3303 - acc: 0.5204 - val_loss: 0.3523 - val_acc: 0.5001\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3301 - acc: 0.5211 - val_loss: 0.3520 - val_acc: 0.5001\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3296 - acc: 0.5207 - val_loss: 0.3516 - val_acc: 0.5002\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3291 - acc: 0.5197 - val_loss: 0.3513 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3288 - acc: 0.5186 - val_loss: 0.3511 - val_acc: 0.4999\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 0.2775 - acc: 0.5181 - val_loss: 0.2786 - val_acc: 0.5004\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2618 - acc: 0.5346 - val_loss: 0.2710 - val_acc: 0.5012\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.2553 - acc: 0.5437 - val_loss: 0.2695 - val_acc: 0.5023\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.2551 - acc: 0.5464 - val_loss: 0.2700 - val_acc: 0.5021\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.2567 - acc: 0.5485 - val_loss: 0.2703 - val_acc: 0.5043\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.2571 - acc: 0.5464 - val_loss: 0.2693 - val_acc: 0.5050\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.2560 - acc: 0.5492 - val_loss: 0.2676 - val_acc: 0.5049\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.2537 - acc: 0.5533 - val_loss: 0.2657 - val_acc: 0.5035\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2512 - acc: 0.5607 - val_loss: 0.2646 - val_acc: 0.5039\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.2497 - acc: 0.5670 - val_loss: 0.2645 - val_acc: 0.5036\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.2493 - acc: 0.5667 - val_loss: 0.2651 - val_acc: 0.5028\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.2499 - acc: 0.5612 - val_loss: 0.2659 - val_acc: 0.5019\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2508 - acc: 0.5562 - val_loss: 0.2664 - val_acc: 0.5008\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2514 - acc: 0.5531 - val_loss: 0.2663 - val_acc: 0.5017\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2514 - acc: 0.5541 - val_loss: 0.2657 - val_acc: 0.5026\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.2510 - acc: 0.5562 - val_loss: 0.2651 - val_acc: 0.5039\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.2502 - acc: 0.5611 - val_loss: 0.2644 - val_acc: 0.5032\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.2496 - acc: 0.5664 - val_loss: 0.2639 - val_acc: 0.5025\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2493 - acc: 0.5713 - val_loss: 0.2639 - val_acc: 0.5049\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.2492 - acc: 0.5727 - val_loss: 0.2639 - val_acc: 0.5048\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 0.4121 - acc: 0.5730 - val_loss: 0.4019 - val_acc: 0.5014\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3796 - acc: 0.5532 - val_loss: 0.3811 - val_acc: 0.4995\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3608 - acc: 0.5239 - val_loss: 0.3701 - val_acc: 0.4992\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3518 - acc: 0.5098 - val_loss: 0.3657 - val_acc: 0.4992\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3491 - acc: 0.5058 - val_loss: 0.3652 - val_acc: 0.4991\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3497 - acc: 0.5046 - val_loss: 0.3665 - val_acc: 0.4991\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3518 - acc: 0.5042 - val_loss: 0.3681 - val_acc: 0.4991\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3538 - acc: 0.5040 - val_loss: 0.3692 - val_acc: 0.4991\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3552 - acc: 0.5040 - val_loss: 0.3696 - val_acc: 0.4991\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3553 - acc: 0.5041 - val_loss: 0.3690 - val_acc: 0.4992\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3543 - acc: 0.5044 - val_loss: 0.3677 - val_acc: 0.4992\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3523 - acc: 0.5048 - val_loss: 0.3659 - val_acc: 0.4994\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3494 - acc: 0.5054 - val_loss: 0.3639 - val_acc: 0.4993\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3462 - acc: 0.5062 - val_loss: 0.3623 - val_acc: 0.4991\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3433 - acc: 0.5079 - val_loss: 0.3613 - val_acc: 0.4992\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3409 - acc: 0.5109 - val_loss: 0.3614 - val_acc: 0.4993\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3396 - acc: 0.5144 - val_loss: 0.3624 - val_acc: 0.4998\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3396 - acc: 0.5176 - val_loss: 0.3641 - val_acc: 0.4999\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3404 - acc: 0.5208 - val_loss: 0.3658 - val_acc: 0.5004\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3418 - acc: 0.5237 - val_loss: 0.3670 - val_acc: 0.5007\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 0.4780 - acc: 0.5253 - val_loss: 0.4705 - val_acc: 0.4998\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.4372 - acc: 0.5156 - val_loss: 0.4435 - val_acc: 0.4992\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.4165 - acc: 0.5090 - val_loss: 0.4302 - val_acc: 0.4994\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.4082 - acc: 0.5061 - val_loss: 0.4246 - val_acc: 0.4992\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.4063 - acc: 0.5049 - val_loss: 0.4233 - val_acc: 0.4991\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.4075 - acc: 0.5042 - val_loss: 0.4238 - val_acc: 0.4991\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.4097 - acc: 0.5040 - val_loss: 0.4251 - val_acc: 0.4991\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.4121 - acc: 0.5038 - val_loss: 0.4264 - val_acc: 0.4991\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.4141 - acc: 0.5037 - val_loss: 0.4274 - val_acc: 0.4991\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.4155 - acc: 0.5037 - val_loss: 0.4280 - val_acc: 0.4991\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.4162 - acc: 0.5037 - val_loss: 0.4282 - val_acc: 0.4991\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.4162 - acc: 0.5037 - val_loss: 0.4279 - val_acc: 0.4991\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.4157 - acc: 0.5039 - val_loss: 0.4272 - val_acc: 0.4991\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.4146 - acc: 0.5040 - val_loss: 0.4264 - val_acc: 0.4992\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.4131 - acc: 0.5042 - val_loss: 0.4255 - val_acc: 0.4992\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.4112 - acc: 0.5044 - val_loss: 0.4246 - val_acc: 0.4991\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4092 - acc: 0.5047 - val_loss: 0.4239 - val_acc: 0.4992\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.4072 - acc: 0.5050 - val_loss: 0.4236 - val_acc: 0.4992\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4053 - acc: 0.5056 - val_loss: 0.4238 - val_acc: 0.4994\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.4038 - acc: 0.5065 - val_loss: 0.4248 - val_acc: 0.4992\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 0.3584 - acc: 0.5072 - val_loss: 0.3595 - val_acc: 0.4993\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3416 - acc: 0.5112 - val_loss: 0.3498 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3299 - acc: 0.5155 - val_loss: 0.3428 - val_acc: 0.4995\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3215 - acc: 0.5213 - val_loss: 0.3415 - val_acc: 0.4998\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3198 - acc: 0.5267 - val_loss: 0.3433 - val_acc: 0.4999\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3217 - acc: 0.5294 - val_loss: 0.3447 - val_acc: 0.4998\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3233 - acc: 0.5320 - val_loss: 0.3443 - val_acc: 0.5003\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3230 - acc: 0.5345 - val_loss: 0.3423 - val_acc: 0.5005\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3210 - acc: 0.5354 - val_loss: 0.3394 - val_acc: 0.5002\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3182 - acc: 0.5345 - val_loss: 0.3367 - val_acc: 0.5009\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3157 - acc: 0.5309 - val_loss: 0.3347 - val_acc: 0.5006\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3144 - acc: 0.5261 - val_loss: 0.3338 - val_acc: 0.5008\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3140 - acc: 0.5217 - val_loss: 0.3337 - val_acc: 0.5001\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3145 - acc: 0.5182 - val_loss: 0.3339 - val_acc: 0.4996\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3154 - acc: 0.5149 - val_loss: 0.3342 - val_acc: 0.4993\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3161 - acc: 0.5132 - val_loss: 0.3345 - val_acc: 0.4992\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3165 - acc: 0.5122 - val_loss: 0.3345 - val_acc: 0.4992\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3166 - acc: 0.5120 - val_loss: 0.3343 - val_acc: 0.4990\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3164 - acc: 0.5123 - val_loss: 0.3340 - val_acc: 0.4991\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3159 - acc: 0.5131 - val_loss: 0.3340 - val_acc: 0.4995\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 227us/step - loss: 0.3444 - acc: 0.5146 - val_loss: 0.3611 - val_acc: 0.4994\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3414 - acc: 0.5103 - val_loss: 0.3604 - val_acc: 0.4995\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3403 - acc: 0.5105 - val_loss: 0.3604 - val_acc: 0.4994\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3391 - acc: 0.5130 - val_loss: 0.3613 - val_acc: 0.5001\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3383 - acc: 0.5167 - val_loss: 0.3630 - val_acc: 0.5002\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 45us/step - loss: 0.3386 - acc: 0.5205 - val_loss: 0.3643 - val_acc: 0.5006\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3389 - acc: 0.5221 - val_loss: 0.3643 - val_acc: 0.5007\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3386 - acc: 0.5218 - val_loss: 0.3638 - val_acc: 0.5006\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3382 - acc: 0.5203 - val_loss: 0.3633 - val_acc: 0.5004\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3381 - acc: 0.5191 - val_loss: 0.3630 - val_acc: 0.5005\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3376 - acc: 0.5191 - val_loss: 0.3628 - val_acc: 0.5008\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3371 - acc: 0.5197 - val_loss: 0.3630 - val_acc: 0.5005\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3369 - acc: 0.5213 - val_loss: 0.3633 - val_acc: 0.5004\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3367 - acc: 0.5225 - val_loss: 0.3635 - val_acc: 0.5005\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3366 - acc: 0.5225 - val_loss: 0.3632 - val_acc: 0.5007\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3364 - acc: 0.5213 - val_loss: 0.3628 - val_acc: 0.5003\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3362 - acc: 0.5200 - val_loss: 0.3626 - val_acc: 0.5004\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3362 - acc: 0.5190 - val_loss: 0.3626 - val_acc: 0.5003\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3360 - acc: 0.5188 - val_loss: 0.3628 - val_acc: 0.5002\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3357 - acc: 0.5197 - val_loss: 0.3632 - val_acc: 0.5002\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 0.3210 - acc: 0.5216 - val_loss: 0.3511 - val_acc: 0.5008\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3228 - acc: 0.5372 - val_loss: 0.3481 - val_acc: 0.5004\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3204 - acc: 0.5319 - val_loss: 0.3464 - val_acc: 0.5007\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3201 - acc: 0.5240 - val_loss: 0.3464 - val_acc: 0.5001\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3209 - acc: 0.5204 - val_loss: 0.3462 - val_acc: 0.5002\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3206 - acc: 0.5209 - val_loss: 0.3460 - val_acc: 0.5007\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3198 - acc: 0.5242 - val_loss: 0.3469 - val_acc: 0.5009\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3196 - acc: 0.5288 - val_loss: 0.3479 - val_acc: 0.5008\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3200 - acc: 0.5323 - val_loss: 0.3480 - val_acc: 0.5007\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3199 - acc: 0.5328 - val_loss: 0.3472 - val_acc: 0.5008\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3193 - acc: 0.5301 - val_loss: 0.3465 - val_acc: 0.5007\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3191 - acc: 0.5271 - val_loss: 0.3462 - val_acc: 0.5005\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3191 - acc: 0.5251 - val_loss: 0.3462 - val_acc: 0.5004\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3193 - acc: 0.5243 - val_loss: 0.3464 - val_acc: 0.5006\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3190 - acc: 0.5253 - val_loss: 0.3467 - val_acc: 0.5007\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3187 - acc: 0.5272 - val_loss: 0.3472 - val_acc: 0.5007\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3186 - acc: 0.5296 - val_loss: 0.3478 - val_acc: 0.5007\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3186 - acc: 0.5321 - val_loss: 0.3479 - val_acc: 0.5009\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3186 - acc: 0.5325 - val_loss: 0.3476 - val_acc: 0.5006\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3183 - acc: 0.5313 - val_loss: 0.3472 - val_acc: 0.5006\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 228us/step - loss: 0.3088 - acc: 0.5291 - val_loss: 0.3422 - val_acc: 0.5006\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3128 - acc: 0.5495 - val_loss: 0.3372 - val_acc: 0.5006\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 0.3085 - acc: 0.5383 - val_loss: 0.3361 - val_acc: 0.5007\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3088 - acc: 0.5268 - val_loss: 0.3367 - val_acc: 0.5003\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3102 - acc: 0.5234 - val_loss: 0.3364 - val_acc: 0.5003\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3095 - acc: 0.5247 - val_loss: 0.3360 - val_acc: 0.5006\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3083 - acc: 0.5297 - val_loss: 0.3367 - val_acc: 0.5004\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3081 - acc: 0.5366 - val_loss: 0.3379 - val_acc: 0.5004\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3087 - acc: 0.5422 - val_loss: 0.3381 - val_acc: 0.5010\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3088 - acc: 0.5429 - val_loss: 0.3372 - val_acc: 0.5008\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3082 - acc: 0.5395 - val_loss: 0.3364 - val_acc: 0.5009\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3076 - acc: 0.5346 - val_loss: 0.3359 - val_acc: 0.5010\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3076 - acc: 0.5308 - val_loss: 0.3361 - val_acc: 0.5010\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3079 - acc: 0.5287 - val_loss: 0.3364 - val_acc: 0.5009\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3079 - acc: 0.5289 - val_loss: 0.3365 - val_acc: 0.5007\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3076 - acc: 0.5316 - val_loss: 0.3369 - val_acc: 0.5007\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3072 - acc: 0.5349 - val_loss: 0.3372 - val_acc: 0.5004\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3071 - acc: 0.5382 - val_loss: 0.3376 - val_acc: 0.5005\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3072 - acc: 0.5402 - val_loss: 0.3376 - val_acc: 0.5009\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3071 - acc: 0.5403 - val_loss: 0.3372 - val_acc: 0.5009\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 0.4316 - acc: 0.5383 - val_loss: 0.4359 - val_acc: 0.5001\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3970 - acc: 0.5181 - val_loss: 0.4170 - val_acc: 0.4993\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3867 - acc: 0.5091 - val_loss: 0.4116 - val_acc: 0.4994\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3866 - acc: 0.5066 - val_loss: 0.4114 - val_acc: 0.4995\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.3896 - acc: 0.5054 - val_loss: 0.4126 - val_acc: 0.4994\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.3925 - acc: 0.5051 - val_loss: 0.4138 - val_acc: 0.4994\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3941 - acc: 0.5049 - val_loss: 0.4143 - val_acc: 0.4993\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3943 - acc: 0.5053 - val_loss: 0.4141 - val_acc: 0.4995\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3931 - acc: 0.5059 - val_loss: 0.4136 - val_acc: 0.4995\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3908 - acc: 0.5072 - val_loss: 0.4134 - val_acc: 0.4995\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3880 - acc: 0.5089 - val_loss: 0.4139 - val_acc: 0.4991\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3854 - acc: 0.5106 - val_loss: 0.4151 - val_acc: 0.4993\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3834 - acc: 0.5133 - val_loss: 0.4170 - val_acc: 0.4997\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3822 - acc: 0.5159 - val_loss: 0.4193 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3815 - acc: 0.5181 - val_loss: 0.4214 - val_acc: 0.5005\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3816 - acc: 0.5203 - val_loss: 0.4233 - val_acc: 0.5008\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3823 - acc: 0.5218 - val_loss: 0.4243 - val_acc: 0.5009\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3830 - acc: 0.5223 - val_loss: 0.4238 - val_acc: 0.5010\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3831 - acc: 0.5219 - val_loss: 0.4219 - val_acc: 0.5010\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3823 - acc: 0.5211 - val_loss: 0.4191 - val_acc: 0.5009\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 219us/step - loss: 0.3786 - acc: 0.5197 - val_loss: 0.4111 - val_acc: 0.4994\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3798 - acc: 0.5130 - val_loss: 0.4136 - val_acc: 0.4998\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3782 - acc: 0.5159 - val_loss: 0.4170 - val_acc: 0.5004\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 3s 51us/step - loss: 0.3781 - acc: 0.5193 - val_loss: 0.4173 - val_acc: 0.5005\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 3s 52us/step - loss: 0.3776 - acc: 0.5199 - val_loss: 0.4147 - val_acc: 0.5004\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 0.3767 - acc: 0.5187 - val_loss: 0.4125 - val_acc: 0.5001\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 3s 53us/step - loss: 0.3767 - acc: 0.5164 - val_loss: 0.4116 - val_acc: 0.5002\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 0.3770 - acc: 0.5152 - val_loss: 0.4115 - val_acc: 0.5001\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 0.3770 - acc: 0.5151 - val_loss: 0.4121 - val_acc: 0.5001\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 3s 63us/step - loss: 0.3764 - acc: 0.5161 - val_loss: 0.4136 - val_acc: 0.5001\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 47us/step - loss: 0.3758 - acc: 0.5179 - val_loss: 0.4153 - val_acc: 0.5003\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 3s 51us/step - loss: 0.3755 - acc: 0.5194 - val_loss: 0.4164 - val_acc: 0.5004\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3755 - acc: 0.5200 - val_loss: 0.4160 - val_acc: 0.5004\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3755 - acc: 0.5197 - val_loss: 0.4151 - val_acc: 0.5004\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3753 - acc: 0.5189 - val_loss: 0.4145 - val_acc: 0.5003\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3752 - acc: 0.5183 - val_loss: 0.4144 - val_acc: 0.5002\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3748 - acc: 0.5187 - val_loss: 0.4152 - val_acc: 0.5003\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3745 - acc: 0.5196 - val_loss: 0.4165 - val_acc: 0.5002\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3745 - acc: 0.5211 - val_loss: 0.4171 - val_acc: 0.5004\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3745 - acc: 0.5217 - val_loss: 0.4167 - val_acc: 0.5002\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 223us/step - loss: 0.3643 - acc: 0.5212 - val_loss: 0.4183 - val_acc: 0.4998\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3699 - acc: 0.5332 - val_loss: 0.4062 - val_acc: 0.5003\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3639 - acc: 0.5247 - val_loss: 0.4017 - val_acc: 0.5001\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3650 - acc: 0.5178 - val_loss: 0.4012 - val_acc: 0.4998\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3662 - acc: 0.5162 - val_loss: 0.4015 - val_acc: 0.5002\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3654 - acc: 0.5176 - val_loss: 0.4029 - val_acc: 0.5002\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3645 - acc: 0.5206 - val_loss: 0.4051 - val_acc: 0.5004\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3642 - acc: 0.5235 - val_loss: 0.4067 - val_acc: 0.5004\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3640 - acc: 0.5254 - val_loss: 0.4068 - val_acc: 0.5008\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3638 - acc: 0.5254 - val_loss: 0.4059 - val_acc: 0.5005\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3637 - acc: 0.5241 - val_loss: 0.4046 - val_acc: 0.5003\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3637 - acc: 0.5220 - val_loss: 0.4035 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3637 - acc: 0.5205 - val_loss: 0.4029 - val_acc: 0.5001\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.3636 - acc: 0.5198 - val_loss: 0.4028 - val_acc: 0.5003\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3632 - acc: 0.5202 - val_loss: 0.4033 - val_acc: 0.5004\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3627 - acc: 0.5218 - val_loss: 0.4046 - val_acc: 0.5004\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3626 - acc: 0.5237 - val_loss: 0.4062 - val_acc: 0.5008\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3628 - acc: 0.5256 - val_loss: 0.4069 - val_acc: 0.5009\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3630 - acc: 0.5263 - val_loss: 0.4063 - val_acc: 0.5006\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3626 - acc: 0.5260 - val_loss: 0.4053 - val_acc: 0.5005\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 228us/step - loss: 0.4012 - acc: 0.5245 - val_loss: 0.4325 - val_acc: 0.4995\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3939 - acc: 0.5123 - val_loss: 0.4297 - val_acc: 0.4993\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3972 - acc: 0.5099 - val_loss: 0.4302 - val_acc: 0.4993\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3977 - acc: 0.5101 - val_loss: 0.4315 - val_acc: 0.4993\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3955 - acc: 0.5119 - val_loss: 0.4345 - val_acc: 0.4998\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3930 - acc: 0.5154 - val_loss: 0.4401 - val_acc: 0.5006\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3921 - acc: 0.5198 - val_loss: 0.4466 - val_acc: 0.5005\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3938 - acc: 0.5234 - val_loss: 0.4484 - val_acc: 0.5007\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3946 - acc: 0.5245 - val_loss: 0.4451 - val_acc: 0.5007\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3931 - acc: 0.5229 - val_loss: 0.4403 - val_acc: 0.5005\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3918 - acc: 0.5202 - val_loss: 0.4367 - val_acc: 0.5001\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 48us/step - loss: 0.3918 - acc: 0.5178 - val_loss: 0.4349 - val_acc: 0.4999\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3925 - acc: 0.5162 - val_loss: 0.4344 - val_acc: 0.4998\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3929 - acc: 0.5156 - val_loss: 0.4346 - val_acc: 0.4999\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3926 - acc: 0.5157 - val_loss: 0.4359 - val_acc: 0.4999\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3919 - acc: 0.5166 - val_loss: 0.4378 - val_acc: 0.5002\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3913 - acc: 0.5183 - val_loss: 0.4403 - val_acc: 0.5003\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3913 - acc: 0.5201 - val_loss: 0.4424 - val_acc: 0.5005\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3916 - acc: 0.5213 - val_loss: 0.4430 - val_acc: 0.5004\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3916 - acc: 0.5218 - val_loss: 0.4419 - val_acc: 0.5003\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 0.3350 - acc: 0.5213 - val_loss: 0.3430 - val_acc: 0.5006\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3102 - acc: 0.5347 - val_loss: 0.3342 - val_acc: 0.5002\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.2998 - acc: 0.5430 - val_loss: 0.3321 - val_acc: 0.5013\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.2979 - acc: 0.5489 - val_loss: 0.3326 - val_acc: 0.5020\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.2988 - acc: 0.5525 - val_loss: 0.3318 - val_acc: 0.5010\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.2979 - acc: 0.5590 - val_loss: 0.3288 - val_acc: 0.5008\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.2942 - acc: 0.5651 - val_loss: 0.3245 - val_acc: 0.5013\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.2895 - acc: 0.5684 - val_loss: 0.3208 - val_acc: 0.5001\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.2863 - acc: 0.5652 - val_loss: 0.3190 - val_acc: 0.5012\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2855 - acc: 0.5581 - val_loss: 0.3187 - val_acc: 0.5011\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2866 - acc: 0.5496 - val_loss: 0.3190 - val_acc: 0.5006\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.2879 - acc: 0.5441 - val_loss: 0.3191 - val_acc: 0.5006\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.2888 - acc: 0.5412 - val_loss: 0.3189 - val_acc: 0.5006\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2889 - acc: 0.5416 - val_loss: 0.3184 - val_acc: 0.5010\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.2883 - acc: 0.5444 - val_loss: 0.3179 - val_acc: 0.5009\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2876 - acc: 0.5494 - val_loss: 0.3178 - val_acc: 0.5016\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.2871 - acc: 0.5561 - val_loss: 0.3181 - val_acc: 0.5013\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.2869 - acc: 0.5620 - val_loss: 0.3184 - val_acc: 0.5018\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.2868 - acc: 0.5670 - val_loss: 0.3186 - val_acc: 0.5023\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.2866 - acc: 0.5713 - val_loss: 0.3185 - val_acc: 0.5020\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 224us/step - loss: 0.3630 - acc: 0.5722 - val_loss: 0.3787 - val_acc: 0.5001\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3420 - acc: 0.5346 - val_loss: 0.3708 - val_acc: 0.4993\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3388 - acc: 0.5175 - val_loss: 0.3704 - val_acc: 0.4992\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3410 - acc: 0.5121 - val_loss: 0.3714 - val_acc: 0.4992\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3423 - acc: 0.5113 - val_loss: 0.3713 - val_acc: 0.4993\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3411 - acc: 0.5131 - val_loss: 0.3703 - val_acc: 0.4997\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3378 - acc: 0.5169 - val_loss: 0.3698 - val_acc: 0.4999\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3342 - acc: 0.5226 - val_loss: 0.3710 - val_acc: 0.5007\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3320 - acc: 0.5303 - val_loss: 0.3743 - val_acc: 0.5011\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3325 - acc: 0.5375 - val_loss: 0.3782 - val_acc: 0.5007\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3350 - acc: 0.5415 - val_loss: 0.3802 - val_acc: 0.5008\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3364 - acc: 0.5433 - val_loss: 0.3791 - val_acc: 0.5007\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3355 - acc: 0.5429 - val_loss: 0.3764 - val_acc: 0.5008\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3335 - acc: 0.5403 - val_loss: 0.3736 - val_acc: 0.5014\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3320 - acc: 0.5362 - val_loss: 0.3719 - val_acc: 0.5008\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.3318 - acc: 0.5320 - val_loss: 0.3711 - val_acc: 0.5005\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3325 - acc: 0.5285 - val_loss: 0.3710 - val_acc: 0.5006\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3332 - acc: 0.5263 - val_loss: 0.3708 - val_acc: 0.5005\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3333 - acc: 0.5259 - val_loss: 0.3706 - val_acc: 0.5006\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3328 - acc: 0.5265 - val_loss: 0.3706 - val_acc: 0.5009\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 220us/step - loss: 0.3145 - acc: 0.5282 - val_loss: 0.3486 - val_acc: 0.5001\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 0.3093 - acc: 0.5504 - val_loss: 0.3511 - val_acc: 0.5011\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3111 - acc: 0.5574 - val_loss: 0.3486 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3091 - acc: 0.5544 - val_loss: 0.3455 - val_acc: 0.5013\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3077 - acc: 0.5444 - val_loss: 0.3445 - val_acc: 0.5010\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3084 - acc: 0.5357 - val_loss: 0.3443 - val_acc: 0.5007\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3091 - acc: 0.5324 - val_loss: 0.3442 - val_acc: 0.5008\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3087 - acc: 0.5337 - val_loss: 0.3444 - val_acc: 0.5012\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3079 - acc: 0.5387 - val_loss: 0.3453 - val_acc: 0.5005\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3076 - acc: 0.5457 - val_loss: 0.3465 - val_acc: 0.5005\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3080 - acc: 0.5508 - val_loss: 0.3469 - val_acc: 0.5005\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3079 - acc: 0.5519 - val_loss: 0.3462 - val_acc: 0.5006\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3074 - acc: 0.5497 - val_loss: 0.3453 - val_acc: 0.5009\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3069 - acc: 0.5448 - val_loss: 0.3450 - val_acc: 0.5012\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3069 - acc: 0.5409 - val_loss: 0.3450 - val_acc: 0.5013\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3071 - acc: 0.5389 - val_loss: 0.3454 - val_acc: 0.5013\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3071 - acc: 0.5398 - val_loss: 0.3457 - val_acc: 0.5011\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3067 - acc: 0.5426 - val_loss: 0.3464 - val_acc: 0.5013\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3064 - acc: 0.5464 - val_loss: 0.3470 - val_acc: 0.5011\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3065 - acc: 0.5499 - val_loss: 0.3474 - val_acc: 0.5011\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 224us/step - loss: 0.3413 - acc: 0.5515 - val_loss: 0.3749 - val_acc: 0.5001\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3355 - acc: 0.5248 - val_loss: 0.3751 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3391 - acc: 0.5189 - val_loss: 0.3752 - val_acc: 0.5001\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3381 - acc: 0.5210 - val_loss: 0.3755 - val_acc: 0.5002\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3350 - acc: 0.5265 - val_loss: 0.3784 - val_acc: 0.5006\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3339 - acc: 0.5353 - val_loss: 0.3828 - val_acc: 0.5016\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3356 - acc: 0.5427 - val_loss: 0.3843 - val_acc: 0.5007\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3364 - acc: 0.5445 - val_loss: 0.3822 - val_acc: 0.5013\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3350 - acc: 0.5417 - val_loss: 0.3791 - val_acc: 0.5008\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3337 - acc: 0.5366 - val_loss: 0.3772 - val_acc: 0.5008\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3338 - acc: 0.5313 - val_loss: 0.3765 - val_acc: 0.5004\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3347 - acc: 0.5284 - val_loss: 0.3764 - val_acc: 0.5002\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3349 - acc: 0.5275 - val_loss: 0.3765 - val_acc: 0.5004\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3342 - acc: 0.5287 - val_loss: 0.3772 - val_acc: 0.5007\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3335 - acc: 0.5316 - val_loss: 0.3786 - val_acc: 0.5008\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3332 - acc: 0.5360 - val_loss: 0.3804 - val_acc: 0.5010\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3335 - acc: 0.5388 - val_loss: 0.3813 - val_acc: 0.5009\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3339 - acc: 0.5406 - val_loss: 0.3809 - val_acc: 0.5009\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 35us/step - loss: 0.3336 - acc: 0.5396 - val_loss: 0.3794 - val_acc: 0.5012\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 41us/step - loss: 0.3330 - acc: 0.5378 - val_loss: 0.3780 - val_acc: 0.5007\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 231us/step - loss: 0.3179 - acc: 0.5344 - val_loss: 0.3698 - val_acc: 0.5003\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3223 - acc: 0.5611 - val_loss: 0.3626 - val_acc: 0.5003\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3169 - acc: 0.5513 - val_loss: 0.3586 - val_acc: 0.5008\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3167 - acc: 0.5364 - val_loss: 0.3583 - val_acc: 0.5002\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3186 - acc: 0.5301 - val_loss: 0.3579 - val_acc: 0.5005\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3181 - acc: 0.5311 - val_loss: 0.3581 - val_acc: 0.5006\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3164 - acc: 0.5368 - val_loss: 0.3599 - val_acc: 0.5007\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 2s 46us/step - loss: 0.3162 - acc: 0.5460 - val_loss: 0.3623 - val_acc: 0.5006\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3171 - acc: 0.5528 - val_loss: 0.3628 - val_acc: 0.5003\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3173 - acc: 0.5540 - val_loss: 0.3613 - val_acc: 0.5007\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 2s 39us/step - loss: 0.3164 - acc: 0.5496 - val_loss: 0.3594 - val_acc: 0.5010\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 38us/step - loss: 0.3157 - acc: 0.5439 - val_loss: 0.3584 - val_acc: 0.5011\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 2s 43us/step - loss: 0.3159 - acc: 0.5381 - val_loss: 0.3584 - val_acc: 0.5006\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 42us/step - loss: 0.3162 - acc: 0.5358 - val_loss: 0.3585 - val_acc: 0.5005\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3162 - acc: 0.5364 - val_loss: 0.3591 - val_acc: 0.5014\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 40us/step - loss: 0.3156 - acc: 0.5395 - val_loss: 0.3601 - val_acc: 0.5012\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3152 - acc: 0.5448 - val_loss: 0.3615 - val_acc: 0.5005\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 2s 37us/step - loss: 0.3154 - acc: 0.5493 - val_loss: 0.3623 - val_acc: 0.5006\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3157 - acc: 0.5512 - val_loss: 0.3618 - val_acc: 0.5009\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 2s 44us/step - loss: 0.3154 - acc: 0.5503 - val_loss: 0.3608 - val_acc: 0.5010\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 11s 222us/step - loss: 0.3665 - acc: 0.5475 - val_loss: 0.4007 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3560 - acc: 0.5222 - val_loss: 0.3982 - val_acc: 0.4994\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 34us/step - loss: 0.3601 - acc: 0.5148 - val_loss: 0.3988 - val_acc: 0.4996\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 2s 36us/step - loss: 0.3611 - acc: 0.5149 - val_loss: 0.3990 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3582 - acc: 0.5187 - val_loss: 0.4009 - val_acc: 0.5003\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3547 - acc: 0.5251 - val_loss: 0.4063 - val_acc: 0.5008\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3540 - acc: 0.5324 - val_loss: 0.4130 - val_acc: 0.5007\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3562 - acc: 0.5388 - val_loss: 0.4154 - val_acc: 0.5006\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3572 - acc: 0.5408 - val_loss: 0.4126 - val_acc: 0.5008\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3555 - acc: 0.5388 - val_loss: 0.4077 - val_acc: 0.5013\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3538 - acc: 0.5337 - val_loss: 0.4040 - val_acc: 0.5008\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3535 - acc: 0.5293 - val_loss: 0.4022 - val_acc: 0.5003\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3544 - acc: 0.5264 - val_loss: 0.4014 - val_acc: 0.5002\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3550 - acc: 0.5251 - val_loss: 0.4014 - val_acc: 0.5002\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3548 - acc: 0.5253 - val_loss: 0.4020 - val_acc: 0.5002\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3540 - acc: 0.5267 - val_loss: 0.4036 - val_acc: 0.5009\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3531 - acc: 0.5292 - val_loss: 0.4061 - val_acc: 0.5013\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3530 - acc: 0.5320 - val_loss: 0.4085 - val_acc: 0.5011\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3535 - acc: 0.5353 - val_loss: 0.4097 - val_acc: 0.5009\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3538 - acc: 0.5364 - val_loss: 0.4090 - val_acc: 0.5011\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 0.3091 - acc: 0.5356 - val_loss: 0.3480 - val_acc: 0.5007\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3020 - acc: 0.5600 - val_loss: 0.3501 - val_acc: 0.5015\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3034 - acc: 0.5686 - val_loss: 0.3466 - val_acc: 0.5010\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3003 - acc: 0.5692 - val_loss: 0.3425 - val_acc: 0.5011\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2980 - acc: 0.5600 - val_loss: 0.3408 - val_acc: 0.5009\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2988 - acc: 0.5495 - val_loss: 0.3406 - val_acc: 0.5003\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3000 - acc: 0.5433 - val_loss: 0.3403 - val_acc: 0.5003\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.2999 - acc: 0.5444 - val_loss: 0.3403 - val_acc: 0.5004\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2991 - acc: 0.5495 - val_loss: 0.3410 - val_acc: 0.5007\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2986 - acc: 0.5567 - val_loss: 0.3422 - val_acc: 0.5012\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2988 - acc: 0.5637 - val_loss: 0.3428 - val_acc: 0.5017\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.2989 - acc: 0.5670 - val_loss: 0.3423 - val_acc: 0.5013\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2983 - acc: 0.5655 - val_loss: 0.3414 - val_acc: 0.5004\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2976 - acc: 0.5612 - val_loss: 0.3406 - val_acc: 0.5009\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2972 - acc: 0.5559 - val_loss: 0.3405 - val_acc: 0.5010\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2974 - acc: 0.5520 - val_loss: 0.3406 - val_acc: 0.5011\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2976 - acc: 0.5505 - val_loss: 0.3411 - val_acc: 0.5009\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2974 - acc: 0.5520 - val_loss: 0.3416 - val_acc: 0.5008\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2973 - acc: 0.5554 - val_loss: 0.3423 - val_acc: 0.5001\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2971 - acc: 0.5598 - val_loss: 0.3430 - val_acc: 0.5002\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.2896 - acc: 0.5627 - val_loss: 0.3330 - val_acc: 0.5016\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2893 - acc: 0.5705 - val_loss: 0.3318 - val_acc: 0.5003\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2908 - acc: 0.5510 - val_loss: 0.3328 - val_acc: 0.5021\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2892 - acc: 0.5692 - val_loss: 0.3347 - val_acc: 0.5022\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2896 - acc: 0.5778 - val_loss: 0.3333 - val_acc: 0.5015\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2886 - acc: 0.5693 - val_loss: 0.3327 - val_acc: 0.5002\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2886 - acc: 0.5598 - val_loss: 0.3329 - val_acc: 0.5005\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2888 - acc: 0.5581 - val_loss: 0.3335 - val_acc: 0.5002\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2884 - acc: 0.5639 - val_loss: 0.3345 - val_acc: 0.5010\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2883 - acc: 0.5702 - val_loss: 0.3347 - val_acc: 0.5010\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2883 - acc: 0.5724 - val_loss: 0.3340 - val_acc: 0.5012\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2878 - acc: 0.5694 - val_loss: 0.3332 - val_acc: 0.5006\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2877 - acc: 0.5650 - val_loss: 0.3328 - val_acc: 0.5013\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2877 - acc: 0.5633 - val_loss: 0.3330 - val_acc: 0.5017\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2876 - acc: 0.5657 - val_loss: 0.3335 - val_acc: 0.5017\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2874 - acc: 0.5703 - val_loss: 0.3340 - val_acc: 0.5019\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2874 - acc: 0.5731 - val_loss: 0.3339 - val_acc: 0.5016\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2871 - acc: 0.5723 - val_loss: 0.3337 - val_acc: 0.5009\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.2869 - acc: 0.5685 - val_loss: 0.3336 - val_acc: 0.5010\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2869 - acc: 0.5668 - val_loss: 0.3338 - val_acc: 0.5012\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 0.2423 - acc: 0.5682 - val_loss: 0.2642 - val_acc: 0.5033\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2290 - acc: 0.5998 - val_loss: 0.2640 - val_acc: 0.5035\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2315 - acc: 0.5954 - val_loss: 0.2644 - val_acc: 0.5050\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2323 - acc: 0.5878 - val_loss: 0.2625 - val_acc: 0.5059\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.2291 - acc: 0.5957 - val_loss: 0.2605 - val_acc: 0.5048\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2258 - acc: 0.6120 - val_loss: 0.2607 - val_acc: 0.5025\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2255 - acc: 0.6174 - val_loss: 0.2622 - val_acc: 0.5020\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2273 - acc: 0.6124 - val_loss: 0.2630 - val_acc: 0.5018\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2284 - acc: 0.6090 - val_loss: 0.2624 - val_acc: 0.5022\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2280 - acc: 0.6123 - val_loss: 0.2610 - val_acc: 0.5017\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2266 - acc: 0.6162 - val_loss: 0.2601 - val_acc: 0.5045\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2257 - acc: 0.6150 - val_loss: 0.2600 - val_acc: 0.5067\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2258 - acc: 0.6114 - val_loss: 0.2603 - val_acc: 0.5060\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2263 - acc: 0.6063 - val_loss: 0.2605 - val_acc: 0.5044\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2264 - acc: 0.6063 - val_loss: 0.2602 - val_acc: 0.5053\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2259 - acc: 0.6102 - val_loss: 0.2598 - val_acc: 0.5054\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2251 - acc: 0.6149 - val_loss: 0.2598 - val_acc: 0.5035\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2246 - acc: 0.6206 - val_loss: 0.2601 - val_acc: 0.5026\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2246 - acc: 0.6216 - val_loss: 0.2606 - val_acc: 0.5028\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2250 - acc: 0.6186 - val_loss: 0.2609 - val_acc: 0.5029\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 199us/step - loss: 0.2388 - acc: 0.6191 - val_loss: 0.2794 - val_acc: 0.5031\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2403 - acc: 0.6087 - val_loss: 0.2779 - val_acc: 0.5033\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2387 - acc: 0.6220 - val_loss: 0.2784 - val_acc: 0.5026\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2396 - acc: 0.6055 - val_loss: 0.2779 - val_acc: 0.5031\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2390 - acc: 0.6148 - val_loss: 0.2778 - val_acc: 0.5029\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2386 - acc: 0.6239 - val_loss: 0.2778 - val_acc: 0.5021\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2381 - acc: 0.6243 - val_loss: 0.2782 - val_acc: 0.5019\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2383 - acc: 0.6179 - val_loss: 0.2786 - val_acc: 0.5027\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2385 - acc: 0.6158 - val_loss: 0.2783 - val_acc: 0.5020\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2380 - acc: 0.6183 - val_loss: 0.2781 - val_acc: 0.5027\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2376 - acc: 0.6227 - val_loss: 0.2781 - val_acc: 0.5021\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2375 - acc: 0.6236 - val_loss: 0.2781 - val_acc: 0.5034\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2375 - acc: 0.6208 - val_loss: 0.2783 - val_acc: 0.5028\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2376 - acc: 0.6169 - val_loss: 0.2783 - val_acc: 0.5036\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2373 - acc: 0.6188 - val_loss: 0.2783 - val_acc: 0.5029\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2370 - acc: 0.6224 - val_loss: 0.2785 - val_acc: 0.5030\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2369 - acc: 0.6255 - val_loss: 0.2787 - val_acc: 0.5025\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2369 - acc: 0.6252 - val_loss: 0.2788 - val_acc: 0.5034\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2368 - acc: 0.6223 - val_loss: 0.2788 - val_acc: 0.5031\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2366 - acc: 0.6213 - val_loss: 0.2786 - val_acc: 0.5029\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 0.3042 - acc: 0.6235 - val_loss: 0.3406 - val_acc: 0.5008\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2931 - acc: 0.5594 - val_loss: 0.3414 - val_acc: 0.5002\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.2978 - acc: 0.5370 - val_loss: 0.3413 - val_acc: 0.5003\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2964 - acc: 0.5394 - val_loss: 0.3401 - val_acc: 0.5008\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2916 - acc: 0.5534 - val_loss: 0.3419 - val_acc: 0.5018\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2893 - acc: 0.5715 - val_loss: 0.3467 - val_acc: 0.5011\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2915 - acc: 0.5856 - val_loss: 0.3494 - val_acc: 0.5008\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2935 - acc: 0.5878 - val_loss: 0.3481 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2922 - acc: 0.5854 - val_loss: 0.3448 - val_acc: 0.5015\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2900 - acc: 0.5782 - val_loss: 0.3425 - val_acc: 0.5010\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2894 - acc: 0.5684 - val_loss: 0.3416 - val_acc: 0.5010\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2903 - acc: 0.5609 - val_loss: 0.3415 - val_acc: 0.5006\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2910 - acc: 0.5574 - val_loss: 0.3411 - val_acc: 0.5007\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2906 - acc: 0.5586 - val_loss: 0.3411 - val_acc: 0.5014\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2895 - acc: 0.5632 - val_loss: 0.3417 - val_acc: 0.5025\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2886 - acc: 0.5724 - val_loss: 0.3431 - val_acc: 0.5013\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2888 - acc: 0.5803 - val_loss: 0.3445 - val_acc: 0.5019\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2895 - acc: 0.5863 - val_loss: 0.3449 - val_acc: 0.5019\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2897 - acc: 0.5882 - val_loss: 0.3440 - val_acc: 0.5014\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2892 - acc: 0.5844 - val_loss: 0.3425 - val_acc: 0.5016\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 0.3169 - acc: 0.5784 - val_loss: 0.3659 - val_acc: 0.5008\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3174 - acc: 0.5326 - val_loss: 0.3662 - val_acc: 0.5011\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3167 - acc: 0.5345 - val_loss: 0.3677 - val_acc: 0.5013\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3125 - acc: 0.5484 - val_loss: 0.3734 - val_acc: 0.5011\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3125 - acc: 0.5658 - val_loss: 0.3777 - val_acc: 0.5009\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3145 - acc: 0.5719 - val_loss: 0.3757 - val_acc: 0.5005\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3135 - acc: 0.5671 - val_loss: 0.3716 - val_acc: 0.5005\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3120 - acc: 0.5585 - val_loss: 0.3689 - val_acc: 0.5008\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3121 - acc: 0.5503 - val_loss: 0.3682 - val_acc: 0.5007\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3127 - acc: 0.5463 - val_loss: 0.3684 - val_acc: 0.5010\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3125 - acc: 0.5477 - val_loss: 0.3692 - val_acc: 0.5009\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.3118 - acc: 0.5532 - val_loss: 0.3708 - val_acc: 0.5013\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3113 - acc: 0.5599 - val_loss: 0.3725 - val_acc: 0.5015\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.3113 - acc: 0.5648 - val_loss: 0.3731 - val_acc: 0.5010\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 2s 32us/step - loss: 0.3116 - acc: 0.5665 - val_loss: 0.3724 - val_acc: 0.5005\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.3115 - acc: 0.5638 - val_loss: 0.3708 - val_acc: 0.5003\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3112 - acc: 0.5591 - val_loss: 0.3694 - val_acc: 0.5008\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3110 - acc: 0.5553 - val_loss: 0.3687 - val_acc: 0.5009\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3110 - acc: 0.5532 - val_loss: 0.3689 - val_acc: 0.5014\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3108 - acc: 0.5543 - val_loss: 0.3699 - val_acc: 0.5012\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 0.3466 - acc: 0.5570 - val_loss: 0.3985 - val_acc: 0.5001\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3462 - acc: 0.5247 - val_loss: 0.3999 - val_acc: 0.5005\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3470 - acc: 0.5259 - val_loss: 0.4031 - val_acc: 0.5006\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3432 - acc: 0.5341 - val_loss: 0.4090 - val_acc: 0.5011\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3409 - acc: 0.5456 - val_loss: 0.4154 - val_acc: 0.5012\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3426 - acc: 0.5537 - val_loss: 0.4152 - val_acc: 0.5014\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3432 - acc: 0.5528 - val_loss: 0.4099 - val_acc: 0.5013\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3413 - acc: 0.5465 - val_loss: 0.4053 - val_acc: 0.5010\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3406 - acc: 0.5402 - val_loss: 0.4034 - val_acc: 0.5005\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.3413 - acc: 0.5363 - val_loss: 0.4038 - val_acc: 0.5002\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3413 - acc: 0.5367 - val_loss: 0.4056 - val_acc: 0.5007\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3408 - acc: 0.5396 - val_loss: 0.4084 - val_acc: 0.5010\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3406 - acc: 0.5443 - val_loss: 0.4107 - val_acc: 0.5007\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3404 - acc: 0.5481 - val_loss: 0.4111 - val_acc: 0.5008\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3401 - acc: 0.5498 - val_loss: 0.4097 - val_acc: 0.5008\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3398 - acc: 0.5481 - val_loss: 0.4077 - val_acc: 0.5017\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3398 - acc: 0.5443 - val_loss: 0.4057 - val_acc: 0.5013\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3399 - acc: 0.5412 - val_loss: 0.4046 - val_acc: 0.5011\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3399 - acc: 0.5393 - val_loss: 0.4046 - val_acc: 0.5011\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3396 - acc: 0.5396 - val_loss: 0.4058 - val_acc: 0.5009\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 0.2947 - acc: 0.5419 - val_loss: 0.3276 - val_acc: 0.5004\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2740 - acc: 0.5786 - val_loss: 0.3320 - val_acc: 0.5035\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2781 - acc: 0.5929 - val_loss: 0.3328 - val_acc: 0.5028\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2786 - acc: 0.6023 - val_loss: 0.3276 - val_acc: 0.5029\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2726 - acc: 0.6059 - val_loss: 0.3223 - val_acc: 0.5019\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2686 - acc: 0.5999 - val_loss: 0.3208 - val_acc: 0.5021\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2695 - acc: 0.5831 - val_loss: 0.3213 - val_acc: 0.5024\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2720 - acc: 0.5706 - val_loss: 0.3213 - val_acc: 0.5021\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2729 - acc: 0.5681 - val_loss: 0.3205 - val_acc: 0.5024\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2718 - acc: 0.5744 - val_loss: 0.3200 - val_acc: 0.5022\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2701 - acc: 0.5856 - val_loss: 0.3206 - val_acc: 0.5039\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2693 - acc: 0.6009 - val_loss: 0.3220 - val_acc: 0.5026\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2698 - acc: 0.6117 - val_loss: 0.3232 - val_acc: 0.5028\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2703 - acc: 0.6169 - val_loss: 0.3231 - val_acc: 0.5026\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2700 - acc: 0.6161 - val_loss: 0.3220 - val_acc: 0.5029\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2688 - acc: 0.6103 - val_loss: 0.3206 - val_acc: 0.5026\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2678 - acc: 0.6018 - val_loss: 0.3198 - val_acc: 0.5015\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2676 - acc: 0.5931 - val_loss: 0.3198 - val_acc: 0.5025\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2679 - acc: 0.5854 - val_loss: 0.3201 - val_acc: 0.5016\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2684 - acc: 0.5812 - val_loss: 0.3203 - val_acc: 0.5017\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 205us/step - loss: 0.4520 - acc: 0.5819 - val_loss: 0.4658 - val_acc: 0.5002\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3916 - acc: 0.5325 - val_loss: 0.4350 - val_acc: 0.4993\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3824 - acc: 0.5149 - val_loss: 0.4287 - val_acc: 0.4994\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3877 - acc: 0.5103 - val_loss: 0.4292 - val_acc: 0.4993\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3937 - acc: 0.5085 - val_loss: 0.4308 - val_acc: 0.4992\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3973 - acc: 0.5082 - val_loss: 0.4319 - val_acc: 0.4993\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3979 - acc: 0.5089 - val_loss: 0.4324 - val_acc: 0.4992\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3958 - acc: 0.5108 - val_loss: 0.4328 - val_acc: 0.4994\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3916 - acc: 0.5134 - val_loss: 0.4341 - val_acc: 0.4995\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3862 - acc: 0.5171 - val_loss: 0.4375 - val_acc: 0.4998\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3811 - acc: 0.5217 - val_loss: 0.4430 - val_acc: 0.5003\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3774 - acc: 0.5273 - val_loss: 0.4506 - val_acc: 0.5008\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3758 - acc: 0.5332 - val_loss: 0.4591 - val_acc: 0.5007\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3765 - acc: 0.5386 - val_loss: 0.4665 - val_acc: 0.5009\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3789 - acc: 0.5426 - val_loss: 0.4697 - val_acc: 0.5007\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3804 - acc: 0.5441 - val_loss: 0.4676 - val_acc: 0.5009\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3792 - acc: 0.5435 - val_loss: 0.4613 - val_acc: 0.5010\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3760 - acc: 0.5413 - val_loss: 0.4538 - val_acc: 0.5006\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3733 - acc: 0.5377 - val_loss: 0.4473 - val_acc: 0.5005\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3724 - acc: 0.5338 - val_loss: 0.4427 - val_acc: 0.5005\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.3477 - acc: 0.5304 - val_loss: 0.4102 - val_acc: 0.5009\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3372 - acc: 0.5545 - val_loss: 0.4200 - val_acc: 0.5002\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3414 - acc: 0.5661 - val_loss: 0.4107 - val_acc: 0.5010\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3364 - acc: 0.5600 - val_loss: 0.4007 - val_acc: 0.5012\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3342 - acc: 0.5451 - val_loss: 0.3962 - val_acc: 0.5003\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3362 - acc: 0.5356 - val_loss: 0.3950 - val_acc: 0.5005\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3372 - acc: 0.5325 - val_loss: 0.3953 - val_acc: 0.5006\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3362 - acc: 0.5340 - val_loss: 0.3975 - val_acc: 0.5004\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 30us/step - loss: 0.3350 - acc: 0.5398 - val_loss: 0.4017 - val_acc: 0.5009\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3349 - acc: 0.5477 - val_loss: 0.4054 - val_acc: 0.5015\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3357 - acc: 0.5543 - val_loss: 0.4058 - val_acc: 0.5014\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3353 - acc: 0.5548 - val_loss: 0.4033 - val_acc: 0.5012\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3340 - acc: 0.5507 - val_loss: 0.4004 - val_acc: 0.5009\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3334 - acc: 0.5453 - val_loss: 0.3987 - val_acc: 0.5008\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3337 - acc: 0.5416 - val_loss: 0.3985 - val_acc: 0.5006\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3340 - acc: 0.5406 - val_loss: 0.3996 - val_acc: 0.5007\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3338 - acc: 0.5425 - val_loss: 0.4017 - val_acc: 0.5013\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3333 - acc: 0.5467 - val_loss: 0.4043 - val_acc: 0.5014\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.3332 - acc: 0.5512 - val_loss: 0.4067 - val_acc: 0.5011\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3334 - acc: 0.5549 - val_loss: 0.4072 - val_acc: 0.5011\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 0.3198 - acc: 0.5564 - val_loss: 0.3987 - val_acc: 0.5021\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3250 - acc: 0.5783 - val_loss: 0.3830 - val_acc: 0.5011\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3196 - acc: 0.5509 - val_loss: 0.3812 - val_acc: 0.5005\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3230 - acc: 0.5386 - val_loss: 0.3823 - val_acc: 0.5008\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3220 - acc: 0.5430 - val_loss: 0.3854 - val_acc: 0.5008\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3200 - acc: 0.5541 - val_loss: 0.3896 - val_acc: 0.5004\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3198 - acc: 0.5659 - val_loss: 0.3913 - val_acc: 0.5011\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3201 - acc: 0.5691 - val_loss: 0.3892 - val_acc: 0.5017\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3198 - acc: 0.5645 - val_loss: 0.3860 - val_acc: 0.5014\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3193 - acc: 0.5572 - val_loss: 0.3838 - val_acc: 0.5016\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3193 - acc: 0.5510 - val_loss: 0.3834 - val_acc: 0.5012\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3193 - acc: 0.5497 - val_loss: 0.3845 - val_acc: 0.5013\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3188 - acc: 0.5526 - val_loss: 0.3870 - val_acc: 0.5011\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3186 - acc: 0.5587 - val_loss: 0.3897 - val_acc: 0.5005\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3189 - acc: 0.5638 - val_loss: 0.3904 - val_acc: 0.5005\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3188 - acc: 0.5659 - val_loss: 0.3890 - val_acc: 0.5014\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3181 - acc: 0.5639 - val_loss: 0.3867 - val_acc: 0.5015\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3179 - acc: 0.5597 - val_loss: 0.3853 - val_acc: 0.5015\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3181 - acc: 0.5555 - val_loss: 0.3847 - val_acc: 0.5014\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3183 - acc: 0.5539 - val_loss: 0.3849 - val_acc: 0.5016\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 179us/step - loss: 0.2693 - acc: 0.5553 - val_loss: 0.2947 - val_acc: 0.5019\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2449 - acc: 0.5964 - val_loss: 0.2943 - val_acc: 0.5051\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2471 - acc: 0.6075 - val_loss: 0.2969 - val_acc: 0.5026\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2508 - acc: 0.6077 - val_loss: 0.2947 - val_acc: 0.5034\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2472 - acc: 0.6138 - val_loss: 0.2903 - val_acc: 0.5036\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2413 - acc: 0.6247 - val_loss: 0.2879 - val_acc: 0.5038\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2385 - acc: 0.6249 - val_loss: 0.2883 - val_acc: 0.5033\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2401 - acc: 0.6134 - val_loss: 0.2897 - val_acc: 0.5032\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2426 - acc: 0.6020 - val_loss: 0.2898 - val_acc: 0.5035\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2433 - acc: 0.6004 - val_loss: 0.2887 - val_acc: 0.5040\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2422 - acc: 0.6069 - val_loss: 0.2874 - val_acc: 0.5034\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2404 - acc: 0.6188 - val_loss: 0.2869 - val_acc: 0.5034\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2394 - acc: 0.6288 - val_loss: 0.2874 - val_acc: 0.5039\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2395 - acc: 0.6324 - val_loss: 0.2881 - val_acc: 0.5026\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2400 - acc: 0.6326 - val_loss: 0.2885 - val_acc: 0.5031\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2401 - acc: 0.6314 - val_loss: 0.2881 - val_acc: 0.5036\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2395 - acc: 0.6328 - val_loss: 0.2874 - val_acc: 0.5019\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2386 - acc: 0.6332 - val_loss: 0.2869 - val_acc: 0.5026\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2380 - acc: 0.6282 - val_loss: 0.2867 - val_acc: 0.5018\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2377 - acc: 0.6234 - val_loss: 0.2869 - val_acc: 0.5026\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 183us/step - loss: 0.2884 - acc: 0.6185 - val_loss: 0.3400 - val_acc: 0.4999\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.2883 - acc: 0.5531 - val_loss: 0.3403 - val_acc: 0.5004\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.2876 - acc: 0.5540 - val_loss: 0.3401 - val_acc: 0.5021\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 30us/step - loss: 0.2827 - acc: 0.5735 - val_loss: 0.3444 - val_acc: 0.5014\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2827 - acc: 0.5962 - val_loss: 0.3482 - val_acc: 0.5012\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.2850 - acc: 0.6022 - val_loss: 0.3472 - val_acc: 0.5010\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2840 - acc: 0.5979 - val_loss: 0.3440 - val_acc: 0.5002\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2822 - acc: 0.5883 - val_loss: 0.3423 - val_acc: 0.5012\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 2s 31us/step - loss: 0.2825 - acc: 0.5771 - val_loss: 0.3420 - val_acc: 0.5015\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2834 - acc: 0.5703 - val_loss: 0.3418 - val_acc: 0.5016\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2832 - acc: 0.5708 - val_loss: 0.3420 - val_acc: 0.5016\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2822 - acc: 0.5779 - val_loss: 0.3430 - val_acc: 0.5010\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2815 - acc: 0.5872 - val_loss: 0.3446 - val_acc: 0.5019\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2819 - acc: 0.5968 - val_loss: 0.3455 - val_acc: 0.5018\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.2823 - acc: 0.6002 - val_loss: 0.3447 - val_acc: 0.5017\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2820 - acc: 0.5975 - val_loss: 0.3431 - val_acc: 0.5014\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2813 - acc: 0.5916 - val_loss: 0.3418 - val_acc: 0.5013\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2813 - acc: 0.5831 - val_loss: 0.3413 - val_acc: 0.5022\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2815 - acc: 0.5779 - val_loss: 0.3413 - val_acc: 0.5024\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.2816 - acc: 0.5776 - val_loss: 0.3417 - val_acc: 0.5025\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 190us/step - loss: 0.4150 - acc: 0.5808 - val_loss: 0.4457 - val_acc: 0.5002\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3727 - acc: 0.5316 - val_loss: 0.4266 - val_acc: 0.4996\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3730 - acc: 0.5168 - val_loss: 0.4248 - val_acc: 0.4993\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3794 - acc: 0.5133 - val_loss: 0.4261 - val_acc: 0.4995\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3825 - acc: 0.5132 - val_loss: 0.4274 - val_acc: 0.4994\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3815 - acc: 0.5150 - val_loss: 0.4292 - val_acc: 0.4996\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3774 - acc: 0.5190 - val_loss: 0.4329 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3724 - acc: 0.5245 - val_loss: 0.4385 - val_acc: 0.5003\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3680 - acc: 0.5314 - val_loss: 0.4456 - val_acc: 0.5008\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3650 - acc: 0.5385 - val_loss: 0.4541 - val_acc: 0.5015\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3648 - acc: 0.5454 - val_loss: 0.4620 - val_acc: 0.5014\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3678 - acc: 0.5503 - val_loss: 0.4647 - val_acc: 0.5010\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3697 - acc: 0.5507 - val_loss: 0.4606 - val_acc: 0.5009\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3677 - acc: 0.5495 - val_loss: 0.4527 - val_acc: 0.5016\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3641 - acc: 0.5463 - val_loss: 0.4450 - val_acc: 0.5011\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3623 - acc: 0.5409 - val_loss: 0.4396 - val_acc: 0.5002\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3625 - acc: 0.5362 - val_loss: 0.4365 - val_acc: 0.5004\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3636 - acc: 0.5329 - val_loss: 0.4352 - val_acc: 0.5004\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3645 - acc: 0.5315 - val_loss: 0.4351 - val_acc: 0.5003\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3646 - acc: 0.5310 - val_loss: 0.4359 - val_acc: 0.5001\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 0.3430 - acc: 0.5320 - val_loss: 0.4105 - val_acc: 0.5004\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3305 - acc: 0.5635 - val_loss: 0.4233 - val_acc: 0.5010\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3377 - acc: 0.5757 - val_loss: 0.4121 - val_acc: 0.5013\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3315 - acc: 0.5668 - val_loss: 0.4009 - val_acc: 0.5015\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3288 - acc: 0.5514 - val_loss: 0.3969 - val_acc: 0.5009\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3311 - acc: 0.5404 - val_loss: 0.3963 - val_acc: 0.5002\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3323 - acc: 0.5374 - val_loss: 0.3974 - val_acc: 0.5004\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3313 - acc: 0.5405 - val_loss: 0.4002 - val_acc: 0.5009\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3300 - acc: 0.5478 - val_loss: 0.4043 - val_acc: 0.5009\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3296 - acc: 0.5564 - val_loss: 0.4072 - val_acc: 0.5012\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3300 - acc: 0.5618 - val_loss: 0.4073 - val_acc: 0.5019\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3298 - acc: 0.5623 - val_loss: 0.4050 - val_acc: 0.5018\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3293 - acc: 0.5593 - val_loss: 0.4020 - val_acc: 0.5017\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3288 - acc: 0.5533 - val_loss: 0.3999 - val_acc: 0.5013\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3287 - acc: 0.5484 - val_loss: 0.3994 - val_acc: 0.5010\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3287 - acc: 0.5470 - val_loss: 0.4003 - val_acc: 0.5012\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3284 - acc: 0.5486 - val_loss: 0.4026 - val_acc: 0.5009\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 29us/step - loss: 0.3282 - acc: 0.5526 - val_loss: 0.4055 - val_acc: 0.5011\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3284 - acc: 0.5575 - val_loss: 0.4074 - val_acc: 0.5009\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3284 - acc: 0.5615 - val_loss: 0.4077 - val_acc: 0.5013\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 10s 200us/step - loss: 0.2967 - acc: 0.5624 - val_loss: 0.3711 - val_acc: 0.5021\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3004 - acc: 0.5933 - val_loss: 0.3631 - val_acc: 0.5009\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2930 - acc: 0.5932 - val_loss: 0.3560 - val_acc: 0.5013\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2903 - acc: 0.5760 - val_loss: 0.3544 - val_acc: 0.5012\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2928 - acc: 0.5631 - val_loss: 0.3540 - val_acc: 0.5011\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2930 - acc: 0.5639 - val_loss: 0.3547 - val_acc: 0.5015\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2917 - acc: 0.5729 - val_loss: 0.3569 - val_acc: 0.5022\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2917 - acc: 0.5865 - val_loss: 0.3589 - val_acc: 0.5026\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2923 - acc: 0.5952 - val_loss: 0.3587 - val_acc: 0.5030\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2920 - acc: 0.5955 - val_loss: 0.3568 - val_acc: 0.5021\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2908 - acc: 0.5886 - val_loss: 0.3549 - val_acc: 0.5024\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2901 - acc: 0.5797 - val_loss: 0.3542 - val_acc: 0.5018\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2901 - acc: 0.5739 - val_loss: 0.3545 - val_acc: 0.5013\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2901 - acc: 0.5730 - val_loss: 0.3557 - val_acc: 0.5016\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2900 - acc: 0.5758 - val_loss: 0.3571 - val_acc: 0.5007\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2899 - acc: 0.5807 - val_loss: 0.3584 - val_acc: 0.5009\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2900 - acc: 0.5843 - val_loss: 0.3591 - val_acc: 0.5005\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.2900 - acc: 0.5872 - val_loss: 0.3587 - val_acc: 0.5006\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.2899 - acc: 0.5863 - val_loss: 0.3577 - val_acc: 0.5009\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.2896 - acc: 0.5836 - val_loss: 0.3567 - val_acc: 0.5015\n",
      "y_true shape (?, ?)\n",
      "y_pred shape (?, 1)\n",
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 0.3520 - acc: 0.5807 - val_loss: 0.4073 - val_acc: 0.5001\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3413 - acc: 0.5340 - val_loss: 0.4054 - val_acc: 0.5003\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3478 - acc: 0.5254 - val_loss: 0.4067 - val_acc: 0.5003\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3462 - acc: 0.5287 - val_loss: 0.4102 - val_acc: 0.5005\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3408 - acc: 0.5380 - val_loss: 0.4180 - val_acc: 0.5015\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3379 - acc: 0.5519 - val_loss: 0.4279 - val_acc: 0.5015\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3399 - acc: 0.5631 - val_loss: 0.4323 - val_acc: 0.5009\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3420 - acc: 0.5665 - val_loss: 0.4282 - val_acc: 0.5013\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3400 - acc: 0.5635 - val_loss: 0.4209 - val_acc: 0.5015\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3374 - acc: 0.5560 - val_loss: 0.4152 - val_acc: 0.5006\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3374 - acc: 0.5485 - val_loss: 0.4126 - val_acc: 0.5007\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3386 - acc: 0.5428 - val_loss: 0.4119 - val_acc: 0.5005\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3393 - acc: 0.5409 - val_loss: 0.4126 - val_acc: 0.5008\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3387 - acc: 0.5424 - val_loss: 0.4146 - val_acc: 0.5012\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3375 - acc: 0.5468 - val_loss: 0.4178 - val_acc: 0.5012\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 28us/step - loss: 0.3367 - acc: 0.5527 - val_loss: 0.4214 - val_acc: 0.5012\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3369 - acc: 0.5589 - val_loss: 0.4235 - val_acc: 0.5015\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3374 - acc: 0.5616 - val_loss: 0.4229 - val_acc: 0.5017\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3374 - acc: 0.5605 - val_loss: 0.4201 - val_acc: 0.5018\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3367 - acc: 0.5572 - val_loss: 0.4168 - val_acc: 0.5015\n"
     ]
    }
   ],
   "source": [
    "num_data_test = 50000\n",
    "lvals = []\n",
    "for i in range(len(thetas)):\n",
    "    theta = thetas[i]\n",
    "    model.compile(optimizer='adam', loss=my_loss_wrapper(train_result_list[i],theta),metrics=['accuracy'])\n",
    "    model.fit(X_fit[:num_data_test].reshape(-1,204), np.argmax(Y_fit[:num_data_test],axis=1), epochs=20, batch_size=int(num_data_test),validation_data=(X_fit[-num_data_test:].reshape(-1,204), np.argmax(Y_fit[-num_data_test:],axis=1)),verbose=1)\n",
    "    lvals+=[model.history.history['val_loss']]\n",
    "    print\n",
    "    pass\n",
    "#print(lvals) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH78BRKtB8IeHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_val = [loss_num[-1] for loss_num in lvals]\n",
    "plt.cla()\n",
    "plt.plot(thetas,loss_val)\n",
    "plt.savefig(\"aLund.png\")\n",
    "display(Image(filename=\"aLund.png\"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
