{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import keras\n",
    "\n",
    "# standard numerical library imports\n",
    "import pandas\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_part_sub_all_175 = pandas.read_csv(\"../output_particles_175.csv\")\n",
    "jet_part_sub_all_175[\"m\"]=0.175\n",
    "jet_part_sub_175 = [y for x,y in jet_part_sub_all_175.groupby([\"entry\"])]\n",
    "len(jet_part_sub_175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_part_sub_all_180 = pandas.read_csv(\"../output_particles_180.csv\")\n",
    "jet_part_sub_all_180[\"m\"]= 0.180\n",
    "jet_part_sub_180 = [y for x,y in jet_part_sub_all_180.groupby([\"entry\"])]\n",
    "len(jet_part_sub_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jet_size_175 = []\n",
    "for i in range(len(jet_part_sub_175)):\n",
    "    jet_size_175.append(len(jet_part_sub_175[i]))\n",
    "\n",
    "jet_size_180 = []\n",
    "for i in range(len(jet_part_sub_180)):\n",
    "    jet_size_180.append(len(jet_part_sub_180[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0748, 5.0911)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(jet_size_175), np.average(jet_size_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(jet_size_175), np.min(jet_size_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(jet_size_175), np.max(jet_size_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_jet_size = max(np.max(jet_size_175), np.max(jet_size_180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want N X 1200 X 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y0 = np.zeros((int(len(jet_part_sub_175)/3)))\n",
    "Y1 = np.ones((int(len(jet_part_sub_180)/3)))\n",
    "Y  = np.concatenate([Y0,Y1])\n",
    "Y = to_categorical(Y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_to_use = [\"pT\",\"eta\",\"phi\",\"btag\",\"m\"]\n",
    "num_feature = len(feature_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0_all = np.zeros((len(jet_part_sub_175), max_jet_size,num_feature))\n",
    "X1_all = np.zeros((len(jet_part_sub_180), max_jet_size,num_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 13, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(jet_part_sub_175)):\n",
    "    jets = np.array(jet_part_sub_175[i][feature_to_use])\n",
    "    X0_all[i,:len(jets),:] = jets[:max_jet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15    0\n",
       "16    1\n",
       "17    1\n",
       "18    0\n",
       "Name: btag, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_part_sub_175[3][\"btag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(jet_part_sub_180)):\n",
    "    jets = np.array(jet_part_sub_180[i][feature_to_use])\n",
    "    X1_all[i,:len(jets),:] = jets[:max_jet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0_train = X0_all[:int(len(jet_part_sub_175)/3),:,:]\n",
    "X1_train = X0_all[:int(len(jet_part_sub_180)/3),:,:]\n",
    "X0_test = X0_all[int(len(jet_part_sub_175)/2):int(len(2*jet_part_sub_180)/3),:,:]\n",
    "X1_test = X1_all[int(len(jet_part_sub_180)/2):int(len(2*jet_part_sub_180)/3),:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3333, 13, 5), (3333, 13, 5))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0_train.shape, X1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([X0_train,X1_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420.3793\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(X[1,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remap_pids(X, pid_i=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i] = normalize(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = data_split(X, Y, test=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4667, 13, 5)\n",
      "(4667, 2)\n",
      "(1999, 13, 5)\n",
      "(1999, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim= num_feature, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_label = 'DCTR_top_tagging'\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(save_label + '.h5', \n",
    "                                                monitor='val_loss', \n",
    "                                                verbose=2, \n",
    "                                                save_best_only=True, \n",
    "                                                mode='min')\n",
    "\n",
    "CSVLogger = keras.callbacks.CSVLogger(save_label + '_loss.csv', append=False)\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                              min_delta=0, \n",
    "                                              patience=10, \n",
    "                                              verbose=1, \n",
    "                                              restore_best_weights=True)\n",
    "\n",
    "callbacks = [checkpoint, CSVLogger, EarlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4667 samples, validate on 1999 samples\n",
      "Epoch 1/10\n",
      "4667/4667 [==============================] - 27s 6ms/step - loss: 0.7131 - acc: 0.4905 - val_loss: 0.6941 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69406, saving model to DCTR_top_tagging.h5\n",
      "Epoch 2/10\n",
      "4667/4667 [==============================] - 20s 4ms/step - loss: 0.6936 - acc: 0.5046 - val_loss: 0.6933 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69406 to 0.69326, saving model to DCTR_top_tagging.h5\n",
      "Epoch 3/10\n",
      "4667/4667 [==============================] - 18s 4ms/step - loss: 0.6936 - acc: 0.4956 - val_loss: 0.6935 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69326\n",
      "Epoch 4/10\n",
      "4667/4667 [==============================] - 18s 4ms/step - loss: 0.6934 - acc: 0.5007 - val_loss: 0.6938 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69326\n",
      "Epoch 5/10\n",
      "4667/4667 [==============================] - 24s 5ms/step - loss: 0.6935 - acc: 0.4965 - val_loss: 0.6933 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69326\n",
      "Epoch 6/10\n",
      "4667/4667 [==============================] - 23s 5ms/step - loss: 0.6935 - acc: 0.4877 - val_loss: 0.6942 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69326\n",
      "Epoch 7/10\n",
      "4667/4667 [==============================] - 33s 7ms/step - loss: 0.6935 - acc: 0.4958 - val_loss: 0.6931 - val_acc: 0.5123\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.69326 to 0.69309, saving model to DCTR_top_tagging.h5\n",
      "Epoch 8/10\n",
      "4667/4667 [==============================] - 33s 7ms/step - loss: 0.6933 - acc: 0.5007 - val_loss: 0.6929 - val_acc: 0.5123\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.69309 to 0.69285, saving model to DCTR_top_tagging.h5\n",
      "Epoch 9/10\n",
      "4667/4667 [==============================] - 37s 8ms/step - loss: 0.6930 - acc: 0.5160 - val_loss: 0.6937 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69285\n",
      "Epoch 10/10\n",
      "4667/4667 [==============================] - 29s 6ms/step - loss: 0.6934 - acc: 0.5067 - val_loss: 0.6934 - val_acc: 0.4877\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69285\n"
     ]
    }
   ],
   "source": [
    "history = dctr.fit(X_train, Y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 1,\n",
    "                    validation_data = (X_val, Y_val),\n",
    "                    verbose = 1, \n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_0 = dctr.predict(X0_test, batch_size=1)\n",
    "preds_1 = dctr.predict(X1_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.4866167 , 0.5133833 ],\n",
       "        [0.43823975, 0.5617603 ],\n",
       "        [0.43075877, 0.5692413 ],\n",
       "        ...,\n",
       "        [0.57236284, 0.42763722],\n",
       "        [0.48088357, 0.51911646],\n",
       "        [0.4826729 , 0.51732713]], dtype=float32),\n",
       " array([[0.5041373 , 0.49586272],\n",
       "        [0.4710738 , 0.52892613],\n",
       "        [0.50332314, 0.49667692],\n",
       "        ...,\n",
       "        [0.44974113, 0.5502588 ],\n",
       "        [0.47740132, 0.5225986 ],\n",
       "        [0.5651662 , 0.43483388]], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_0,preds_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_0 = preds_0[:,0]/preds_0[:,1]\n",
    "weights_1 = preds_1[:,0]/preds_1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5725048\n",
      "2.7264304\n",
      "1.9646866\n",
      "2.3508766\n"
     ]
    }
   ],
   "source": [
    "print(max(weights_0))\n",
    "print(max(1/weights_0))\n",
    "print(max(weights_1))\n",
    "print(max(1/weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddParams2Input(keras.layers.Layer):\n",
    "    \"\"\" Custom layer for tuning with DCTR: \n",
    "    Arguments:\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Usage: \n",
    "    Let X_dim be the input dimension of each particle to a PFN model, and n_MC_params be the number of MC parameters. \n",
    "    Defines a Layer that takes in an array of dimension \n",
    "    (batch_size, padded_multiplicity, X_dim - n_MC_params)\n",
    "    This layer appends each particle by the default_MC_params and makes then trainable or non-trainable based on trainable_MC_params\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_MC_params, default_MC_params, trainable_MC_params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        # Definitions\n",
    "        self.n_MC_params = n_MC_params\n",
    "        self.MC_params = default_MC_params\n",
    "        self.trainable_MC_params = trainable_MC_params\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Convert input MC parameters to weights and make then trainable or non-trainable\n",
    "        for i in range(self.n_MC_params):\n",
    "            self.MC_params[i] = self.add_weight(name='MC_param_{}'.format(i), \n",
    "                                                shape=(1, 1),\n",
    "                                                initializer=keras.initializers.Constant(self.MC_params[i]),\n",
    "                                                trainable=self.trainable_MC_params[i])\n",
    "            \n",
    "        self.MC_params = keras.backend.tf.concat(self.MC_params, axis = -1)\n",
    "        super(AddParams2Input, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Add MC params to each input particle (but not to the padded rows)\n",
    "        concat_input_and_params = keras.backend.tf.where(keras.backend.abs(input[...,0])>0,\n",
    "                                                         self.MC_params*keras.backend.ones_like(input[...,0:self.n_MC_params]),\n",
    "                                                         keras.backend.zeros_like(input[...,0:self.n_MC_params]))\n",
    "        return keras.backend.concatenate([input, concat_input_and_params], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]+self.n_MC_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_DCTR_fit_model(DCTR_model, \n",
    "                       X_dim, \n",
    "                       n_MC_params, \n",
    "                       default_MC_params,\n",
    "                       trainable_MC_params):\n",
    "    \"\"\" \n",
    "    Get a DCTR model that trains on the input MC parameters\n",
    "    \n",
    "    Arguments:\n",
    "    - DCTR_model : a PFN model that has been trained on a to continuously interpolate over the input MC dimensions\n",
    "    - X_dim : (int) - the dimension of the input expected by DCTR_model\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Returns:\n",
    "    - DCTR_fit_model: a compiled model that gradient descends only on the trainable MC parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do sanity checks on inputs\n",
    "    assert X_dim >=n_MC_params, \"X_dim must be larger than n_MC_params. X_dim includes the dimensionality of the 4-vector + number of MC parameters\"\n",
    "    assert n_MC_params == len(default_MC_params), \"Dimension mismatch between n_MC_params and number of default MC parameters given. len(default_MC_params) must equal n_MC_params\"\n",
    "    assert n_MC_params == len(trainable_MC_params), \"Dimension mismatch between n_MC_params and trainable_MC_params. len(trainable_MC_params) must equal n_MC_params.\"\n",
    "    assert np.any(trainable_MC_params), \"All parameters are set to non-trainable.\"\n",
    "    \n",
    "    # Define input to DCTR_fit_model\n",
    "    non_param_input = keras.layers.Input((None, X_dim - n_MC_params))\n",
    "\n",
    "    # Construct layer that adds trainable and non-trainable parameters to the input\n",
    "    add_params_layer = AddParams2Input(n_MC_params, default_MC_params, trainable_MC_params)\n",
    "    time_dist     = keras.layers.TimeDistributed(add_params_layer, name='tdist')(non_param_input)     \n",
    "\n",
    "    # Set all weights in DCTR_model to non-trainable\n",
    "    for layer in DCTR_model.model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # get the graph and the weights from the DCTR_model\n",
    "    output = DCTR_model.model(inputs = time_dist)\n",
    "\n",
    "    # Define full model\n",
    "    DCTR_fit_model = fitmodel = keras.models.Model(inputs = non_param_input, outputs = output)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    # Compile with loss function\n",
    "    DCTR_fit_model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    return DCTR_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "tdist (TimeDistributed)      (None, None, 5)           1         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 2)                 56930     \n",
      "=================================================================\n",
      "Total params: 56,931\n",
      "Trainable params: 1\n",
      "Non-trainable params: 56,930\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dctr_fit_model = get_DCTR_fit_model(dctr, \n",
    "                       X_dim =5, \n",
    "                       n_MC_params = 1, \n",
    "                       default_MC_params   = [0.175], # default params for [alpha_s, aLund, StoUD]\n",
    "                       trainable_MC_params = [True]) # Only train aLund\n",
    "\n",
    "dctr_fit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_MC_params(dctr_fit_model, MC_params):\n",
    "    top_mass = MC_params\n",
    "    weights = np.array([[top_mass]],dtype=np.float32)\n",
    "    dctr_fit_model.layers[1].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dctr_fit_model.layers[1].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6666, 13, 5), (6666, 2))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0_fit = X0_all[int(2*len(jet_part_sub_175)/3):9999,:,:]\n",
    "X1_fit = X1_all[int(2*len(jet_part_sub_180)/3):9999,:,:]\n",
    "X_fit = np.concatenate([X0_fit,X1_fit])[:,:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_fit = Y[:len(X_fit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6666, 13, 4), (6666, 2))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fit.shape,Y_fit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_fit, _, Y_fit, _ = data_split(X_fit, Y_fit, test=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(X, Y, dctr_fit_model, MC_params, batch_size = 1000):\n",
    "    set_MC_params(dctr_fit_model, MC_params)\n",
    "    return dctr_fit_model.evaluate(x=X, y = Y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.175]], dtype=float32)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dctr_fit_model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6666/6666 [==============================] - 1s 95us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 75us/step\n",
      "6666/6666 [==============================] - 0s 73us/step\n",
      "6666/6666 [==============================] - 0s 73us/step\n",
      "6666/6666 [==============================] - 0s 72us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 1s 75us/step\n",
      "6666/6666 [==============================] - 0s 73us/step\n",
      "6666/6666 [==============================] - 1s 75us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 72us/step\n",
      "6666/6666 [==============================] - 1s 75us/step\n",
      "6666/6666 [==============================] - 0s 73us/step\n",
      "6666/6666 [==============================] - 0s 73us/step\n",
      "6666/6666 [==============================] - 0s 72us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 73us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 72us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 72us/step\n",
      "6666/6666 [==============================] - 1s 76us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 72us/step\n",
      "6666/6666 [==============================] - 0s 74us/step\n",
      "6666/6666 [==============================] - 0s 72us/step\n",
      "6666/6666 [==============================] - 0s 71us/step\n"
     ]
    }
   ],
   "source": [
    "top_mass_loss = np.array([(top_mass, get_loss(X_fit, Y_fit, dctr_fit_model, [top_mass])) for top_mass in np.linspace(0.170,0.190, 31)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17       0.69458045]\n",
      " [0.17066667 0.69458099]\n",
      " [0.17133333 0.69458152]\n",
      " [0.172      0.69458199]\n",
      " [0.17266667 0.69458256]\n",
      " [0.17333333 0.69458304]\n",
      " [0.174      0.69458355]\n",
      " [0.17466667 0.69458412]\n",
      " [0.17533333 0.69458455]\n",
      " [0.176      0.69458502]\n",
      " [0.17666667 0.69458555]\n",
      " [0.17733333 0.69458608]\n",
      " [0.178      0.69458663]\n",
      " [0.17866667 0.69458716]\n",
      " [0.17933333 0.69458772]\n",
      " [0.18       0.69458822]\n",
      " [0.18066667 0.69458876]\n",
      " [0.18133333 0.69458923]\n",
      " [0.182      0.69458977]\n",
      " [0.18266667 0.69459031]\n",
      " [0.18333333 0.69459086]\n",
      " [0.184      0.69459117]\n",
      " [0.18466667 0.69459169]\n",
      " [0.18533333 0.69459208]\n",
      " [0.186      0.69459243]\n",
      " [0.18666667 0.69459284]\n",
      " [0.18733333 0.69459322]\n",
      " [0.188      0.69459364]\n",
      " [0.18866667 0.69459394]\n",
      " [0.18933333 0.69459427]\n",
      " [0.19       0.69459471]]\n"
     ]
    }
   ],
   "source": [
    "print(top_mass_loss)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
