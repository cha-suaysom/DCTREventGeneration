{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import keras\n",
    "\n",
    "# standard numerical library imports\n",
    "import pandas\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# energyflow imports\n",
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split, remap_pids, to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_part_sub_all_175 = pandas.read_csv(\"../output_all_stable_particles_175.csv\")\n",
    "jet_part_sub_all_175[\"m\"]=0.175\n",
    "jet_part_sub_175 = [y for x,y in jet_part_sub_all_175.groupby([\"entry\"])]\n",
    "len(jet_part_sub_175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_part_sub_all_180 = pandas.read_csv(\"../output_all_stable_particles_180.csv\")\n",
    "jet_part_sub_all_180[\"m\"]= 0.180\n",
    "jet_part_sub_180 = [y for x,y in jet_part_sub_all_180.groupby([\"entry\"])]\n",
    "len(jet_part_sub_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jet_size_175 = []\n",
    "for i in range(len(jet_part_sub_175)):\n",
    "    jet_size_175.append(len(jet_part_sub_175[i]))\n",
    "\n",
    "jet_size_180 = []\n",
    "for i in range(len(jet_part_sub_180)):\n",
    "    jet_size_180.append(len(jet_part_sub_180[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538.4231, 538.9929)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(jet_size_175), np.average(jet_size_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 122)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(jet_size_175), np.min(jet_size_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1113, 1192)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(jet_size_175), np.max(jet_size_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_jet_size = max(np.max(jet_size_175), np.max(jet_size_180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want N X 1200 X 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y0 = np.zeros((int(len(jet_part_sub_175)/3)))\n",
    "Y1 = np.ones((int(len(jet_part_sub_180)/3)))\n",
    "Y  = np.concatenate([Y0,Y1])\n",
    "Y = to_categorical(Y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_to_use = [\"pT\",\"eta\",\"phi\",\"pid\",\"m\"]\n",
    "num_feature = len(feature_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0_all = np.zeros((len(jet_part_sub_175), max_jet_size,num_feature))\n",
    "X1_all = np.zeros((len(jet_part_sub_180), max_jet_size,num_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(jet_part_sub_175)):\n",
    "    jets = np.array(jet_part_sub_175[i][feature_to_use])\n",
    "    X0_all[i,:len(jets),:] = jets[:max_jet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(jet_part_sub_180)):\n",
    "    jets = np.array(jet_part_sub_180[i][feature_to_use])\n",
    "    X1_all[i,:len(jets),:] = jets[:max_jet_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remap_pids(X0_all, pid_i=3)\n",
    "remap_pids(X1_all, pid_i=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0_train = X0_all[:int(len(jet_part_sub_175)/3),:,:]\n",
    "X1_train = X0_all[:int(len(jet_part_sub_180)/3),:,:]\n",
    "X0_test = X0_all[int(len(jet_part_sub_175)/2):int(len(2*jet_part_sub_180)/3),:,:]\n",
    "X1_test = X1_all[int(len(jet_part_sub_180)/2):int(len(2*jet_part_sub_180)/3),:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3333, 1192, 5), (3333, 1192, 5))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0_train.shape, X1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([X0_train,X1_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735.573565333\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(X[1,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i] = normalize(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = data_split(X, Y, test=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4667, 1192, 5)\n",
      "(4667, 2)\n",
      "(1999, 1192, 5)\n",
      "(1999, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# network architecture parameters\n",
    "Phi_sizes = (100,100, 128)\n",
    "F_sizes = (100,100, 100)\n",
    "\n",
    "dctr = PFN(input_dim= num_feature, \n",
    "           Phi_sizes=Phi_sizes, F_sizes=F_sizes,\n",
    "           summary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_label = 'DCTR_top_tagging'\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(save_label + '.h5', \n",
    "                                                monitor='val_loss', \n",
    "                                                verbose=2, \n",
    "                                                save_best_only=True, \n",
    "                                                mode='min')\n",
    "\n",
    "CSVLogger = keras.callbacks.CSVLogger(save_label + '_loss.csv', append=False)\n",
    "\n",
    "EarlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                              min_delta=0, \n",
    "                                              patience=10, \n",
    "                                              verbose=1, \n",
    "                                              restore_best_weights=True)\n",
    "\n",
    "callbacks = [checkpoint, CSVLogger, EarlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4667 samples, validate on 1999 samples\n",
      "Epoch 1/10\n",
      "4667/4667 [==============================] - 111s 24ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8.13565, saving model to DCTR_top_tagging.h5\n",
      "Epoch 2/10\n",
      "4667/4667 [==============================] - 86s 18ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 8.13565\n",
      "Epoch 3/10\n",
      "4667/4667 [==============================] - 64s 14ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 8.13565\n",
      "Epoch 4/10\n",
      "4667/4667 [==============================] - 66s 14ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 8.13565\n",
      "Epoch 5/10\n",
      "4667/4667 [==============================] - 72s 15ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 8.13565\n",
      "Epoch 6/10\n",
      "4667/4667 [==============================] - 76s 16ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 8.13565\n",
      "Epoch 7/10\n",
      "4667/4667 [==============================] - 69s 15ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 8.13565\n",
      "Epoch 8/10\n",
      "4667/4667 [==============================] - 76s 16ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 8.13565\n",
      "Epoch 9/10\n",
      "4667/4667 [==============================] - 81s 17ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 8.13565\n",
      "Epoch 10/10\n",
      "4667/4667 [==============================] - 79s 17ms/step - loss: 8.0262 - acc: 0.5020 - val_loss: 8.1356 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 8.13565\n"
     ]
    }
   ],
   "source": [
    "history = dctr.fit(X_train, Y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 1,\n",
    "                    validation_data = (X_val, Y_val),\n",
    "                    verbose = 1, \n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_0 = dctr.predict(X0_test, batch_size=1)\n",
    "preds_1 = dctr.predict(X1_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]], dtype=float32), array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_0,preds_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if __name__ == '__main__':\n",
      "/root/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "weights_0 = preds_0[:,0]/preds_0[:,1]\n",
    "weights_1 = preds_1[:,0]/preds_1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "0.0\n",
      "inf\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(max(weights_0))\n",
    "print(max(1/weights_0))\n",
    "print(max(weights_1))\n",
    "print(max(1/weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddParams2Input(keras.layers.Layer):\n",
    "    \"\"\" Custom layer for tuning with DCTR: \n",
    "    Arguments:\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Usage: \n",
    "    Let X_dim be the input dimension of each particle to a PFN model, and n_MC_params be the number of MC parameters. \n",
    "    Defines a Layer that takes in an array of dimension \n",
    "    (batch_size, padded_multiplicity, X_dim - n_MC_params)\n",
    "    This layer appends each particle by the default_MC_params and makes then trainable or non-trainable based on trainable_MC_params\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_MC_params, default_MC_params, trainable_MC_params):\n",
    "        super(AddParams2Input, self).__init__()\n",
    "        # Definitions\n",
    "        self.n_MC_params = n_MC_params\n",
    "        self.MC_params = default_MC_params\n",
    "        self.trainable_MC_params = trainable_MC_params\n",
    "\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Convert input MC parameters to weights and make then trainable or non-trainable\n",
    "        for i in range(self.n_MC_params):\n",
    "            self.MC_params[i] = self.add_weight(name='MC_param_{}'.format(i), \n",
    "                                                shape=(1, 1),\n",
    "                                                initializer=keras.initializers.Constant(self.MC_params[i]),\n",
    "                                                trainable=self.trainable_MC_params[i])\n",
    "            \n",
    "        self.MC_params = keras.backend.tf.concat(self.MC_params, axis = -1)\n",
    "        super(AddParams2Input, self).build(input_shape)\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Add MC params to each input particle (but not to the padded rows)\n",
    "        concat_input_and_params = keras.backend.tf.where(keras.backend.abs(input[...,0])>0,\n",
    "                                                         self.MC_params*keras.backend.ones_like(input[...,0:self.n_MC_params]),\n",
    "                                                         keras.backend.zeros_like(input[...,0:self.n_MC_params]))\n",
    "        return keras.backend.concatenate([input, concat_input_and_params], -1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1]+self.n_MC_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_DCTR_fit_model(DCTR_model, \n",
    "                       X_dim, \n",
    "                       n_MC_params, \n",
    "                       default_MC_params,\n",
    "                       trainable_MC_params):\n",
    "    \"\"\" \n",
    "    Get a DCTR model that trains on the input MC parameters\n",
    "    \n",
    "    Arguments:\n",
    "    - DCTR_model : a PFN model that has been trained on a to continuously interpolate over the input MC dimensions\n",
    "    - X_dim : (int) - the dimension of the input expected by DCTR_model\n",
    "    - n_MC_params : (int) - the number of n_MC_params that are in X_dim\n",
    "    - default_MC_params : (list of floats) - default values for each of the MC parameters\n",
    "    - trainable_MC_params : (list of booleans) - True for parameters that you want to fit, false for parameters that should be fixed at default value\n",
    "\n",
    "    Returns:\n",
    "    - DCTR_fit_model: a compiled model that gradient descends only on the trainable MC parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do sanity checks on inputs\n",
    "    assert X_dim >=n_MC_params, \"X_dim must be larger than n_MC_params. X_dim includes the dimensionality of the 4-vector + number of MC parameters\"\n",
    "    assert n_MC_params == len(default_MC_params), \"Dimension mismatch between n_MC_params and number of default MC parameters given. len(default_MC_params) must equal n_MC_params\"\n",
    "    assert n_MC_params == len(trainable_MC_params), \"Dimension mismatch between n_MC_params and trainable_MC_params. len(trainable_MC_params) must equal n_MC_params.\"\n",
    "    assert np.any(trainable_MC_params), \"All parameters are set to non-trainable.\"\n",
    "    \n",
    "    # Define input to DCTR_fit_model\n",
    "    non_param_input = keras.layers.Input((None, X_dim - n_MC_params))\n",
    "\n",
    "    # Construct layer that adds trainable and non-trainable parameters to the input\n",
    "    add_params_layer = AddParams2Input(n_MC_params, default_MC_params, trainable_MC_params)\n",
    "    time_dist     = keras.layers.TimeDistributed(add_params_layer, name='tdist')(non_param_input)     \n",
    "\n",
    "    # Set all weights in DCTR_model to non-trainable\n",
    "    for layer in DCTR_model.model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # get the graph and the weights from the DCTR_model\n",
    "    output = DCTR_model.model(inputs = time_dist)\n",
    "\n",
    "    # Define full model\n",
    "    DCTR_fit_model = fitmodel = keras.models.Model(inputs = non_param_input, outputs = output)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    # Compile with loss function\n",
    "    DCTR_fit_model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "    \n",
    "    return DCTR_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "tdist (TimeDistributed)      (None, None, 5)           1         \n",
      "_________________________________________________________________\n",
      "model_5 (Model)              (None, 2)                 56930     \n",
      "=================================================================\n",
      "Total params: 56,931\n",
      "Trainable params: 1\n",
      "Non-trainable params: 56,930\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dctr_fit_model = get_DCTR_fit_model(dctr, \n",
    "                       X_dim =5, \n",
    "                       n_MC_params = 1, \n",
    "                       default_MC_params   = [0.175], # default params for [alpha_s, aLund, StoUD]\n",
    "                       trainable_MC_params = [True]) # Only train aLund\n",
    "\n",
    "dctr_fit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_MC_params(dctr_fit_model, MC_params):\n",
    "    top_mass = MC_params\n",
    "    weights = np.array([[top_mass]],dtype=np.float32)\n",
    "    dctr_fit_model.layers[1].set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dctr_fit_model.layers[1].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6666, 1192, 5), (6666, 2))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0_fit = X0_all[int(2*len(jet_part_sub_175)/3):9999,:,:]\n",
    "X1_fit = X1_all[int(2*len(jet_part_sub_180)/3):9999,:,:]\n",
    "X_fit = np.concatenate([X0_fit,X1_fit])[:,:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_fit = Y[:len(X_fit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6666, 1192, 4), (6666, 2))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fit.shape,Y_fit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_fit, _, Y_fit, _ = data_split(X_fit, Y_fit, test=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(X, Y, dctr_fit_model, MC_params, batch_size = 1000):\n",
    "    set_MC_params(dctr_fit_model, MC_params)\n",
    "    return dctr_fit_model.evaluate(x=X, y = Y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.175]], dtype=float32)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dctr_fit_model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6666/6666 [==============================] - 43s 6ms/step\n",
      "6666/6666 [==============================] - 37s 6ms/step\n",
      "6666/6666 [==============================] - 42s 6ms/step\n",
      "6666/6666 [==============================] - 44s 7ms/step\n",
      "6666/6666 [==============================] - 50s 8ms/step\n",
      "6666/6666 [==============================] - 42s 6ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 37s 6ms/step\n",
      "6666/6666 [==============================] - 37s 6ms/step\n",
      "6666/6666 [==============================] - 37s 6ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 40s 6ms/step\n",
      "6666/6666 [==============================] - 37s 6ms/step\n",
      "6666/6666 [==============================] - 40s 6ms/step\n",
      "6666/6666 [==============================] - 38s 6ms/step\n",
      "6666/6666 [==============================] - 37s 6ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 36s 5ms/step\n",
      "6666/6666 [==============================] - 43s 6ms/step\n",
      "6666/6666 [==============================] - 40s 6ms/step\n",
      "6666/6666 [==============================] - 37s 6ms/step\n",
      "6666/6666 [==============================] - 37s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "top_mass_loss = np.array([(top_mass, get_loss(X_fit, Y_fit, dctr_fit_model, [top_mass])) for top_mass in np.linspace(0.170,0.190, 31)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17       8.05903825]\n",
      " [0.17066667 8.05903825]\n",
      " [0.17133333 8.05903825]\n",
      " [0.172      8.05903825]\n",
      " [0.17266667 8.05903825]\n",
      " [0.17333333 8.05903825]\n",
      " [0.174      8.05903825]\n",
      " [0.17466667 8.05903825]\n",
      " [0.17533333 8.05903825]\n",
      " [0.176      8.05903825]\n",
      " [0.17666667 8.05903825]\n",
      " [0.17733333 8.05903825]\n",
      " [0.178      8.05903825]\n",
      " [0.17866667 8.05903825]\n",
      " [0.17933333 8.05903825]\n",
      " [0.18       8.05903825]\n",
      " [0.18066667 8.05903825]\n",
      " [0.18133333 8.05903825]\n",
      " [0.182      8.05903825]\n",
      " [0.18266667 8.05903825]\n",
      " [0.18333333 8.05903825]\n",
      " [0.184      8.05903825]\n",
      " [0.18466667 8.05903825]\n",
      " [0.18533333 8.05903825]\n",
      " [0.186      8.05903825]\n",
      " [0.18666667 8.05903825]\n",
      " [0.18733333 8.05903825]\n",
      " [0.188      8.05903825]\n",
      " [0.18866667 8.05903825]\n",
      " [0.18933333 8.05903825]\n",
      " [0.19       8.05903825]]\n"
     ]
    }
   ],
   "source": [
    "print(top_mass_loss)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
